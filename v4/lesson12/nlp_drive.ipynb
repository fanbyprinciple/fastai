{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_drive",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQQq1kd3fty5",
        "outputId": "6d6829e3-0478-4176-bd94-51469560d397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 727kB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 14.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 18.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 4.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 16.8MB/s \n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5ZeevCEf5PR"
      },
      "source": [
        "from fastbook import *\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IsZ1Teff9bF"
      },
      "source": [
        "## Training a language model from scratch\n",
        "\n",
        "Getting the data to quickly prototype the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brdb_Oj_f8Lx",
        "outputId": "8877f230-9bed-4900-d5f6-ddab1590e394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# human numbers dataset\n",
        "\n",
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.HUMAN_NUMBERS)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FYxLTNEgb2g"
      },
      "source": [
        "Path.BASE_PATH = path"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBSD8mBVgfZ6",
        "outputId": "311a6157-4514-4824-f1e9-0944ab80163f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('train.txt'),Path('valid.txt')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsIFlgcxgguT",
        "outputId": "b23ffe98-065c-4eb0-a0c9-d8d3ae5b9bde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lines = L()\n",
        "\n",
        "with open(path/'train.txt') as f : lines += L(*f.readlines())\n",
        "with open(path/'valid.txt') as f : lines += L(*f.readlines())\n",
        "lines"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfnusiTUgyg0",
        "outputId": "85b6d67e-ace2-4be3-a48b-5f2a14de8465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "text = ' . '.join([l.strip() for l in lines])\n",
        "text[:100]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMWpsGTTg-BS",
        "outputId": "2f455c49-f41f-4cbb-92fa-7ef315ca41ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# lets tokenize\n",
        "\n",
        "tokens = text.split(' ')\n",
        "tokens[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8uORNCLhEpI"
      },
      "source": [
        "# lets numericalize\n",
        "\n",
        "# creating a list of unique tokens\n",
        "vocab = L(*tokens).unique()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dCESazYhMmu",
        "outputId": "38628045-25ee-403e-c02f-b734d8032e87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwhtOYlbhNpx",
        "outputId": "bfeca593-1456-4627-9801-308e76d53543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "nums = L(word2idx[i] for i in tokens)\n",
        "nums"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaZNyKcBhyzM"
      },
      "source": [
        "## First language Model from scratch\n",
        "\n",
        "We are going to predict each word baded on previous three words\n",
        "\n",
        "Lets attempt with plain python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_JtSCcChdBo",
        "outputId": "2657b76e-bf31-4c87-d5db-7fcd8c515c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "L((tokens[i:i+3], tokens[i+3]) for i in range(0, len(tokens)-4, 3))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVvTOLn7iSSd"
      },
      "source": [
        "Noe lets do it with tensors of numericalised values which is what the model will actually use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxlW6i2WiOcJ",
        "outputId": "62b8edd8-cf99-4d1b-953b-27945b1afbbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0, len(nums) - 4,3))\n",
        "seqs"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwB9zyD6ilgk"
      },
      "source": [
        "# batching it with dataloaders class\n",
        "\n",
        "bs= 64\n",
        "cut = int(len(seqs)*0.8)\n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0jTTTtJi3zI",
        "outputId": "7339786d-8a70-4139-c6cf-f82e4fa1a0df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for d in dls : print(d)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<fastai.data.core.TfmdDL object at 0x7ffb552d1240>\n",
            "<fastai.data.core.TfmdDL object at 0x7ffb552d1550>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YejiB4TZjWSE"
      },
      "source": [
        "Now we can create a neural network that takes three words as input and returns a prediction of probability of each possible next word in vocab.\n",
        "\n",
        "we will use three standard layers with tweaks\n",
        "\n",
        "first linear layer will only use first word embedding as activations\n",
        "\n",
        "second will used second word embeddingplus the first layers output\n",
        "\n",
        "the third will use word embedding plus the second layer output \n",
        "\n",
        "each will have same weight matrix, only activation change but layer weights will not change from layer to layer if that makes sense\n",
        "\n",
        "since layer weights do not change, it ismore like same layer repeated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhCY0eVtkTts"
      },
      "source": [
        "## Language model in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lc5J5bAkSk-"
      },
      "source": [
        "class LMModel1(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz,n_hidden) # for input ot hidden\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden) # for hidden to hidden\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz) # for hidden to output\n",
        "\n",
        "  def forward(self,x):\n",
        "    h =F.relu(self.h_h(self.i_h(x[:,0])))\n",
        "    h = h + self.i_h(x[:,1])\n",
        "    h = F.relu(self.h_h(h))\n",
        "    h = h + self.i_h(x[:,2])\n",
        "    h = F.relu(self.h_h(h))\n",
        "    return self.h_o(h)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L7NoJIQlYeR",
        "outputId": "a218ac63-7cd2-4da4-c218-0d71d1db219f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.824297</td>\n",
              "      <td>1.970941</td>\n",
              "      <td>0.467554</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.386973</td>\n",
              "      <td>1.823242</td>\n",
              "      <td>0.467554</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.417556</td>\n",
              "      <td>1.654497</td>\n",
              "      <td>0.494414</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.376440</td>\n",
              "      <td>1.650849</td>\n",
              "      <td>0.494414</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTn0GrUFl8zd"
      },
      "source": [
        "to check if this model is any good,lets find the most common token for target in validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2aaokuFjHxQ",
        "outputId": "86d97e14-2e8e-49f1-ade4-6224822ce30e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n, counts = 0, torch.zeros(len(vocab))\n",
        "for x, y in dls.valid:\n",
        "  n += y.shape[0]\n",
        "  for i in range_of(vocab): counts[i] += (y==i).long().sum()\n",
        "idx = torch.argmax(counts)\n",
        "idx, vocab[idx.item()], counts[idx].item()/n\n",
        "\n",
        "# it might have been . but it seems thousand is used a lot\n",
        "\n",
        "# this is the baseline model lets try refactor it with a loop"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(29), 'thousand', 0.15165200855716662)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ExT5IRlmzQr"
      },
      "source": [
        "# we can replace the duplicated code with a for loop\n",
        "\n",
        "class LMModel2(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden,n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = 0 \n",
        "    for i in range(3):\n",
        "      h = h+ self.i_h(x[:,i])\n",
        "      h = F.relu(self.h_h(h))\n",
        "    return self.h_o(h)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqTqnbrtnuff",
        "outputId": "77637616-244c-4e8e-8c18-22a51e1c90b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func = F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.816274</td>\n",
              "      <td>1.964143</td>\n",
              "      <td>0.460185</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.423805</td>\n",
              "      <td>1.739964</td>\n",
              "      <td>0.473259</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.430327</td>\n",
              "      <td>1.685172</td>\n",
              "      <td>0.485382</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.388390</td>\n",
              "      <td>1.657033</td>\n",
              "      <td>0.470406</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kahYRapPoHPl"
      },
      "source": [
        "we see that there is a set of activations that are being updated each time through the loop stored in variable h - called the hidden state\n",
        "\n",
        "A neural network that is defined using a loop is called RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL59fefFoYoP"
      },
      "source": [
        "## Improving the RNN\n",
        "\n",
        "looking at the code for our RNN one thing that seem problemating is that we are initialising them to 0\n",
        "\n",
        "But if we order the samples correctly, those samplesequence will be read in order by the model exposing the model to ong stretches of the original sequence\n",
        "\n",
        "another thing to look at is having more signal: which just predict the fourth word when we can also predict the second and third words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4IUR1aWquJw"
      },
      "source": [
        "## Maintaining the state of RNN\n",
        "\n",
        "we can remove initialising to zero and simply create another function for init\n",
        "\n",
        "this will create another problem our neural network then will be as deep as the document. \n",
        "\n",
        "for example if there were 10,000 tokens in our dataset we would be creating a 10,000 layer neural network\n",
        "\n",
        "this is because if we dont initialise all the loops stack on top of each other\n",
        "\n",
        "the problem with 10,000 layers is that when you get to 10,000th word you stillhave to calculate the derivatives all the way to first layer\n",
        "\n",
        "It is unlikely that you will be ableto store even one minibatch of GPU\n",
        "\n",
        "The solution of this is to tell PyTorch that we do not want to back propagate the entire neural network, instead just keep last three layers of gradients.\n",
        "\n",
        "we use `detach` to achieve this\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_2tDYzQoWOL"
      },
      "source": [
        "class LMModel3(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz,n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden,n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "    self.h = 0\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i in range(3):\n",
        "      self.h = self.h + self.i_h(x[:,i])\n",
        "      self.h = F.relu(self.h_h(self.h))\n",
        "    out = self.h_o(self.h)\n",
        "    self.h = self.h.detach()\n",
        "    return out\n",
        "  \n",
        "  def reset(self):\n",
        "    self.h = 0\n",
        "\n",
        "    # this model will have the same activations whatever the sequence length we pick because the hidden state will remmeber the last activation from previous batch\n",
        "    # because of the hidden state only thing that willbe different is that gradient computed at each step\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTEDMXrttVdi"
      },
      "source": [
        "this approach is called BPTT: back propagationthrough time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o5LoOsOpB6y"
      },
      "source": [
        "earlier we were using LMDataloaders to ensure that samples are goingone after the other this time we will do it urselves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OUnShdHsxeH",
        "outputId": "f1bd6d23-4202-4d38-c11c-8b4dbd2d6cb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "m = len(seqs)//bs\n",
        "m,bs,len(seqs)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(328, 64, 21031)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnkXAc5wpUoa"
      },
      "source": [
        "def group_chunks(ds, bs):\n",
        "  m = len(ds)//bs\n",
        "  new_ds = L()\n",
        "  for i in range(m): new_ds += L(ds[i+m*j] for j in range(bs))\n",
        "  return new_ds"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6192SeCpshF"
      },
      "source": [
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(\n",
        "    group_chunks(seqs[:cut], bs),\n",
        "    group_chunks(seqs[cut:], bs),\n",
        "    bs = bs, drop_last=True, shuffle=False\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBsFd6N0qF-A",
        "outputId": "b12e273f-d26d-45bb-8236-c4e612d40047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# we will use a little tweak using thetraining lopp via a callback\n",
        "\n",
        "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(10,3e-3)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.677074</td>\n",
              "      <td>1.827367</td>\n",
              "      <td>0.467548</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.282722</td>\n",
              "      <td>1.870913</td>\n",
              "      <td>0.388942</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.090705</td>\n",
              "      <td>1.651793</td>\n",
              "      <td>0.462500</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.005092</td>\n",
              "      <td>1.613794</td>\n",
              "      <td>0.516587</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.965975</td>\n",
              "      <td>1.560775</td>\n",
              "      <td>0.551202</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.916182</td>\n",
              "      <td>1.595857</td>\n",
              "      <td>0.560577</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.897657</td>\n",
              "      <td>1.539733</td>\n",
              "      <td>0.574279</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.836274</td>\n",
              "      <td>1.585141</td>\n",
              "      <td>0.583173</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.805877</td>\n",
              "      <td>1.629808</td>\n",
              "      <td>0.586779</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.795096</td>\n",
              "      <td>1.651267</td>\n",
              "      <td>0.588942</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFWUCTKurBA9"
      },
      "source": [
        "## Creating more signal\n",
        "\n",
        "using more targets andcomparing them for intermediate predictions\n",
        "\n",
        "we will predict enxt word for every single word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRU2M9Dqr0N"
      },
      "source": [
        "# instead of 3 we will use 16 length sequence\n",
        "sl = 16\n",
        "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl))\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut],bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False )"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky4y9wC-sKw2",
        "outputId": "3d0ef8e6-b7d5-4e1b-bbab-a0046d0d4572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# looking at the first element of the sequence\n",
        "\n",
        "[L(vocab[o] for o in s) for s in seqs[0]]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n",
              " (#16) ['.','two','.','three','.','four','.','five','.','six'...]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmuWij2PsgLZ"
      },
      "source": [
        "class LMModel4(Module):\n",
        "  def __init__(self,vocab_sz,n_hidden):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden,n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "    self.h = 0\n",
        "\n",
        "  def forward(self, x):\n",
        "    outs = []\n",
        "    for i in range(sl):\n",
        "      self.h = self.h + self.i_h(x[:,i])\n",
        "      self.h = F.relu(self.h_h(self.h))\n",
        "      outs.append(self.h_o(self.h))\n",
        "    self.h = self.h.detach()\n",
        "    return torch.stack(outs, dim=1)\n",
        "  \n",
        "  def reset(self):\n",
        "    self.h = 0"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s7Edd49tltH"
      },
      "source": [
        "def loss_func(inp, targ):\n",
        "  return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYdL6eNit2D5",
        "outputId": "b72b6226-1de4-465f-9c19-49b96d76d12f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics = accuracy, cbs = ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.285931</td>\n",
              "      <td>3.072032</td>\n",
              "      <td>0.212565</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.330371</td>\n",
              "      <td>1.969522</td>\n",
              "      <td>0.425781</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.742317</td>\n",
              "      <td>1.841378</td>\n",
              "      <td>0.441488</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.470120</td>\n",
              "      <td>1.810857</td>\n",
              "      <td>0.494303</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.297998</td>\n",
              "      <td>1.867457</td>\n",
              "      <td>0.480062</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.175093</td>\n",
              "      <td>1.773528</td>\n",
              "      <td>0.501709</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.071012</td>\n",
              "      <td>1.714759</td>\n",
              "      <td>0.512044</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.976119</td>\n",
              "      <td>1.711606</td>\n",
              "      <td>0.545573</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.888830</td>\n",
              "      <td>1.707983</td>\n",
              "      <td>0.562663</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.821714</td>\n",
              "      <td>1.632179</td>\n",
              "      <td>0.581787</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.773912</td>\n",
              "      <td>1.706030</td>\n",
              "      <td>0.586426</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.744912</td>\n",
              "      <td>1.740803</td>\n",
              "      <td>0.586751</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.708750</td>\n",
              "      <td>1.807236</td>\n",
              "      <td>0.591960</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.690133</td>\n",
              "      <td>1.810613</td>\n",
              "      <td>0.580892</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.677138</td>\n",
              "      <td>1.752561</td>\n",
              "      <td>0.602132</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhwR-jM7ucRw"
      },
      "source": [
        "## Multi layer RNNs\n",
        "\n",
        "for multi layer RNN we pass the activations from our recurrent network into a second reco=urrent pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n8CNe16um3I"
      },
      "source": [
        "class LMModel5(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden,n_layers):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = torch.zeros(n_layers, bs, n_hidden)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    res,h = self.rnn(self.i_h(x), self.h) # for multi layer ?\n",
        "    self.h = h.detach()\n",
        "    return self.h_o(res)\n",
        "  \n",
        "  def reset(self):\n",
        "    self.h.zero_()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AygTawmcv4B5",
        "outputId": "b27c3c7e-8239-408b-d872-f1c8d134f11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "learn = Learner(dls, LMModel5(len(vocab), 64,2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cb=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.041790</td>\n",
              "      <td>2.549923</td>\n",
              "      <td>0.455729</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.128606</td>\n",
              "      <td>1.716949</td>\n",
              "      <td>0.469808</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.700232</td>\n",
              "      <td>1.870062</td>\n",
              "      <td>0.340169</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.513695</td>\n",
              "      <td>1.767692</td>\n",
              "      <td>0.423584</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.358153</td>\n",
              "      <td>1.714393</td>\n",
              "      <td>0.503906</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.229992</td>\n",
              "      <td>1.793889</td>\n",
              "      <td>0.510173</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.110304</td>\n",
              "      <td>1.937699</td>\n",
              "      <td>0.532959</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.000071</td>\n",
              "      <td>2.018610</td>\n",
              "      <td>0.544759</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.907561</td>\n",
              "      <td>1.978048</td>\n",
              "      <td>0.553874</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.829595</td>\n",
              "      <td>2.016860</td>\n",
              "      <td>0.560954</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.770132</td>\n",
              "      <td>1.998807</td>\n",
              "      <td>0.581055</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.727698</td>\n",
              "      <td>1.989101</td>\n",
              "      <td>0.584798</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.700424</td>\n",
              "      <td>1.968074</td>\n",
              "      <td>0.588623</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.683205</td>\n",
              "      <td>1.949744</td>\n",
              "      <td>0.590739</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.674146</td>\n",
              "      <td>1.955959</td>\n",
              "      <td>0.592936</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI5INoctwu1t"
      },
      "source": [
        "## Exploding and disappearing activation\n",
        "\n",
        "the accuracy is less than prevoius accuracy\n",
        "\n",
        "this happens when you multiply a number by a number slightly more than 1 or less than 1 then the number cimpletely dissapear or explides.\n",
        "\n",
        "since computer use float the bumber becaomes more inaccurate as we move outside zero.\n",
        "\n",
        "inorder to reduce it we use LSTM memory and GRU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nclaNwB2xrUw"
      },
      "source": [
        "## LSTM\n",
        "\n",
        "Long short term memory. it has two hidden states one the normal one that we are using :\n",
        "\n",
        "Having the right information for the output layer and retaining memory for everything that happened in the sentence. \n",
        "\n",
        "However as it turns out RNNs are not very good at reatining memory thats why theyintroduced a hidden memory long short term memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN_69LigwsHN"
      },
      "source": [
        "class LSTMCell(Module):\n",
        "  def __init__(self, ni,nh):\n",
        "    self.forget_gate = nn.Linear(ni + nh,nh)\n",
        "    self.input_gate = nn.Linear(ni + nh, nh)\n",
        "    self.cell_gate = nn.Linear(ni + nh, nh)\n",
        "    self.output_gate = nn.Linear(ni +nh,nh)\n",
        "\n",
        "  def forward(self,input, state):\n",
        "    h,c = static\n",
        "    h = torch.stack([h, input], dim=1)\n",
        "    forget = torch.sigmoid(sef.forget_gate(h))\n",
        "    c = c * forget\n",
        "    inp = torch.sigmoid(self.input_gate(h))\n",
        "    cell = torch.tanh(self.cell_gate(h))\n",
        "    c = c + inp * cell \n",
        "    out = torch.sigmoid(self.output_gate(h))\n",
        "    h = outgate * torch.tanh(c)\n",
        "    return h, (h,c)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJSPhG3yuU5-"
      },
      "source": [
        "#refactoring\n",
        "\n",
        "class LSTMCell(Module):\n",
        "    def __init__(self, ni, nh):\n",
        "        self.ih = nn.Linear(ni,4*nh)\n",
        "        self.hh = nn.Linear(nh,4*nh)\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        h,c = state\n",
        "        # One big multiplication for all the gates is better than 4 smaller ones\n",
        "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n",
        "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
        "        cellgate = gates[3].tanh()\n",
        "\n",
        "        c = (forgetgate*c) + (ingate*cellgate)\n",
        "        h = outgate * c.tanh()\n",
        "        return h, (h,c)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npZpHZDN_VSz",
        "outputId": "a108ba78-b81a-4de2-f619-74d11a856e5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "t = torch.arange(0,10); t"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyPUmmXx_bMV",
        "outputId": "487b1faa-a84a-4419-b604-d32f278424f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "t.chunk(2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEG1DFOv_g6g"
      },
      "source": [
        "class LMModel6(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "    self.i_h = nn.Embedding(vocab_sz,n_hidden)\n",
        "    self.rnn = nn.LSTM(n_hidden,n_hidden,n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = [torch.zeros(n_layers,bs, n_hidden) for _ in range(2)]\n",
        "\n",
        "  def forward(self, x):\n",
        "    res,h = self.rnn(self.i_h(x), self.h)\n",
        "    self.h = [h_.detach() for h_ in h]\n",
        "    return self.h_o(res)\n",
        "  \n",
        "  def reset(self):\n",
        "    for h in self.h:\n",
        "      h.zero_()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1ejVtDMAWxi",
        "outputId": "79ba59da-6a08-4efa-c259-23c6d8892714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "learn = Learner(dls, LMModel6(len(vocab), 64,2),\n",
        "                 loss_func=CrossEntropyLossFlat(),\n",
        "                 metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 1e-2)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.026113</td>\n",
              "      <td>2.772102</td>\n",
              "      <td>0.153076</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.216185</td>\n",
              "      <td>2.089064</td>\n",
              "      <td>0.269124</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.613937</td>\n",
              "      <td>1.826248</td>\n",
              "      <td>0.478760</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.316856</td>\n",
              "      <td>2.086804</td>\n",
              "      <td>0.501546</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.093758</td>\n",
              "      <td>2.008277</td>\n",
              "      <td>0.610026</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.854747</td>\n",
              "      <td>1.814557</td>\n",
              "      <td>0.663005</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.612584</td>\n",
              "      <td>1.935545</td>\n",
              "      <td>0.707845</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.421090</td>\n",
              "      <td>1.706476</td>\n",
              "      <td>0.743490</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.274809</td>\n",
              "      <td>1.597815</td>\n",
              "      <td>0.762777</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.180654</td>\n",
              "      <td>1.717633</td>\n",
              "      <td>0.791585</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.121550</td>\n",
              "      <td>1.833439</td>\n",
              "      <td>0.786214</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.085148</td>\n",
              "      <td>1.853766</td>\n",
              "      <td>0.797282</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.062242</td>\n",
              "      <td>1.839967</td>\n",
              "      <td>0.798747</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.048723</td>\n",
              "      <td>1.848449</td>\n",
              "      <td>0.802002</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.042393</td>\n",
              "      <td>1.861080</td>\n",
              "      <td>0.801351</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzT0ORwqA7xC"
      },
      "source": [
        "## Regularisation \n",
        "using dropouts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XV74tkyAubB"
      },
      "source": [
        "class Dropout(Module):\n",
        "  def __init(self, p):\n",
        "    self.p = p\n",
        "  def forward(self, x):\n",
        "    if not self.training:\n",
        "      return x\n",
        "    mask = x.new(*x.shape).bernoulli_(1-p)\n",
        "    return x * mask.div_(1-p)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NfX2s4BiDe"
      },
      "source": [
        "## Training a weight tird Regularized LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaOTDmUgBg6v"
      },
      "source": [
        "# combining dropout with Activation Regularisation and Temporal Regularisation\n",
        "\n",
        "class LMModel7(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
        "    self.i_h = nn.Embedding(vocab_sz,n_hidden)\n",
        "    self.rnn = nn.LSTM(n_hidden,n_hidden, n_layers, batch_first=True)\n",
        "    self.drop = nn.Dropout(p)\n",
        "    self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "    self.h_o.weight = self.i_h.weight\n",
        "    self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "  \n",
        "  def forward(self, x):\n",
        "    raw, h = self.rnn(self.i_h(x), self.h)\n",
        "    out = self.drop(raw)\n",
        "    self.h = [h_.detach() for h_ in h]\n",
        "    return self.h_o(out), raw, out\n",
        "  \n",
        "  def reset(self):\n",
        "    for h in self.h:\n",
        "      h.zero_()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Z7s7qfCw5I"
      },
      "source": [
        "learn = Learner(dls, LMModel7(len(vocab), 64, 2,0.5), loss_func=CrossEntropyLossFlat(),\n",
        "                 metrics=accuracy, cbs=[ModelResetter , RNNRegularizer(alpha=2, beta=1)])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQtNomOSDLu0"
      },
      "source": [
        "# same as \n",
        "\n",
        "learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n",
        "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-uhFeswDQvr",
        "outputId": "46d76583-8b11-4d56-9603-64b64479e432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "learn.fit_one_cycle(15, 1e-2, wd=0.1)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.491892</td>\n",
              "      <td>1.932190</td>\n",
              "      <td>0.512288</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.591766</td>\n",
              "      <td>1.156541</td>\n",
              "      <td>0.644287</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.912455</td>\n",
              "      <td>0.704656</td>\n",
              "      <td>0.785889</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.539007</td>\n",
              "      <td>0.689711</td>\n",
              "      <td>0.796468</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.365283</td>\n",
              "      <td>0.531121</td>\n",
              "      <td>0.840658</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.255364</td>\n",
              "      <td>0.517289</td>\n",
              "      <td>0.843587</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.202057</td>\n",
              "      <td>0.422045</td>\n",
              "      <td>0.871419</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.170183</td>\n",
              "      <td>0.422867</td>\n",
              "      <td>0.876546</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.148466</td>\n",
              "      <td>0.441083</td>\n",
              "      <td>0.866618</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.136160</td>\n",
              "      <td>0.428933</td>\n",
              "      <td>0.872721</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.125077</td>\n",
              "      <td>0.402033</td>\n",
              "      <td>0.881917</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.116373</td>\n",
              "      <td>0.386478</td>\n",
              "      <td>0.881348</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.110172</td>\n",
              "      <td>0.390179</td>\n",
              "      <td>0.883789</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.105373</td>\n",
              "      <td>0.392722</td>\n",
              "      <td>0.882487</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.103069</td>\n",
              "      <td>0.389774</td>\n",
              "      <td>0.884440</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}