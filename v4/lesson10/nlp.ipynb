{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-pO2oZXe2pv",
        "outputId": "c5946bf9-0c10-4c1e-e56e-0a55b16e55e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "try:\n",
        "  fastbook.setup_book()\n",
        "except:\n",
        "  print(\"No password entered\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 727kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 53.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 14.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 10.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 55.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No password entered\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppki7EByfWIz"
      },
      "source": [
        "from fastbook import *\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I88mGp6k43ms"
      },
      "source": [
        "## NLP introduction\n",
        "\n",
        "nlp is about guessing the next word. for that the model needs to create its own labels. It uses self supervised learning.\n",
        "\n",
        "The language model used to classify IMDb was pretrained on Wikipedia. \n",
        "\n",
        "why learn in detail ?\n",
        "\n",
        "One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine-tune the (sequence-based) language model prior to fine-tuning the classification model\n",
        "\n",
        "We will be finetuning the pretrained language model which was trained on wikipedia articles.\n",
        "\n",
        "This is called ULMFit approach.\n",
        "\n",
        "### text preprocessing\n",
        "\n",
        "we already know how categorical variables can be used as independent variables for neural network\n",
        "\n",
        "make a list of all possible levels of the variable (vocab)\n",
        "\n",
        "Replace each level with its index in the vocab\n",
        "\n",
        "create an embedding matrix for this contatining a row for each level i.e. for each item in the vocab\n",
        "\n",
        "Use this embedding matrix as the first layer of a neural network.\n",
        "\n",
        "we do the same thing with text. \n",
        "\n",
        "whats new is idea of sequence. first we concatenate all of the document in our dataset into onw big long string and split it into words giving us very long words or tokens. \n",
        "\n",
        "Our independent variable will be the entire string exect for the second last and last will be labe;\n",
        "\n",
        "Our vocab would be mix of common words from wikipedia and new words specific to our corpus would be movie actors\n",
        "\n",
        "for building embedding matrix: for words in vocabualary of pre trained modelwe will take corresponding row in the embedding matrix of the pretrained model but for new words we won't have anythong, we willjust initialize the corresponding row with a random vector\n",
        "\n",
        "jargon\n",
        "\n",
        "tokenisation\n",
        "\n",
        "Numericalisations - making list of unique words -vocab and convert each word to index to look up in vocab\n",
        "\n",
        "Language model data loader creation -  LMDDataLoader class for seperating the last token as label\n",
        "\n",
        "Language model creation - creating a model that handles the input list that are arbitaryily small or big."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw0ysUpNMhQz"
      },
      "source": [
        "### Tokenisation\n",
        "\n",
        "converting to words\n",
        " word based - split a sentence on spaces and apply language specific rules to try to seperate parts of meaning even when there are no spaces\n",
        "\n",
        " subword - word is split like occ aa sion\n",
        "\n",
        " Character based - individual character\n",
        "\n",
        " with fastai word tokenisation is done through exernal API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zltl7zWxfssL",
        "outputId": "bd9b2d1e-cf1c-44c0-aa43-08ff089a3bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.IMDB)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsT8CSRKNfAC"
      },
      "source": [
        "need to gradb the text file, like get_imagefiles we have get_text_file.\n",
        "\n",
        "we can also pass folder to restrict the search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72cCnGYjxCZR",
        "outputId": "0ad49371-744f-4fa4-f05c-d2340015f0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!dir {path}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdb.vocab  README  test  tmp_clas  tmp_lm  train  unsup\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiLVdGxANWcn"
      },
      "source": [
        "files = get_text_files(path, folders=['train', 'test', 'unsup'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vupmfimgNzBj",
        "outputId": "004aa3f4-a03e-4cb8-c99e-1f004cc5ff7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "txt = files[0].open().read()\n",
        "txt[:75]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hmmmm, interesting. I'll keep this short on detail, as so many have done su\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCeII5zIOAkz"
      },
      "source": [
        "fastai by default used spacyTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgg1fu18N6-1",
        "outputId": "fcef5f73-ca24-4ddd-faad-3a66e42158eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "spacy = WordTokenizer()\n",
        "toks = first(spacy([txt]))\n",
        "print(toks)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hmmmm', ',', 'interesting', '.', 'I', \"'ll\", 'keep', 'this', 'short', 'on', 'detail', ',', 'as', 'so', 'many', 'have', 'done', 'such', 'a', 'good', 'job', 'of', 'pointing', 'out', 'the', 'myriad', 'problems', 'with', 'this', 'series', 'both', 'of', 'itself', 'and', 'as', 'a', 'laughably', 'awful', 'remake', 'of', 'a', 'good', 'original', 'series', '.', 'Ludicrously', 'similar', 'to', 'the', '(', 'usually', ')', 'teen', '-', 'aged', 'modern', '\"', 'role', 'playing', 'game', '\"', 'fans', 'who', 'constantly', 'talked', 'of', 'the', 'lack', 'of', '\"', 'darkness', '\"', 'in', 'some', 'game', 'they', 'were', 'involved', 'in', 'a', 'decade', 'past', ',', 'but', 'themselves', 'had', 'so', 'little', 'experience', 'of', '\"', 'DAHKNESSSSSSS', '\"', 'that', 'they', \"'d\", 'have', 'shrieked', 'and', 'run', 'not', 'only', 'from', 'anything', 'remotely', 'like', 'one', 'of', 'the', 'creepy', 'crawlies', '(', 'usually', 'human', 'appearing', ',', 'but', 'horrific', 'in', 'some', 'psychological', 'or', 'spiritual', 'way', ')', 'they', 'wanted', 'featured', 'and', 'emphasized', 'into', 'the', 'ground', 'but', 'also', 'shielded', 'and', 'run', 'or', 'simply', 'self', '-', 'destructed', 'from', 'any', 'realistic', 'darkness', 'that', 'had', 'presented', 'itself', 'into', 'their', 'unthinking', 'little', 'lives', '.', 'Or', 'perhaps', 'the', 'equally', 'silly', 'conviction', 'of', 'the', 'teen', '-', 'aged', '\"', 'goths', '\"', ';', 'kids', 'asserting', 'the', 'world', 'had', 'already', 'gone', 'to', 'hell', 'in', 'its', 'very', 'own', 'handbasket', 'and', 'wearing', 'black', 'and', 'being', 'morose', 'was', 'utterly', 'original', 'and', 'beautifully', '\"', 'realistic', '\"', 'and', 'anyone', 'who', 'did', 'not', 'agree', 'just', 'did', \"n't\", 'understand', '(', 'and', 'was', 'really', 'dumb', 'besides', ')', '.', 'Come', 'to', 'think', 'of', 'it', ',', 'they', 'had', 'the', 'the', 'same', 'rabid', 'reaction', 'to', 'anyone', 'pointing', 'out', 'the', 'fact', 'that', 'the', 'last', 'few', 'generations', 'have', 'also', 'had', 'a', 'cadre', 'of', 'black', '-', 'wearing', '\"', 'originalists', '\"', 'in', 'high', 'school', '...', 'LOL!!!<br', '/><br', '/>The', 'single', 'detailed', 'comment', 'I;d', 'like', 'to', 'make', 'for', 'those', 'seeking', 'enough', 'info', 'to', 'decide', 'on', 'watching', 'or', 'not', 'watching', 'the', '\"', 'new', '\"', 'series', 'involves', 'a', 'couple', 'of', 'subtle', 'concepts', 'called', 'family', 'and', 'wisdom', '(', 'or', 'alternatively', ',', 'wise', 'leadership', ')', '.', 'The', 'original', 'series', 'included', 'truly', 'adult', 'themes', 'such', 'as', 'family', 'and', 'friendship', ';', 'it', 'is', 'mystifying', 'to', 'those', 'actually', '\"', 'adult', '\"', 'as', 'to', 'why', 'explicit', 'sexual', 'intercourse', ',', 'drug', 'use', ',', 'and', 'foul', 'language', 'is', 'supposed', 'to', 'depict', 'adulthood', '.', 'The', 'original', 'series', 'contained', 'such', 'scenes', 'as', 'an', 'not', '-', 'seriously', '-', 'but', '-', 'still', '-', 'ill', 'Adama', 'in', 'bed', 'receiving', 'a', 'visit', 'from', 'his', 'small', 'grandson', '-', 'by', '-', 'marriage', 'Boxey', ',', 'who', ',', 'tucked', 'up', 'beside', 'his', 'grandfather', 'and', 'soundly', 'kissed', ',', 'proceeds', 'to', 'tell', 'Adama', 'a', 'small', '-', 'boy', 'version', 'of', 'his', 'own', 'favorite', 'bedtime', 'story.<br', '/><br', '/>\"There', 'once', 'was', 'a', 'shining', 'planet', ',', 'called', '...', '\"', '\"', 'Earth', ',', '\"', 'says', 'Adama', ',', 'recognizing', 'the', 'beginning', 'line', 'of', 'the', 'story', 'he', \"'s\", 'often', 'told', 'the', 'child', '.', '\"', 'No', ',', 'Mushieland', '(', 'a', 'type', 'of', 'sweet', 'in', 'the', 'series', ')', ',', '\"', 'replies', 'Boxey', '.', '\"', 'Mushieland', '?', '\"', 'asks', 'Adama', ',', 'voice', 'and', 'expression', 'exactly', 'what', 'you', \"'d\", 'expect', 'from', 'an', 'affectionate', 'grandparent', 'both', 'a', 'little', 'surprised', 'and', 'offering', 'encouragement', 'to', 'go', 'on', '.', '\"', 'It', 'was', 'full', 'of', 'daggets', '(', 'essentially', 'an', 'extraterrestial', 'dog', ')', ',', '\"', 'continues', 'Boxey', ',', '\"', 'but', 'the', 'best', 'of', 'them', 'all', 'was', 'Sire', 'Muffie', '(', 'the', 'name', 'of', 'his', 'own', 'pet', ')', '.', '\"', '\"', 'Sire', 'Muffie', ',', '\"', 'Adama', 'repeats', ',', 'clearly', 'understanding', 'where', 'this', 'is', 'going', ',', 'and', 'as', 'clearly', 'content', ',', 'happy', 'with', 'the', 'boy', \"'s\", 'company', '.', 'Again', ',', 'very', 'much', 'real', 'world', ',', 'real', 'grandparent.<br', '/><br', '/>A', 'moment', 'later', 'the', 'camera', 'shows', 'the', 'medical', 'technician', 'who', \"'s\", 'been', 'keeping', 'visitors', 'away', 'so', 'Adama', 'can', 'rest', '...', 'standing', 'just', 'out', 'of', 'sight', 'listening', ',', 'smiling', '.', 'She', \"'s\", 'pretended', 'to', 'be', 'asleep', 'so', 'the', 'boy', 'could', 'sneak', 'in', 'and', 'give', 'the', 'old', 'man', 'some', 'of', 'the', 'medicine', 'he', 'needs', 'most', 'right', 'then.<br', '/><br', '/>This', 'is', 'the', 'stuff', 'of', 'reality', '.', 'This', 'is', 'adulthood', '.', 'All', 'the', '\"', 'F', '&', 'S', '\"', '(', 'reproductive', 'activity', 'and', 'drug', 'sucking', ')', 'ever', 'shown', 'can', 'not', 'trump', 'such', 'things', 'on', 'those', 'two', 'counts', '.', 'It', 'shows', 'when', 'Adama', 'and', 'Apollo', '--his', 'son--', 'embrace', 'after', 'Apollo', 'has', 'been', 'out', 'on', 'a', 'dangerous', 'mission', '.', 'It', 'shows', 'when', 'Starbuck', ',', 'all', 'choked', 'up', 'and', 'having', 'trouble', 'speaking', ',', 'sincerely', 'tells', 'the', 'heavily', 'advanced', 'civilization', 'of', 'the', 'Ship', 'of', 'Lights', 'who', 'just', 'revived', 'his', 'dead', 'best', 'friend', 'Apollo', '\"', 'Whatever', 'you', 'want', 'from', 'me', ',', 'you', 'can', 'have', '\"', '.', 'It', 'shows', 'in', 'the', 'not', '-', 'too', '-', 'often', ',', 'not', '-', 'too', '-', 'little', 'use', 'of', 'the', 'term', '\"', 'buddy', '\"', 'between', 'Starbuck', 'and', 'Apollo', 'and', 'Boomer', 'and', 'the', 'trio', 'handclasp', 'colonial', 'warriors', 'use', 'now', 'and', 'then', 'at', 'a', 'difficult', 'or', 'dangerous', 'moment', '.', 'It', 'shows', 'in', 'the', 'trust', 'evidenced', 'across', 'rank', 'lines', ',', 'generation', 'lines', ',', 'and', 'gender', 'lines', 'by', 'people', 'who', 'have', 'worked', 'together', ',', 'understand', 'each', 'other', \"'s\", 'strengths', 'and', 'weaknesses', ',', 'and', 'are', 'committed', 'to', 'each', 'other', 'and', 'to', 'a', 'goal', 'worth', 'reaching', 'out', 'for.<br', '/><br', '/>Adama', 'led', 'so', 'well', 'precisely', 'because', 'he', 'showed', 'emotion', ',', 'valued', 'his', 'family', ',', 'worried', 'sometimes', 'intensely', 'but', 'let', 'others', 'live', 'their', 'lives', ',', 'cared', 'and', 'managed', 'situations', 'with', 'flexibility', 'but', 'brooked', 'no', 'nonsense', ',', 'and', 'was', 'fundamentally', 'an', 'older', 'human', 'being', 'saddened', 'by', 'pain', 'but', 'capable', 'of', 'happiness', ',', 'flawed', '(', 'without', 'being', 'a', 'mess', ')', 'but', 'striving', 'to', 'be', 'the', 'best', 'he', 'could', '.', 'This', ',', 'too', ',', 'is', 'reality', '.', 'There', 'are', 'bad', 'people', 'in', 'this', 'world', ',', 'but', 'the', 'sort', 'of', 'disintegration', 'into', 'whoring', ',', 'in', '-', 'fighting', ',', 'and', 'drugs', 'touted', 'as', '\"', 'real', '\"', 'by', 'such', 'shows', 'as', 'this', 'ridiculous', 'new', 'rip', 'off', 'are', 'unreal', 'because', 'they', 'show', 'only', 'the', 'worst', 'case', 'most', 'extreme', 'scenario', '.', '(', 'Come', 'on', ',', 'how', 'many', 'disasters', 'have', 'we', 'read', 'about', ',', 'seen', ',', 'or', 'been', 'in', 'ourselves', 'in', 'which', 'EVERYONE', 'turned', 'into', 'a', 'sick', '-', 'minded', 'nit?)<br', '/><br', '/>My', 'advice', '?', 'Go', 'for', 'the', 'real', 'thing', ';', 'ignore', 'the', 'fake', 'stuff', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTeeYsSwOmHW",
        "outputId": "7d297098-0bbc-4587-d908-b930f4135624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "first(spacy(['The U.S. dollar $1 is $1.00']))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#8) ['The','U.S.','dollar','$','1','is','$','1.00']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOypO_s_QiJr",
        "outputId": "cd2003ce-8faf-4f80-b145-c2674ae70da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# some additional functionality is added to Tokenizer class\n",
        "\n",
        "tkn = Tokenizer(spacy)\n",
        "\n",
        "print(tkn(txt))\n",
        "\n",
        "# xxbos indicates beginning of the stream"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['xxbos', 'h', 'xxrep', '4', 'm', ',', 'interesting', '.', 'xxmaj', 'i', \"'ll\", 'keep', 'this', 'short', 'on', 'detail', ',', 'as', 'so', 'many', 'have', 'done', 'such', 'a', 'good', 'job', 'of', 'pointing', 'out', 'the', 'myriad', 'problems', 'with', 'this', 'series', 'both', 'of', 'itself', 'and', 'as', 'a', 'laughably', 'awful', 'remake', 'of', 'a', 'good', 'original', 'series', '.', 'xxmaj', 'ludicrously', 'similar', 'to', 'the', '(', 'usually', ')', 'teen', '-', 'aged', 'modern', '\"', 'role', 'playing', 'game', '\"', 'fans', 'who', 'constantly', 'talked', 'of', 'the', 'lack', 'of', '\"', 'darkness', '\"', 'in', 'some', 'game', 'they', 'were', 'involved', 'in', 'a', 'decade', 'past', ',', 'but', 'themselves', 'had', 'so', 'little', 'experience', 'of', '\"', 'dahkne', 'xxrep', '7', 's', '\"', 'that', 'they', \"'d\", 'have', 'shrieked', 'and', 'run', 'not', 'only', 'from', 'anything', 'remotely', 'like', 'one', 'of', 'the', 'creepy', 'crawlies', '(', 'usually', 'human', 'appearing', ',', 'but', 'horrific', 'in', 'some', 'psychological', 'or', 'spiritual', 'way', ')', 'they', 'wanted', 'featured', 'and', 'emphasized', 'into', 'the', 'ground', 'but', 'also', 'shielded', 'and', 'run', 'or', 'simply', 'self', '-', 'destructed', 'from', 'any', 'realistic', 'darkness', 'that', 'had', 'presented', 'itself', 'into', 'their', 'unthinking', 'little', 'lives', '.', 'xxmaj', 'or', 'perhaps', 'the', 'equally', 'silly', 'conviction', 'of', 'the', 'teen', '-', 'aged', '\"', 'goths', '\"', ';', 'kids', 'asserting', 'the', 'world', 'had', 'already', 'gone', 'to', 'hell', 'in', 'its', 'very', 'own', 'handbasket', 'and', 'wearing', 'black', 'and', 'being', 'morose', 'was', 'utterly', 'original', 'and', 'beautifully', '\"', 'realistic', '\"', 'and', 'anyone', 'who', 'did', 'not', 'agree', 'just', 'did', \"n't\", 'understand', '(', 'and', 'was', 'really', 'dumb', 'besides', ')', '.', 'xxmaj', 'come', 'to', 'think', 'of', 'it', ',', 'they', 'had', 'the', 'the', 'same', 'rabid', 'reaction', 'to', 'anyone', 'pointing', 'out', 'the', 'fact', 'that', 'the', 'last', 'few', 'generations', 'have', 'also', 'had', 'a', 'cadre', 'of', 'black', '-', 'wearing', '\"', 'originalists', '\"', 'in', 'high', 'school', '…', 'lol', 'xxrep', '3', '!', '\\n\\n', 'xxmaj', 'the', 'single', 'detailed', 'comment', 'xxmaj', 'i;d', 'like', 'to', 'make', 'for', 'those', 'seeking', 'enough', 'info', 'to', 'decide', 'on', 'watching', 'or', 'not', 'watching', 'the', '\"', 'new', '\"', 'series', 'involves', 'a', 'couple', 'of', 'subtle', 'concepts', 'called', 'family', 'and', 'wisdom', '(', 'or', 'alternatively', ',', 'wise', 'leadership', ')', '.', 'xxmaj', 'the', 'original', 'series', 'included', 'truly', 'adult', 'themes', 'such', 'as', 'family', 'and', 'friendship', ';', 'it', 'is', 'mystifying', 'to', 'those', 'actually', '\"', 'adult', '\"', 'as', 'to', 'why', 'explicit', 'sexual', 'intercourse', ',', 'drug', 'use', ',', 'and', 'foul', 'language', 'is', 'supposed', 'to', 'depict', 'adulthood', '.', 'xxmaj', 'the', 'original', 'series', 'contained', 'such', 'scenes', 'as', 'an', 'not', '-', 'seriously', '-', 'but', '-', 'still', '-', 'ill', 'xxmaj', 'adama', 'in', 'bed', 'receiving', 'a', 'visit', 'from', 'his', 'small', 'grandson', '-', 'by', '-', 'marriage', 'xxmaj', 'boxey', ',', 'who', ',', 'tucked', 'up', 'beside', 'his', 'grandfather', 'and', 'soundly', 'kissed', ',', 'proceeds', 'to', 'tell', 'xxmaj', 'adama', 'a', 'small', '-', 'boy', 'version', 'of', 'his', 'own', 'favorite', 'bedtime', 'story', '.', '\\n\\n', '\"', 'there', 'once', 'was', 'a', 'shining', 'planet', ',', 'called', '…', '\"', '\"', 'earth', ',', '\"', 'says', 'xxmaj', 'adama', ',', 'recognizing', 'the', 'beginning', 'line', 'of', 'the', 'story', 'he', \"'s\", 'often', 'told', 'the', 'child', '.', '\"', 'no', ',', 'xxmaj', 'mushieland', '(', 'a', 'type', 'of', 'sweet', 'in', 'the', 'series', ')', ',', '\"', 'replies', 'xxmaj', 'boxey', '.', '\"', 'mushieland', '?', '\"', 'asks', 'xxmaj', 'adama', ',', 'voice', 'and', 'expression', 'exactly', 'what', 'you', \"'d\", 'expect', 'from', 'an', 'affectionate', 'grandparent', 'both', 'a', 'little', 'surprised', 'and', 'offering', 'encouragement', 'to', 'go', 'on', '.', '\"', 'it', 'was', 'full', 'of', 'daggets', '(', 'essentially', 'an', 'extraterrestial', 'dog', ')', ',', '\"', 'continues', 'xxmaj', 'boxey', ',', '\"', 'but', 'the', 'best', 'of', 'them', 'all', 'was', 'xxmaj', 'sire', 'xxmaj', 'muffie', '(', 'the', 'name', 'of', 'his', 'own', 'pet', ')', '.', '\"', '\"', 'sire', 'xxmaj', 'muffie', ',', '\"', 'xxmaj', 'adama', 'repeats', ',', 'clearly', 'understanding', 'where', 'this', 'is', 'going', ',', 'and', 'as', 'clearly', 'content', ',', 'happy', 'with', 'the', 'boy', \"'s\", 'company', '.', 'xxmaj', 'again', ',', 'very', 'much', 'real', 'world', ',', 'real', 'grandparent', '.', '\\n\\n', 'a', 'moment', 'later', 'the', 'camera', 'shows', 'the', 'medical', 'technician', 'who', \"'s\", 'been', 'keeping', 'visitors', 'away', 'so', 'xxmaj', 'adama', 'can', 'rest', '…', 'standing', 'just', 'out', 'of', 'sight', 'listening', ',', 'smiling', '.', 'xxmaj', 'she', \"'s\", 'pretended', 'to', 'be', 'asleep', 'so', 'the', 'boy', 'could', 'sneak', 'in', 'and', 'give', 'the', 'old', 'man', 'some', 'of', 'the', 'medicine', 'he', 'needs', 'most', 'right', 'then', '.', '\\n\\n', 'xxmaj', 'this', 'is', 'the', 'stuff', 'of', 'reality', '.', 'xxmaj', 'this', 'is', 'adulthood', '.', 'xxmaj', 'all', 'the', '\"', 'f', '&', 'xxup', 's', '\"', '(', 'reproductive', 'activity', 'and', 'drug', 'sucking', ')', 'ever', 'shown', 'can', 'not', 'trump', 'such', 'things', 'on', 'those', 'two', 'counts', '.', 'xxmaj', 'it', 'shows', 'when', 'xxmaj', 'adama', 'and', 'xxmaj', 'apollo', '--his', 'son--', 'embrace', 'after', 'xxmaj', 'apollo', 'has', 'been', 'out', 'on', 'a', 'dangerous', 'mission', '.', 'xxmaj', 'it', 'shows', 'when', 'xxmaj', 'starbuck', ',', 'all', 'choked', 'up', 'and', 'having', 'trouble', 'speaking', ',', 'sincerely', 'tells', 'the', 'heavily', 'advanced', 'civilization', 'of', 'the', 'xxmaj', 'ship', 'of', 'xxmaj', 'lights', 'who', 'just', 'revived', 'his', 'dead', 'best', 'friend', 'xxmaj', 'apollo', '\"', 'whatever', 'you', 'want', 'from', 'me', ',', 'you', 'can', 'have', '\"', '.', 'xxmaj', 'it', 'shows', 'in', 'the', 'not', '-', 'too', '-', 'often', ',', 'not', '-', 'too', '-', 'little', 'use', 'of', 'the', 'term', '\"', 'buddy', '\"', 'between', 'xxmaj', 'starbuck', 'and', 'xxmaj', 'apollo', 'and', 'xxmaj', 'boomer', 'and', 'the', 'trio', 'handclasp', 'colonial', 'warriors', 'use', 'now', 'and', 'then', 'at', 'a', 'difficult', 'or', 'dangerous', 'moment', '.', 'xxmaj', 'it', 'shows', 'in', 'the', 'trust', 'evidenced', 'across', 'rank', 'lines', ',', 'generation', 'lines', ',', 'and', 'gender', 'lines', 'by', 'people', 'who', 'have', 'worked', 'together', ',', 'understand', 'each', 'other', \"'s\", 'strengths', 'and', 'weaknesses', ',', 'and', 'are', 'committed', 'to', 'each', 'other', 'and', 'to', 'a', 'goal', 'worth', 'reaching', 'out', 'for', '.', '\\n\\n', 'xxmaj', 'adama', 'led', 'so', 'well', 'precisely', 'because', 'he', 'showed', 'emotion', ',', 'valued', 'his', 'family', ',', 'worried', 'sometimes', 'intensely', 'but', 'let', 'others', 'live', 'their', 'lives', ',', 'cared', 'and', 'managed', 'situations', 'with', 'flexibility', 'but', 'brooked', 'no', 'nonsense', ',', 'and', 'was', 'fundamentally', 'an', 'older', 'human', 'being', 'saddened', 'by', 'pain', 'but', 'capable', 'of', 'happiness', ',', 'flawed', '(', 'without', 'being', 'a', 'mess', ')', 'but', 'striving', 'to', 'be', 'the', 'best', 'he', 'could', '.', 'xxmaj', 'this', ',', 'too', ',', 'is', 'reality', '.', 'xxmaj', 'there', 'are', 'bad', 'people', 'in', 'this', 'world', ',', 'but', 'the', 'sort', 'of', 'disintegration', 'into', 'whoring', ',', 'in', '-', 'fighting', ',', 'and', 'drugs', 'touted', 'as', '\"', 'real', '\"', 'by', 'such', 'shows', 'as', 'this', 'ridiculous', 'new', 'rip', 'off', 'are', 'unreal', 'because', 'they', 'show', 'only', 'the', 'worst', 'case', 'most', 'extreme', 'scenario', '.', '(', 'come', 'on', ',', 'how', 'many', 'disasters', 'have', 'we', 'read', 'about', ',', 'seen', ',', 'or', 'been', 'in', 'ourselves', 'in', 'which', 'xxup', 'everyone', 'turned', 'into', 'a', 'sick', '-', 'minded', 'nit', '?', ')', '\\n\\n', 'xxmaj', 'my', 'advice', '?', 'xxmaj', 'go', 'for', 'the', 'real', 'thing', ';', 'ignore', 'the', 'fake', 'stuff', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mflWbqdoRPr6",
        "outputId": "7e24b154-7de6-47d4-c451-c176d5cd3f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "defaults.text_proc_rules"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<function fastai.text.core.fix_html>,\n",
              " <function fastai.text.core.replace_rep>,\n",
              " <function fastai.text.core.replace_wrep>,\n",
              " <function fastai.text.core.spec_add_spaces>,\n",
              " <function fastai.text.core.rm_useless_spaces>,\n",
              " <function fastai.text.core.replace_all_caps>,\n",
              " <function fastai.text.core.replace_maj>,\n",
              " <function fastai.text.core.lowercase>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvRpCRzHRX5o"
      },
      "source": [
        "#??replace_rep"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAU31o6sRi5i",
        "outputId": "f4bd4550-7c06-4e4c-a805-e0570c1a1c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "coll_repr(tkn('&copy; Fast.ai ww.fast.ai/INDEX'), 31)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#8) ['xxbos','©','xxmaj','fast.ai','ww.fast.ai','/','xxup','index']\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu4s3qENRx_y"
      },
      "source": [
        "### subword tokenisation\n",
        "\n",
        "Used especially in language like chinese as they have no spaces\n",
        "\n",
        "then we \n",
        "1. analyse the corpus for most commonly occuring grouping\n",
        "2. tokenize using the vocab of subword units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnqz1BETRsof"
      },
      "source": [
        "txts = L(o.open().read() for o in files[:2000])\n",
        "# L behaves like a list of items with extra functionalities"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ut69WKSUiiN",
        "outputId": "db4db914-bc20-4c12-ea5d-34934b5cadb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "txts"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2000) ['Hmmmm, interesting. I\\'ll keep this short on detail, as so many have done such a good job of pointing out the myriad problems with this series both of itself and as a laughably awful remake of a good original series. Ludicrously similar to the (usually) teen-aged modern \"role playing game\" fans who constantly talked of the lack of \"darkness\" in some game they were involved in a decade past, but themselves had so little experience of \"DAHKNESSSSSSS\" that they\\'d have shrieked and run not only from anything remotely like one of the creepy crawlies (usually human appearing, but horrific in some psychological or spiritual way) they wanted featured and emphasized into the ground but also shielded and run or simply self-destructed from any realistic darkness that had presented itself into their unthinking little lives. Or perhaps the equally silly conviction of the teen-aged \"goths\"; kids asserting the world had already gone to hell in its very own handbasket and wearing black and being morose was utterly original and beautifully \"realistic\" and anyone who did not agree just didn\\'t understand (and was really dumb besides). Come to think of it, they had the the same rabid reaction to anyone pointing out the fact that the last few generations have also had a cadre of black-wearing \"originalists\" in high school...LOL!!!<br /><br />The single detailed comment I;d like to make for those seeking enough info to decide on watching or not watching the \"new\" series involves a couple of subtle concepts called family and wisdom (or alternatively, wise leadership). The original series included truly adult themes such as family and friendship; it is mystifying to those actually \"adult\" as to why explicit sexual intercourse, drug use, and foul language is supposed to depict adulthood. The original series contained such scenes as an not-seriously-but-still-ill Adama in bed receiving a visit from his small grandson-by-marriage Boxey, who, tucked up beside his grandfather and soundly kissed, proceeds to tell Adama a small-boy version of his own favorite bedtime story.<br /><br />\"There once was a shining planet, called...\" \"Earth,\" says Adama, recognizing the beginning line of the story he\\'s often told the child. \"No, Mushieland (a type of sweet in the series),\" replies Boxey. \"Mushieland?\" asks Adama, voice and expression exactly what you\\'d expect from an affectionate grandparent both a little surprised and offering encouragement to go on. \"It was full of daggets (essentially an extraterrestial dog),\" continues Boxey, \"but the best of them all was Sire Muffie (the name of his own pet).\" \"Sire Muffie,\" Adama repeats, clearly understanding where this is going, and as clearly content, happy with the boy\\'s company. Again, very much real world, real grandparent.<br /><br />A moment later the camera shows the medical technician who\\'s been keeping visitors away so Adama can rest...standing just out of sight listening, smiling. She\\'s pretended to be asleep so the boy could sneak in and give the old man some of the medicine he needs most right then.<br /><br />This is the stuff of reality. This is adulthood. All the \"F & S\" (reproductive activity and drug sucking) ever shown cannot trump such things on those two counts. It shows when Adama and Apollo --his son-- embrace after Apollo has been out on a dangerous mission. It shows when Starbuck, all choked up and having trouble speaking, sincerely tells the heavily advanced civilization of the Ship of Lights who just revived his dead best friend Apollo \"Whatever you want from me, you can have\". It shows in the not-too-often, not-too-little use of the term \"buddy\" between Starbuck and Apollo and Boomer and the trio handclasp colonial warriors use now and then at a difficult or dangerous moment. It shows in the trust evidenced across rank lines, generation lines, and gender lines by people who have worked together, understand each other\\'s strengths and weaknesses, and are committed to each other and to a goal worth reaching out for.<br /><br />Adama led so well precisely because he showed emotion, valued his family, worried sometimes intensely but let others live their lives, cared and managed situations with flexibility but brooked no nonsense, and was fundamentally an older human being saddened by pain but capable of happiness, flawed (without being a mess) but striving to be the best he could. This, too, is reality. There are bad people in this world, but the sort of disintegration into whoring, in-fighting, and drugs touted as \"real\" by such shows as this ridiculous new rip off are unreal because they show only the worst case most extreme scenario. (Come on, how many disasters have we read about, seen, or been in ourselves in which EVERYONE turned into a sick-minded nit?)<br /><br />My advice? Go for the real thing; ignore the fake stuff.',\"While I applaud the film makers for their effort and terrific use of the camera and very little funds, I can't say that I enjoyed this film a great deal. I feel bad saying anything negative about what is clearly such a labor of love, but the story didn't move me or involve me very much. Yes, it's a scrappy tale boldly told, but the tale itself was weak. All the other elements were fine, but if I'm not engaged by the story then I stop caring, and eventually lose interest, which is what happened when I saw this at the festival.<br /><br />The acting is all fine, particularly the lead man and the voice on the phone (you'll know what I mean when you see it, it really is a lead role even though you never meet him) but there are so many wholes in the story that it never came together for me.<br /><br />I look forward to these film makers next film, and hope they have a bigger budget, but mostly I urge them to find a better script.\",\"A sparse and atmospheric cross between Suspiria and A Watcher in the Woods -THE WOODS delivers subtly for its first two thirds and erupts in a rushed finale that deserved a more thorough and complex screenplay. This film needed more odd side characters and more Angela Bettis she was the voice of the woods though and that and the 60s settings along with Bruce Campbell and Patricia Clarkson's performances made this worthwhile from my viewpoint, I liked the pacing and effects and the bed in the corner had creepy possibilities that were misused by the writer. Overall I liked it but a lot of gorror fans might struggle with this one\",\"I was sceptical at first because straight to video martial arts films tend to scream 'rubbish' from the front cover all the way to the final credits. But this film is certainly different. The story is beautifully basic and uncomplicated, yet never seems too simplistic or rehashed. A biomechanically augmented ex-assassin and wise cracking bar-fly hardly seem the most likely pair yet this film manages to pull it off as it follows them running from bounty hunters, police and various explosions alike. The star of the film is certainly Dacascos who demonstrates such physical ability on a par with the likes of Jet Li and Ong Bak's star Tony Jaa. Due to the low budget - which adds to rather than detracts from the style of the film - there's no CGI and wire work is clearly minimal. Kinetic, frenetic and beautiful are certainly words that describe the fight sequences in this film. Dacascos fits the part of an enhanced fighter with a speed reminiscent of Bruce Lee, and the varied and stylised arenas complement the action no end. I cant believe Hollywood didn't target Dacascos for the biggest projects, save Cradle II the Grave. Kareem Hardison is hilarious at times but also sometimes a distraction from Dacascos but his improvisations and ability to carry the story between the action cannot be overlooked. This film has everything you could want from a martial arts flick and smacks of Chinese influence despite being an Amercian project. The main reason this film is probably unheard of is that it was released at the same time as Rush Hour which was a big budget version of this film, in the sense it is an action comedy with African American/Chinese protagonists. I strongly recommend this film to anyone who has seen Dacascos before as this is surly his finest film, and also to anyone that is intrigued by an American martial arts film that can hold its own against its Chinese counterpart.\",'The earliest episodes with John Amos as the captain were silly. His continuing bad attitude toward them in the face of their success was unrealistic. Beyond that, however, the entire series was great. It suffered some after Stepfanie Kramer left, but it was still good.','Metal: A head banger\\'s journey<br /><br />When this documentary starts showing at the multiplexes, it will be the biggest exposure this musical genre would have gotten thus far, aside from a few ridiculous lawsuits that went public throughout the years. The concept itself that mainstream audiences will be learning of everything from Dio\\'s unique opinions on things to Norwegian Black Metal is quite appalling. More importantly, hopefully this documentary will clear once and for all every false doubt and misconception that Metal has always portrayed to the misinformed.<br /><br />A head banger\\'s journey starts off by examining Metal\\'s roots, the long debate in regards to who was the first Metal band ever. Then, we are taken through an analysis of how metal sub-genres came to exist. Dunn also gives his thoughts about Metal\\'s culture, viewpoints, religious standpoints, the fans and every other aspect of metal. Some of the veterans of the scene as well as some insightful outsiders give their interesting opinions to support the argument at hand. The film balances the serious aspects of Metal with some unintentional humor. Personalities like Alice Cooper, Bruce Dickinson, Alex Webster, Slipknot, Ghaal, Dee Snider, Doro, Ihsahn and many others offer their own individual thoughts in an intellectual way while bands like Mayhem make a fool out of themselves during a drunken interview.<br /><br />Dunn is a true metal head. It is his passion, yet he gives a critical and sometimes subjective opinion of the music at hand. The documentary is extremely entertaining and informative. It made me proud to be a metal head. It even made me give bands I\\'m not too fond of another chance. Bottom line is, if you\\'re a Metal fan you\\'ll enjoy this from beginning to end. If you\\'re a curious outsider, here\\'s your chance to understand our world better and get a more defined opinion of our music. And even if you don\\'t, well to quote Dunn at the end: \"We\\'re doing just fine without you\\x85\"','According to the new two-disc DVD of this movie, it wasn\\'t until the 1950s that Olivia de Havilland realized how fantastic a movie she had been involved in. That was when she saw \"The Adventures of Robin Hood\" for the first time in many years in a packed theater in Paris and caught the buzz of the people seeing it around her.<br /><br />It sort of makes sense. \"The Adventures of Robin Hood\" was made as popular entertainment, designed to amuse and thrill rather than make audiences think. It was probably a drag to make, especially with bad weather, clumsy new Technicolor technology to accommodate, and the harsh director Michael Curtiz ordering everyone around like Colonel Klink. De Havilland was in \"Gone With The Wind,\" she won Oscars playing much more serious roles than Maid Marian. The words of Sir Guy might have rung in her ears, as they have been echoed over time by certain less-kind judges of this film: \"Very charming, but not exactly clever.\"<br /><br />But as Olivia discovered in that French theater and so many of us have been discovering ever since, \"Robin Hood\" is audaciously clever, a perpetual-motion motion-picture machine when it comes to delivering the goods. Beginning with the performances, and carrying forward with the script, the set design, the Technicolor camera work, and especially the fantastic Erich Wolfgang Korngold score, \"Robin Hood\" is one of those older films actually gaining stature over time, as it becomes clearer how much of a miracle it really is.<br /><br />Taking elements of the Robin Hood saga, both from longtime legends and screen treatments like an earlier Douglas Fairbanks Sr. silent film, the scenarists manage to create an upbeat story of wit and pace fully in tune with the merry cast and score. As played by Errol Flynn, this Robin doesn\\'t die in bed an old man, and his banditry is underplayed in favor of a historically incongruous but richly satisfying freedom-fighter figure trying to save his fellow Saxons from the predations of their Norman overlords. He meets Marian, a proud Norman maiden, and, while showing her how the people suffer under the yoke of her fellows, they fall in love.<br /><br />Making Robin a little less radical, and more palatable to mainstream audiences, is the fact he is actually loyal to England\\'s real king, the usurped Richard. This could have been mawkish but sets up a terrific reveal later that jump-starts the last act of the film.<br /><br />There were two directors on \"Robin Hood.\" My guess is that both brought something important to the table, and together created a film neither would have been able to on their own. William Keighley set the gorgeous template, a nod in the direction of N.C. Wyeth\\'s storybook illustrations as Raul daSilva of New Haven has already noted, and as an actor\\'s director, gave useful pointers to the cast, especially the bad guys, who uniformly give subtle, off-center performances. Then, as Keighley wrestled with his bete noir, action, he was replaced by the great Curtiz, who took Keighley\\'s sturdy framework and cracked the whip to liven things up.<br /><br />People wouldn\\'t celebrate \"Robin Hood\" as much without the rescue-from-the-gallows sequence, or the final duel between Robin and Sir Guy, both of which showcase Curtiz at his best. But you wouldn\\'t have so much invested in the principals without Keighley\\'s help in giving the actors the proper space and depth to connect with viewers.<br /><br />You can run out of room rapidly singing the praises of a film like this, a classic adventure that works as a love story and makes one laugh, so effortlessly and all at once. Flynn was never better as a Robin that sacrifices grittiness for humanity and crack one-liners. And De Havilland is quite possibly the most gorgeous woman ever filmed, at the very least up there with the likes of Maureen O\\'Hara in \"The Quiet Man\" and Cameron Diaz in \"Something About Mary\" for the way she glows on screen, her gorgeous browns gazing up at Robin in a way that inspires a sad sort of rapture, at least for me.<br /><br />There has never been a film since that did the things \"Robin Hood\" does, but I find that less remarkable than the fact a film this good was ever made at all.',\"It's out on rent so, rent it. <br /><br />Man, is really good. It deals with a few stories that are connected if you think about it, mainly by a building. <br /><br />Love, lost, memories and discoveries are the main emotions that drive this movie. Again, another perfect film that's plot-less. This film even has an event, which has been done over and over in film but, just because you are not attached to a plot it seems like the most original event in the movie. Mothers, daughters, friends and fathers are all living their lives, like any of us, not noticing the small things in life. Our sense, specially sight and sound are over stimulated by this beautiful movie. The little simple things is what drives this movie and god that's filmmaking, that's art. Let me tell you how good it is, is so good, the movie was over and I stopped the DVD and started it again, I was it twice last night I think I'll see it tonight again.<br /><br />\",\"Here we are taken inside Paul Green's School of Rock. Why are we taken in there? I don't know. The man himself is an obnoxious, immature wannabe who stopped trying to be and who know finances his life on the money paid by parents who want their kids to be rock stars or to find a new outlet or learning environment. This is no learning environment, at least no in the traditional sense. From what is shown, all the viewer can gather is that those who are already gifted are given the best opportunities and those who struggle are left to struggle but as long as the money keeps coming in, they can stay in the school to be shouted at and verbally abused. The documentary maker really failed for me in that he really made no point at all and failed to really question or press Paul Green at all. So we are guided through a time when some students are preparing for a Frank Zappa festival in Germany. We see toward the end that Mr. Green has no problem taking limelight and applause at the festival's end but what had he really contributed? Frank Zappa may have been revolutionary to some but his music is mostly aimless for me, rather like this film. I shall never watch it again, ever!\",\"This movie is not for everyone. The story is pretty weak and the humor is very age oriented. The movie was very humorous if you enjoy the white man who thinks he's a black man humor. The most amusing piece of the movie is the dialogue that is spoken by B-Rad (Jaime Kennedy). Jaime Kennedy does a very good job portraying B-Rad,the ghetto like white boy. It comes across as hilarious because everyone finds it amussing except him. IF you are looking for a good laugh, and you like that type of humor, I would recommend going to see this movie.\"...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xPF5ZHgSa4b"
      },
      "source": [
        "we instantiate the tokeniser passing in the size of vocab and then find the common sequences of characters to create the vocab. this is done with setup. it is called automatically but we are doing ti here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQI1pTujSNCO"
      },
      "source": [
        "def subword(sz):\n",
        "    sp = SubwordTokenizer(vocab_sz=sz)\n",
        "    sp.setup(txts)\n",
        "    return ' '.join(first(sp([txts]))[:40])\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_imJA-7XVG2H",
        "outputId": "a2968080-6473-443a-a676-c945c8024d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sp = SubwordTokenizer(vocab_sz=1000)\n",
        "sp.setup(txts)\n",
        "print(sp([txts]))\n",
        "for i in str(sp([txts])):\n",
        "  print(str(i))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<generator object SentencePieceTokenizer.__call__ at 0x7f502e5e55c8>\n",
            "<\n",
            "g\n",
            "e\n",
            "n\n",
            "e\n",
            "r\n",
            "a\n",
            "t\n",
            "o\n",
            "r\n",
            " \n",
            "o\n",
            "b\n",
            "j\n",
            "e\n",
            "c\n",
            "t\n",
            " \n",
            "S\n",
            "e\n",
            "n\n",
            "t\n",
            "e\n",
            "n\n",
            "c\n",
            "e\n",
            "P\n",
            "i\n",
            "e\n",
            "c\n",
            "e\n",
            "T\n",
            "o\n",
            "k\n",
            "e\n",
            "n\n",
            "i\n",
            "z\n",
            "e\n",
            "r\n",
            ".\n",
            "_\n",
            "_\n",
            "c\n",
            "a\n",
            "l\n",
            "l\n",
            "_\n",
            "_\n",
            " \n",
            "a\n",
            "t\n",
            " \n",
            "0\n",
            "x\n",
            "7\n",
            "f\n",
            "5\n",
            "0\n",
            "2\n",
            "e\n",
            "5\n",
            "e\n",
            "5\n",
            "5\n",
            "c\n",
            "8\n",
            ">\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5yI_bMlT-mV"
      },
      "source": [
        "#subword(1000)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l-dx_7b7d5W"
      },
      "source": [
        "If er use samller vocab then each token would be smaller bbut it will take more tokens to represent a sentence.\n",
        "\n",
        "I its large then most common phrases themselves would be in vocab\n",
        "\n",
        "subwords are important because then we can also train on genomic sequence and MIDI music data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dft49j2c8NSN"
      },
      "source": [
        "### Numericalisation\n",
        "\n",
        "IN this we map the token to integers. It is similar to steps needded to create a categorical model.\n",
        "\n",
        "in this we make a list of all possible levels of that categorical variable\n",
        "\n",
        "Replace each level with index in vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxWlVAOBUbWQ",
        "outputId": "2a90dc3b-9196-4c51-9089-3688d4cd776f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "toks = tkn(txt)\n",
        "\n",
        "print(coll_repr(tkn(txt), 5))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#1033) ['xxbos','h','xxrep','4','m'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uImBVVj492-C"
      },
      "source": [
        "in orderto use numericalise we need to call setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrgqSAyLw_WR",
        "outputId": "e676bb85-9249-4840-bafa-411c9a1a7bef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "toks200 = txts[:200].map(tkn)\n",
        "toks200[199]\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#177) ['xxbos','xxmaj','this','is','xxmaj','uma','xxmaj','thurman',\"'s\",'first'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4odh7kF-xwg",
        "outputId": "09ee7fab-2ade-4182-cdda-487ca8c55f08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks200)\n",
        "coll_repr(num.vocab, 20)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#2240) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oREiYhsCoY3"
      },
      "source": [
        "thedefaults to Numericalize are min_freq=2 and max _vocab=60000 this results in fastai replacing all words other than the most common 60 000 with a special unknown word token xxunk.\n",
        "\n",
        "this is to avoid overly large embedding matrix\n",
        "\n",
        "after creating num we can use it like a token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iiC8PwgBwYo",
        "outputId": "68871b42-20b1-4149-867f-43d8d96d7f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "nums = num(toks)[:20]\n",
        "nums"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,    0,    5,  362,    0,   10,  215,   11,    8,   19,  306,  307,   20,  257,   33, 1609,   10,   26,   46,  140])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAZuoPzaDynd"
      },
      "source": [
        "they can be mapped back intooriginal text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1PrIqPlDlCg",
        "outputId": "98ec9ae8-7c2c-4f5b-f91f-8660d39fe5b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num.vocab[20]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfZUlI1ND9CR"
      },
      "source": [
        "### Putting text into batches for a language Model\n",
        "\n",
        "two considerations : irregular length and importance of begining when the previous batch left off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqzxxrMMD7Gp",
        "outputId": "701f9bac-32bf-4257-c2cc-43e41971a5cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "stream = \"In this chapter, we will go over what has been done previously and try to be the best thing that ever got into the the history of these leaves. To be frank I never expected me to be the kind of guy who would follow through. But what do you kno here we are doing what we do worst. You can't be at your best everytime, however the problem is I am at my worst more time htan I would like. And to make matters worse I rarely learn.\"\n",
        "tokens = tkn(stream)\n",
        "bs, seq_len = 6, 15\n",
        "d_tokens = np.array( [ tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False, header=None)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>over</td>\n",
              "      <td>what</td>\n",
              "      <td>has</td>\n",
              "      <td>been</td>\n",
              "      <td>done</td>\n",
              "      <td>previously</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>and</td>\n",
              "      <td>try</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>best</td>\n",
              "      <td>thing</td>\n",
              "      <td>that</td>\n",
              "      <td>ever</td>\n",
              "      <td>got</td>\n",
              "      <td>into</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>history</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>these</td>\n",
              "      <td>leaves</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>frank</td>\n",
              "      <td>i</td>\n",
              "      <td>never</td>\n",
              "      <td>expected</td>\n",
              "      <td>me</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>kind</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>guy</td>\n",
              "      <td>who</td>\n",
              "      <td>would</td>\n",
              "      <td>follow</td>\n",
              "      <td>through</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>but</td>\n",
              "      <td>what</td>\n",
              "      <td>do</td>\n",
              "      <td>you</td>\n",
              "      <td>kno</td>\n",
              "      <td>here</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>are</td>\n",
              "      <td>doing</td>\n",
              "      <td>what</td>\n",
              "      <td>we</td>\n",
              "      <td>do</td>\n",
              "      <td>worst</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>you</td>\n",
              "      <td>ca</td>\n",
              "      <td>n't</td>\n",
              "      <td>be</td>\n",
              "      <td>at</td>\n",
              "      <td>your</td>\n",
              "      <td>best</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>everytime</td>\n",
              "      <td>,</td>\n",
              "      <td>however</td>\n",
              "      <td>the</td>\n",
              "      <td>problem</td>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>am</td>\n",
              "      <td>at</td>\n",
              "      <td>my</td>\n",
              "      <td>worst</td>\n",
              "      <td>more</td>\n",
              "      <td>time</td>\n",
              "      <td>htan</td>\n",
              "      <td>i</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAuPYw5eJOoh"
      },
      "source": [
        "we cant simple give the above words to model because of GPU restrictions.\n",
        "\n",
        "Sowe neeedto devide this array more finely into subarrays of fixed sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt-SJtAaJGmN",
        "outputId": "8103ae0a-6c4c-4533-9d5e-58dfdee047b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# if we want a sequence len of five for 6 batches of length 15\n",
        "\n",
        "bs, seq_len = 6, 5\n",
        "for j in range(0,15):\n",
        "  d_tokens = np.array([tokens[i*15:i*15+j * seq_len] for i in range(bs)])\n",
        "  df = pd.DataFrame(d_tokens)\n",
        "  display(HTML(df.to_html(index=False, header=None)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "    </tr>\n",
              "    <tr>\n",
              "    </tr>\n",
              "    <tr>\n",
              "    </tr>\n",
              "    <tr>\n",
              "    </tr>\n",
              "    <tr>\n",
              "    </tr>\n",
              "    <tr>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>and</td>\n",
              "      <td>try</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>these</td>\n",
              "      <td>leaves</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>guy</td>\n",
              "      <td>who</td>\n",
              "      <td>would</td>\n",
              "      <td>follow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>are</td>\n",
              "      <td>doing</td>\n",
              "      <td>what</td>\n",
              "      <td>we</td>\n",
              "      <td>do</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>everytime</td>\n",
              "      <td>,</td>\n",
              "      <td>however</td>\n",
              "      <td>the</td>\n",
              "      <td>problem</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>over</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>and</td>\n",
              "      <td>try</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>best</td>\n",
              "      <td>thing</td>\n",
              "      <td>that</td>\n",
              "      <td>ever</td>\n",
              "      <td>got</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>these</td>\n",
              "      <td>leaves</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>frank</td>\n",
              "      <td>i</td>\n",
              "      <td>never</td>\n",
              "      <td>expected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>guy</td>\n",
              "      <td>who</td>\n",
              "      <td>would</td>\n",
              "      <td>follow</td>\n",
              "      <td>through</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>but</td>\n",
              "      <td>what</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>are</td>\n",
              "      <td>doing</td>\n",
              "      <td>what</td>\n",
              "      <td>we</td>\n",
              "      <td>do</td>\n",
              "      <td>worst</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>you</td>\n",
              "      <td>ca</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>everytime</td>\n",
              "      <td>,</td>\n",
              "      <td>however</td>\n",
              "      <td>the</td>\n",
              "      <td>problem</td>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>am</td>\n",
              "      <td>at</td>\n",
              "      <td>my</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>over</td>\n",
              "      <td>what</td>\n",
              "      <td>has</td>\n",
              "      <td>been</td>\n",
              "      <td>done</td>\n",
              "      <td>previously</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>and</td>\n",
              "      <td>try</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>best</td>\n",
              "      <td>thing</td>\n",
              "      <td>that</td>\n",
              "      <td>ever</td>\n",
              "      <td>got</td>\n",
              "      <td>into</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>history</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>these</td>\n",
              "      <td>leaves</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>frank</td>\n",
              "      <td>i</td>\n",
              "      <td>never</td>\n",
              "      <td>expected</td>\n",
              "      <td>me</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>kind</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>guy</td>\n",
              "      <td>who</td>\n",
              "      <td>would</td>\n",
              "      <td>follow</td>\n",
              "      <td>through</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>but</td>\n",
              "      <td>what</td>\n",
              "      <td>do</td>\n",
              "      <td>you</td>\n",
              "      <td>kno</td>\n",
              "      <td>here</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>are</td>\n",
              "      <td>doing</td>\n",
              "      <td>what</td>\n",
              "      <td>we</td>\n",
              "      <td>do</td>\n",
              "      <td>worst</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>you</td>\n",
              "      <td>ca</td>\n",
              "      <td>n't</td>\n",
              "      <td>be</td>\n",
              "      <td>at</td>\n",
              "      <td>your</td>\n",
              "      <td>best</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>everytime</td>\n",
              "      <td>,</td>\n",
              "      <td>however</td>\n",
              "      <td>the</td>\n",
              "      <td>problem</td>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>am</td>\n",
              "      <td>at</td>\n",
              "      <td>my</td>\n",
              "      <td>worst</td>\n",
              "      <td>more</td>\n",
              "      <td>time</td>\n",
              "      <td>htan</td>\n",
              "      <td>i</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>over</td>\n",
              "      <td>what</td>\n",
              "      <td>has</td>\n",
              "      <td>been</td>\n",
              "      <td>done</td>\n",
              "      <td>previously</td>\n",
              "      <td>and</td>\n",
              "      <td>try</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>and</td>\n",
              "      <td>try</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>best</td>\n",
              "      <td>thing</td>\n",
              "      <td>that</td>\n",
              "      <td>ever</td>\n",
              "      <td>got</td>\n",
              "      <td>into</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>history</td>\n",
              "      <td>of</td>\n",
              "      <td>these</td>\n",
              "      <td>leaves</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>these</td>\n",
              "      <td>leaves</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>frank</td>\n",
              "      <td>i</td>\n",
              "      <td>never</td>\n",
              "      <td>expected</td>\n",
              "      <td>me</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>kind</td>\n",
              "      <td>of</td>\n",
              "      <td>guy</td>\n",
              "      <td>who</td>\n",
              "      <td>would</td>\n",
              "      <td>follow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>guy</td>\n",
              "      <td>who</td>\n",
              "      <td>would</td>\n",
              "      <td>follow</td>\n",
              "      <td>through</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>but</td>\n",
              "      <td>what</td>\n",
              "      <td>do</td>\n",
              "      <td>you</td>\n",
              "      <td>kno</td>\n",
              "      <td>here</td>\n",
              "      <td>we</td>\n",
              "      <td>are</td>\n",
              "      <td>doing</td>\n",
              "      <td>what</td>\n",
              "      <td>we</td>\n",
              "      <td>do</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>are</td>\n",
              "      <td>doing</td>\n",
              "      <td>what</td>\n",
              "      <td>we</td>\n",
              "      <td>do</td>\n",
              "      <td>worst</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>you</td>\n",
              "      <td>ca</td>\n",
              "      <td>n't</td>\n",
              "      <td>be</td>\n",
              "      <td>at</td>\n",
              "      <td>your</td>\n",
              "      <td>best</td>\n",
              "      <td>everytime</td>\n",
              "      <td>,</td>\n",
              "      <td>however</td>\n",
              "      <td>the</td>\n",
              "      <td>problem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>everytime</td>\n",
              "      <td>,</td>\n",
              "      <td>however</td>\n",
              "      <td>the</td>\n",
              "      <td>problem</td>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>am</td>\n",
              "      <td>at</td>\n",
              "      <td>my</td>\n",
              "      <td>worst</td>\n",
              "      <td>more</td>\n",
              "      <td>time</td>\n",
              "      <td>htan</td>\n",
              "      <td>i</td>\n",
              "      <td>would</td>\n",
              "      <td>like</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>over</td>\n",
              "      <td>what</td>\n",
              "      <td>has</td>\n",
              "      <td>been</td>\n",
              "      <td>done</td>\n",
              "      <td>previously</td>\n",
              "      <td>and</td>\n",
              "      <td>try</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>best</td>\n",
              "      <td>thing</td>\n",
              "      <td>that</td>\n",
              "      <td>ever</td>\n",
              "      <td>got</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>and</td>\n",
              "      <td>try</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>best</td>\n",
              "      <td>thing</td>\n",
              "      <td>that</td>\n",
              "      <td>ever</td>\n",
              "      <td>got</td>\n",
              "      <td>into</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>history</td>\n",
              "      <td>of</td>\n",
              "      <td>these</td>\n",
              "      <td>leaves</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>frank</td>\n",
              "      <td>i</td>\n",
              "      <td>never</td>\n",
              "      <td>expected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>these</td>\n",
              "      <td>leaves</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>frank</td>\n",
              "      <td>i</td>\n",
              "      <td>never</td>\n",
              "      <td>expected</td>\n",
              "      <td>me</td>\n",
              "      <td>to</td>\n",
              "      <td>be</td>\n",
              "      <td>the</td>\n",
              "      <td>kind</td>\n",
              "      <td>of</td>\n",
              "      <td>guy</td>\n",
              "      <td>who</td>\n",
              "      <td>would</td>\n",
              "      <td>follow</td>\n",
              "      <td>through</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>but</td>\n",
              "      <td>what</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>guy</td>\n",
              "      <td>who</td>\n",
              "      <td>would</td>\n",
              "      <td>follow</td>\n",
              "      <td>through</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>but</td>\n",
              "      <td>what</td>\n",
              "      <td>do</td>\n",
              "      <td>you</td>\n",
              "      <td>kno</td>\n",
              "      <td>here</td>\n",
              "      <td>we</td>\n",
              "      <td>are</td>\n",
              "      <td>doing</td>\n",
              "      <td>what</td>\n",
              "      <td>we</td>\n",
              "      <td>do</td>\n",
              "      <td>worst</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>you</td>\n",
              "      <td>ca</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>are</td>\n",
              "      <td>doing</td>\n",
              "      <td>what</td>\n",
              "      <td>we</td>\n",
              "      <td>do</td>\n",
              "      <td>worst</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>you</td>\n",
              "      <td>ca</td>\n",
              "      <td>n't</td>\n",
              "      <td>be</td>\n",
              "      <td>at</td>\n",
              "      <td>your</td>\n",
              "      <td>best</td>\n",
              "      <td>everytime</td>\n",
              "      <td>,</td>\n",
              "      <td>however</td>\n",
              "      <td>the</td>\n",
              "      <td>problem</td>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>am</td>\n",
              "      <td>at</td>\n",
              "      <td>my</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>everytime</td>\n",
              "      <td>,</td>\n",
              "      <td>however</td>\n",
              "      <td>the</td>\n",
              "      <td>problem</td>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>am</td>\n",
              "      <td>at</td>\n",
              "      <td>my</td>\n",
              "      <td>worst</td>\n",
              "      <td>more</td>\n",
              "      <td>time</td>\n",
              "      <td>htan</td>\n",
              "      <td>i</td>\n",
              "      <td>would</td>\n",
              "      <td>like</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>and</td>\n",
              "      <td>to</td>\n",
              "      <td>make</td>\n",
              "      <td>matters</td>\n",
              "      <td>worse</td>\n",
              "      <td>i</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[xxbos, xxmaj, in, this, chapter, ,, we, will, go, over, what, has, been, done, previously, and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[xxbos, xxmaj, in, this, chapter, ,, we, will, go, over, what, has, been, done, previously, and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[xxbos, xxmaj, in, this, chapter, ,, we, will, go, over, what, has, been, done, previously, and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[xxbos, xxmaj, in, this, chapter, ,, we, will, go, over, what, has, been, done, previously, and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[xxbos, xxmaj, in, this, chapter, ,, we, will, go, over, what, has, been, done, previously, and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[xxbos, xxmaj, in, this, chapter, ,, we, will, go, over, what, has, been, done, previously, and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[xxbos, xxmaj, in, this, chapter, ,, we, will, go, over, what, has, been, done, previously, and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[xxbos, xxmaj, in, this, chapter, ,, we, will, go, over, what, has, been, done, previously, and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[xxbos, xxmaj, in, this, chapter, ,, we, will, go, over, what, has, been, done, previously, and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[and, try, to, be, the, best, thing, that, ever, got, into, the, the, history, of, these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[these, leaves, ., xxmaj, to, be, frank, i, never, expected, me, to, be, the, kind, of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[of, guy, who, would, follow, through, ., xxmaj, but, what, do, you, kno, here, we, are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[are, doing, what, we, do, worst, ., xxmaj, you, ca, n't, be, at, your, best, everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[everytime, ,, however, the, problem, is, i, am, at, my, worst, more, time, htan, i, would, like, ., xxmaj, and, to, make, matters, worse, i, rarely, learn, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAg9B7QKM66J"
      },
      "source": [
        "we need to transform texts inot a stream by concatenating them together. As with images it is best to randomize order of documents (not the text!)\n",
        "\n",
        "Then we cut the stream to batch which is our batch size. for instance if stream is 50000 then we set a batch size of 10 this will give us 10 mini streams of 5000 tokens\n",
        "\n",
        "this is the behindthe scenes for fastai librart when we create LMDataLoader. we do it when we apply Numericalize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfVynKMWMXEv"
      },
      "source": [
        "nums200 = toks200.map(num)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C44JSyAOaJd"
      },
      "source": [
        "dl = LMDataLoader(nums200)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz4T3c6MOgg_",
        "outputId": "c37e50c9-c722-4185-f632-a08210f8a721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# grabbing the first batch\n",
        "x,y = first(dl)\n",
        "x.shape, y.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 72]), torch.Size([64, 72]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keMUY-kFO_EX",
        "outputId": "22da9851-8ee8-4bea-fcb4-12de2d435db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x[0][:20]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,    0,    5,  362,    0,   10,  215,   11,    8,   19,  306,  307,   20,  257,   33, 1609,   10,   26,   46,  140])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnCbJW8KOwe3",
        "outputId": "b80d2b68-4dd6-4588-9b50-9680fe836696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "' '.join([num.vocab[o] for o in x[0][:20]])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"xxbos xxunk xxrep 4 xxunk , interesting . xxmaj i 'll keep this short on detail , as so many\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIQySG4nPNxd"
      },
      "source": [
        "### Training a Text classifier\n",
        "\n",
        "we need to finetune our model pretrained on wikipedia to the corpus of IMDb review and then use that model to train a calssifier\n",
        "\n",
        "### Language model using datablock\n",
        "\n",
        "fastai handles Tokenize and Numericalize when TextBlock is passed to datablock\n",
        "All of the arguments that can be passed to Tokenize and Numericalize can also be passed to TextBlock\n",
        "\n",
        "there is umary method that is very useful for debugging data issues\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J05RvDOqPMrW",
        "outputId": "801d4316-5968-492a-e2df-47bece2e51e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
        "\n",
        "dls_lm = DataBlock(\n",
        "    blocks= TextBlock.from_folder(path, is_lm=True),\n",
        "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
        ").dataloaders(path, path=path, bs=128, seq_len=80)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chtck9xfiu8G"
      },
      "source": [
        "One thing that's different to previous types we've used in `DataBlock` is that we're not just using the class directly (i.e., `TextBlock(...)`, but instead are calling a *class method*. \n",
        "\n",
        "The reason that `TextBlock` is special is that setting up the numericalizer's vocab can take a long time (we have to read and tokenize every document to get the vocab). To be as efficient as possible it performs a few optimizations: \n",
        "\n",
        "- It saves the tokenized documents in a temporary folder, so it doesn't have to tokenize them more than once\n",
        "- It runs multiple tokenization processes in parallel, to take advantage of your computer's CPUs\n",
        "\n",
        "We need to tell `TextBlock` how to access the texts, so that it can do this initial preprocessing—that's what `from_folder` does.\n",
        "\n",
        "`show_batch` then works in the usual way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45FlW2uapSiU",
        "outputId": "e7b15565-fbe8-4745-876b-515ab84ee257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "dls_lm.show_batch(max_n=2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj as a massive fan of xxup dm , it goes without saying that i have seen this film numerous times . xxmaj however , i watch it purely for the concert footage … the rest of the film is , um , pretty dreadful , sad to say . \\n\\n xxmaj famed rock music film director xxup da xxmaj pennebaker followed xxmaj mode around on their late 80s xxmaj music xxmaj for xxmaj the xxmaj masses tour ,</td>\n",
              "      <td>xxmaj as a massive fan of xxup dm , it goes without saying that i have seen this film numerous times . xxmaj however , i watch it purely for the concert footage … the rest of the film is , um , pretty dreadful , sad to say . \\n\\n xxmaj famed rock music film director xxup da xxmaj pennebaker followed xxmaj mode around on their late 80s xxmaj music xxmaj for xxmaj the xxmaj masses tour , which</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it sounds ridiculous but the lead actor does his best to play a modern xxmaj james dean type role where you do care about whether he will recover his piano ability . \\n\\n xxmaj there are nice cameos from this business partner 's wife as well as his piano teacher . \\n\\n a couple of the scenes do n't quite run as smoothly as most of the film but you definitely follow the path of this moody wanna - be</td>\n",
              "      <td>sounds ridiculous but the lead actor does his best to play a modern xxmaj james dean type role where you do care about whether he will recover his piano ability . \\n\\n xxmaj there are nice cameos from this business partner 's wife as well as his piano teacher . \\n\\n a couple of the scenes do n't quite run as smoothly as most of the film but you definitely follow the path of this moody wanna - be musician</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-F97KFPqs3X"
      },
      "source": [
        "### Fine tuning the language model\n",
        "\n",
        "to conver integer word indices inot activation that we can use, we will use embeddings, just like we did for collaborative filtering and tabular modelling. We will feed those embeddings into a recurrent neural network using an architechtire called AWD-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u0CzFa3qcDn",
        "outputId": "a672ca10-1235-4df5-b47f-5ca7883fe880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]\n",
        ").to_fp16()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuBJRcPHE09f",
        "outputId": "c605fa58-3671-4cb9-8504-897cb888e67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.save('1epoch')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/root/.fastai/data/imdb/models/1epoch.pth')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHtO5d7kE-p8",
        "outputId": "922cc063-b0b7-4f2d-d6f9-234591d49535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.load('1epoch')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fastai.text.learner.LMLearner at 0x7f5027a96898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYpuXIoXFEk0"
      },
      "source": [
        "### fine tuning after unfreezing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbzIbwwkFDUj",
        "outputId": "ef7b5cdb-4939-4622-ee9d-f601437fb7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(1, 2e-3)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.970833</td>\n",
              "      <td>3.782441</td>\n",
              "      <td>0.323944</td>\n",
              "      <td>43.923122</td>\n",
              "      <td>22:40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHLVUWVJGSQs"
      },
      "source": [
        "learn.save_encoder('finetuned')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvo_P1ysGbZ9"
      },
      "source": [
        "### Text Generation\n",
        "\n",
        "using the model to create random reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMnyr9AVGWIY",
        "outputId": "72e3c1bc-523d-438d-e004-8e105f87dacb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tHB09ZqHD98",
        "outputId": "76521203-f5d3-4ee5-b0cb-44dea50107b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(\"\\n\".join(preds))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i liked this movie because it begins with Neil Simon and Chris Rock ( james Earl Jones ) who manage to steal the Pacific Station . It 's the same movie the plot is about .\n",
            "i liked this movie because it looks like a documentary to us about a young woman who 's pals find out that the girl is a virgin , and she goes to \" variable \" to see what happens to them . The reason\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1QqMVoQLQWO"
      },
      "source": [
        "# Creating the Classifier DataLoader\n",
        "\n",
        "We're now moving from language model fine-tuning to classifier fine-tuning. To recap, a language model predicts the next word of a document, so it doesn't need any external labels. A classifier, however, predicts some external label—in the case of IMDb, it's the sentiment of a document.\n",
        "\n",
        "This means that the structure of our DataBlock for NLP classification will look very familiar. It's actually nearly the same as we've seen for the many image classification datasets we've worked with:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcTAn6H_HNTq"
      },
      "source": [
        "dls_clas = DataBlock(\n",
        "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab), CategoryBlock),\n",
        "    get_y = parent_label,\n",
        "    get_items = partial(get_text_files,folders=['train', 'test']),\n",
        "    splitter = GrandparentSplitter(valid_name='test')\n",
        ").dataloaders(path, path=path, bs=128, seq_len=72)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm_tiJbE6Hhd",
        "outputId": "9944d8f0-e556-4712-c5dc-e71bc7093bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "dls_clas.show_batch(max_n=3)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szRQtDnk6XvL"
      },
      "source": [
        "The only difference in the above datablock is that\n",
        "> is_lm is True is not present in TextBlock from_folder\n",
        "\n",
        "> we pass the vocab that we created forthe langage model fine -tuning so that we have same correspondence to index.\n",
        "\n",
        "There is achallenge with collating multiple documents \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpoFNhgZA0Eu",
        "outputId": "cabdd263-c4ec-472b-b934-f112ae991fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(num)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numericalize:\n",
            "encodes: (object,object) -> encodes\n",
            "decodes: (object,object) -> decodes\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg8L8lX56MK4"
      },
      "source": [
        "\n",
        "\n",
        "nums_samp = toks200[:10].map(num)\n",
        "#numericalisation\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OziSOG3x8PA3",
        "outputId": "5f43be3a-a5d4-4f96-8ad5-5a1f3f53c9f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nums_samp.map(len)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#10) [1033,212,128,398,61,426,945,218,265,129]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QXZbAX0GzFc"
      },
      "source": [
        "Pytorch DataLoaders need to collate all the items in a batch into a single tensor and a single tensor has a fixed shape \n",
        "\n",
        "In images we used croppping padding etc.\n",
        "we cant squish so we pad\n",
        "\n",
        "sorting and padding s automaticalling doene when using a textblock with is_lm =false\n",
        "\n",
        "because in language model we concatenate data first then split them into equally size sections\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGcs6Nkk8RPR"
      },
      "source": [
        "learn = text_classifier_learner(dls_clas, AWD_LSTM,drop_mult=0.5, metrics=accuracy).to_fp16()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdJ4QnS4Qy8I"
      },
      "source": [
        "final step is to use load_encoder instead of load we only have pretrained wieghts available for the encoder, load by default raised an exeption if an incomplete model is loaded\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGCYky8zQyFB"
      },
      "source": [
        "learn = learn.load_encoder('finetuned')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT1xm8tqRf2d"
      },
      "source": [
        "### fine tuning the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcdTTvSvQuZN",
        "outputId": "e2878621-c105-4342-d5a8-003de1dc5307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "learn.fit_one_cycle(1,2e-2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.404233</td>\n",
              "      <td>0.235010</td>\n",
              "      <td>0.904760</td>\n",
              "      <td>01:11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2fzye2pRp0H"
      },
      "source": [
        "we can pass -2 to the freeze_to to freaaze all exept the last two parameter groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX521lDsRnEa",
        "outputId": "ca01e27e-cea1-4064-99d2-503d53aced6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.291886</td>\n",
              "      <td>0.203919</td>\n",
              "      <td>0.918920</td>\n",
              "      <td>01:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ifbJ2WR7c4"
      },
      "source": [
        "we will unfreeze a bit more and continue training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYlMsdXaR6Ew",
        "outputId": "6024b201-36ef-4ff1-fff6-72f375565456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1,slice(5e-3/(2.6**4), 5e-3))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.225767</td>\n",
              "      <td>0.178110</td>\n",
              "      <td>0.931120</td>\n",
              "      <td>01:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTze9ar0SLWv"
      },
      "source": [
        "finally the whole model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i9VsTEySKf8",
        "outputId": "c8f9182c-876b-43dc-e393-16d61f222ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.194780</td>\n",
              "      <td>0.174615</td>\n",
              "      <td>0.932480</td>\n",
              "      <td>01:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.176370</td>\n",
              "      <td>0.171268</td>\n",
              "      <td>0.934240</td>\n",
              "      <td>01:59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH0EfJkfSfbn",
        "outputId": "f4e8d4c5-9abb-4c3c-95d0-8cccfd2084bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        }
      },
      "source": [
        "learn.show_results(max_n=3)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>category_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj there 's a sign on xxmaj the xxmaj lost xxmaj highway that says : \\n\\n * major xxup spoilers xxup ahead * \\n\\n ( but you already knew that , did n't you ? ) \\n\\n xxmaj since there 's a great deal of people that apparently did not get the point of this movie , xxmaj i 'd like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can \" cheat \" by looking at xxmaj david xxmaj lynch 's \" top 10 xxmaj hints to xxmaj unlocking xxup md \" ( but only upon second or third viewing , please . ) ;) \\n\\n xxmaj first of all , xxmaj mulholland xxmaj drive is</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxbos xxmaj back in the mid / late 80s , an xxup oav anime by title of \" bubblegum xxmaj crisis \" ( which i think is a military slang term for when technical equipment goes haywire ) made its debut on video , taking inspiration from \" blade xxmaj runner \" , \" the xxmaj terminator \" and maybe even \" robocop \" , with a little dash of xxmaj batman / xxmaj bruce xxmaj wayne -</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0l8vcj6Scyz"
      },
      "source": [
        "## disinformation \n",
        "\n",
        "Its easy to spread misinformation madlibs style.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXXxZ_o7SZVU",
        "outputId": "95550f4a-4aad-4b7b-d373-b2546761c277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.predict('I never likedb the movie at all')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pos', tensor(1), tensor([0.0147, 0.9853]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_v2MMgBYdVQ"
      },
      "source": [
        "How do you do predictions with this though."
      ]
    }
  ]
}