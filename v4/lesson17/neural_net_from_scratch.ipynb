{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural net from scratch",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPo1dI8QtQNX"
      },
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kineq8yItabj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee6416c-a185-4d82-8eb8-c5ef3de23833"
      },
      "source": [
        "!pip install nbformat nbconvert\n",
        "try:\n",
        "  from fastai.gen_doc.nbdoc import *\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (5.0.8)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (5.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat) (4.6.3)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat) (4.3.3)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat) (0.2.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.4.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.11.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert) (3.2.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1->nbformat) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1->nbformat) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (20.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert) (2.4.7)\n",
            "No module named 'fastai.gen_doc'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yQubRjYuab8"
      },
      "source": [
        "# Building stuff from ground up\n",
        "\n",
        "We will be building everything from scratch only using basic indexing into a tensor\n",
        "\n",
        "and see what exactly what is happening in loss.backward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLHpPdDNu-BB"
      },
      "source": [
        "## Building a neural net from scratch\n",
        "\n",
        "modelling a neuron : a neuron is shaped as out = sum of (xiwi) + b\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1X5c2o7to7F"
      },
      "source": [
        "inputs =[0,0,0]\n",
        "weights =[0,0,0]\n",
        "bias = 0\n",
        "output = sum([x*w for x,w in zip(inputs, weights)]) + bias"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xJ5hf9tTQm"
      },
      "source": [
        "THis is then sent to an activation function Relu for example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhu48jAewGfm"
      },
      "source": [
        "def relu(x):\n",
        "  return x if x>=0 else 0"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNQUF6_kwNeU"
      },
      "source": [
        "Deep learning model is then bilt by stacking a lot of those nerurons in successive layer with a certain number of neurons  and linking the output and inputs ato each neurtoron. it is called a dense layer or densly connectedor linear layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3B8rVyswMoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0152e09-7686-49ae-daf0-ceff435b2a15"
      },
      "source": [
        "sum([x*w for x,w in zip(inputs, weights)]) + bias"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLXWthRtxSI1"
      },
      "source": [
        "we are essentially doing matrix multiplication here\n",
        "\n",
        "x is supposed to hace batch_Size by n_inputs\n",
        "\n",
        "while w would be n_neurons by n_inputs\n",
        "\n",
        "so "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IctYtavExA3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f103bd-6bc2-4d5c-8e2c-411fe84572f8"
      },
      "source": [
        "input = [[0],[0],[0]]\n",
        "weights = [[0,0,0]] \n",
        "def transpose(weights):\n",
        "  return weights\n",
        "\n",
        "try:\n",
        "  y =  inputs @  transpose(weights)+ b\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unsupported operand type(s) for @: 'list' and 'list'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ucD7_oe4D8N"
      },
      "source": [
        "the output then should be if size batch_size by neurons and in position (i,j)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0XcQkNv4Bca"
      },
      "source": [
        "x = inputs\n",
        "w = weights\n",
        "for i in range(0, 2):\n",
        "  for j in range(0,2):\n",
        "    try:\n",
        "      y[i,j] = sum([a*b for a,b in zip(x[i,:],w.t[j,:])]) + b[j]\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMZ_VHas5CwZ"
      },
      "source": [
        "## doing matirx multi\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eRCD3t548SW"
      },
      "source": [
        "import torch\n",
        "from torch import tensor"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54EbaIf65V1h"
      },
      "source": [
        "def matmul(a,b):\n",
        "  ar, ac = a.shape\n",
        "  br, bc = b.shape\n",
        "  assert ac==br\n",
        "  c= torch.zeros(ar,bc)\n",
        "  for i in range(ar):\n",
        "    for j in range(bc):\n",
        "      for k in range(ac):\n",
        "        c[i,j] += a[i,k] * b[k,j]\n",
        "  return c"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b-nWzky53OA"
      },
      "source": [
        "m1 = torch.randn(5,28*28)\n",
        "m2 = torch.randn(784,10)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ugXpP85_Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f00b75b-024c-46f6-8fc7-c6382aca3780"
      },
      "source": [
        "# lets see our time\n",
        "%time t1 = matmul(m1, m2)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.07 s, sys: 0 ns, total: 1.07 s\n",
            "Wall time: 1.08 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os9xyD7u6OyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16362e8-1a9d-485d-ca62-444dac33e977"
      },
      "source": [
        "# compare it with time of pytorch built in @\n",
        "%timeit -n 20 t2=m1@m2"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 loops, best of 3: 7.76 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sODOvQpV6s34"
      },
      "source": [
        "How come pytorch so fast? it used C++.\n",
        "we need to vectorize the computations tensor so that we can take advantage of speed of pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd9DjXij7Kiz"
      },
      "source": [
        "### Element wise broadcasting\n",
        "\n",
        "all these are element wise operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEky4oLz6rJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c54193f-63fd-47a1-86ab-67523644e7af"
      },
      "source": [
        "a = tensor([10., 6, -4])\n",
        "b = tensor([2., 8, 7])\n",
        "a+b"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12., 14.,  3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPtdkbs47YPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8a6cc8-3fb8-4307-a6e9-d8f46bac763a"
      },
      "source": [
        "a<b"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPpzB-Y67Zbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d302f0c-0d34-40f6-d9ee-49c98b298554"
      },
      "source": [
        "(a<b).all(), (a==b).all()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(False), tensor(False))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UmFdAXb8RsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfb3a83-b99e-4f08-8dc4-7cac392e33a4"
      },
      "source": [
        "(a+b).mean()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9.6667)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un_g1qK07iSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e140ba7-8770-44d6-c2e8-cf074a361351"
      },
      "source": [
        "(a+b).mean().item()\n",
        "\n",
        "# item converts into a normal boolean number"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.666666984558105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1GMHctx8tXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5237dbbd-f735-4a72-a2d7-a1693cad3c2c"
      },
      "source": [
        "# elementwise cannot be done on different shaped tesnor without broadcasting\n",
        "m = tensor([[0,0,0]])\n",
        "n = tensor([[0,0]])\n",
        "\n",
        "try:\n",
        "  m*n\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYWerAHB-P8X"
      },
      "source": [
        "With elementwise arithmetic, we can remove one of our three nested loops: we can multiply the tensors that correspond to the `i`-th row of `a` and the `j`-th column of `b` before summing all the elements, which will speed things up because the inner loop will now be executed by PyTorch at C speed. \n",
        "\n",
        "To access one column or row, we can simply write `a[i,:]` or `b[:,j]`. The `:` means take everything in that dimension. We could restrict this and take only a slice of that particular dimension by passing a range, like `1:5`, instead of just `:`. In that case, we would take the elements in columns or rows 1 to 4 (the second number is noninclusive). \n",
        "\n",
        "One simplification is that we can always omit a trailing colon, so `a[i,:]` can be abbreviated to `a[i]`. With all of that in mind, we can write a new version of our matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itfCEAZg-ONu"
      },
      "source": [
        "def matmul(a,b):\n",
        "  ar,ac = a.shape\n",
        "  br, bc = b.shape\n",
        "  assert ac==br\n",
        "  c = torch.zeros(ar, bc)\n",
        "  for i in range(ar):\n",
        "    for j in range(bc):\n",
        "      c[i,j] = (a[i] * b[:,j]).sum()\n",
        "  return c"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYjL_3B__QhB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ebc2b3-ff41-4778-d11f-84a02b4d16c1"
      },
      "source": [
        "m1"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7415,  0.7774, -0.8424,  ..., -0.3819, -1.9647,  0.4475],\n",
              "        [-1.7090,  0.4627, -0.6896,  ...,  1.3506,  1.0114, -0.8189],\n",
              "        [ 1.3658,  0.7219, -1.0544,  ..., -0.7875, -2.2777,  1.1233],\n",
              "        [-1.1400,  0.8963,  0.3066,  ..., -2.2176, -0.0887,  0.5459],\n",
              "        [-1.2235, -0.3458, -1.2032,  ..., -0.6197,  0.0967,  0.4876]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOeYql8s_SFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0574201-f344-41da-b04f-e77768402a4e"
      },
      "source": [
        "m2"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9013, -0.6256, -0.5285,  ..., -0.5408,  0.7302, -1.1211],\n",
              "        [ 1.0788,  0.1453,  0.1452,  ...,  0.8222, -0.2700,  0.5019],\n",
              "        [ 1.0633, -0.6268,  0.5282,  ...,  0.6808, -0.9093, -0.6016],\n",
              "        ...,\n",
              "        [-0.0309,  1.0257, -1.0388,  ...,  0.2959,  0.5726,  0.7132],\n",
              "        [-0.2292,  0.7344, -1.9675,  ..., -0.4090,  0.5380, -0.1674],\n",
              "        [-0.1377, -0.4448,  0.7137,  ..., -1.3121,  0.1527,  0.2252]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bppa_MG_TOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c0c347-6c70-45a7-f25e-d6cdd76b0912"
      },
      "source": [
        "%time t3 = matmul(m1,m2)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.78 ms, sys: 0 ns, total: 1.78 ms\n",
            "Wall time: 3.88 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTFYkpap_5QW"
      },
      "source": [
        "### Broadcasting\n",
        "\n",
        "Broadcasting gives specific rules to codify when shapes are compatible when trying to do an elementwise operation, and how the tensor of the smaller shape is expanded to match the tensor of the bigger shape. It's essential to master those rules if you want to be able to write code that executes quickly. In this section, we'll expand our previous treatment of broadcasting to understand these rules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIMTVclX_4p6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171ac51d-c1d1-4875-bb87-757696745b1b"
      },
      "source": [
        "# broadcasting with a scalar\n",
        "\n",
        "a= tensor([10., 6, -4])\n",
        "a > 0"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True,  True, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIsgMZOEauz7",
        "outputId": "f4d60022-9027-4f97-afff-b6be9ed2c404"
      },
      "source": [
        "m = tensor([[1.,2,3], [4,5,6], [7,8,9]])\n",
        "(m-5)/2.73"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.4652, -1.0989, -0.7326],\n",
              "        [-0.3663,  0.0000,  0.3663],\n",
              "        [ 0.7326,  1.0989,  1.4652]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lraYwLP1a8rn",
        "outputId": "3a58d3af-f3ea-4718-bd15-af0acc2178f9"
      },
      "source": [
        "c = tensor([10.,20,30])\n",
        "m = tensor([[1.,2,3],[4,5,6],[7,8,9]])\n",
        "m.shape, c.shape"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 3]), torch.Size([3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQaIcV44bOcH",
        "outputId": "d89688ef-28d9-4331-b9d8-4bbd4070c0fa"
      },
      "source": [
        "m + c"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[11., 22., 33.],\n",
              "        [14., 25., 36.],\n",
              "        [17., 28., 39.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFHO2yaxbXzR",
        "outputId": "d47ea3d1-2a9e-49fa-d976-c8d27c2aaf15"
      },
      "source": [
        "# behind the scenes\n",
        "\n",
        "c.expand_as(m)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 20., 30.],\n",
              "        [10., 20., 30.],\n",
              "        [10., 20., 30.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lta3qVY3bkkv",
        "outputId": "42774960-1df2-457a-d090-d3474d0c0d0f"
      },
      "source": [
        "t = c.expand_as(m)\n",
        "t.storage()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 10.0\n",
              " 20.0\n",
              " 30.0\n",
              "[torch.FloatStorage of size 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVY4w3y-dcFf",
        "outputId": "cda98847-dc00-4b49-e888-c864137fdb11"
      },
      "source": [
        "t.stride(), t.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0, 1), torch.Size([3, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-rnZ8uQejbT",
        "outputId": "0aa4e9ba-f86c-4ad7-b57c-917141c54f99"
      },
      "source": [
        "c = tensor([10.,20])\n",
        "m = tensor([[1.,2,3],[4,5,6]])\n",
        "try:\n",
        "  c+m\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tu14RzNeuCV",
        "outputId": "65399fc9-6dd8-4bd3-c99c-479d1d810093"
      },
      "source": [
        "c = tensor([10.,20,30])\n",
        "m = tensor([[1.,2,3],[4,5,6],[7,8,9]])\n",
        "c = c.unsqueeze(1)\n",
        "m.shape, c.shape"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 3]), torch.Size([3, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ryM-xnIfqM2",
        "outputId": "ede26f89-84e2-4f63-b09c-fabc9e853243"
      },
      "source": [
        "c + m"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[11., 12., 13.],\n",
              "        [24., 25., 26.],\n",
              "        [37., 38., 39.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etN1hRd1fsaS",
        "outputId": "12cccb7e-b9fa-4269-f3f4-fac83e7fba43"
      },
      "source": [
        "t = c.expand_as(m)\n",
        "t.storage()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 10.0\n",
              " 20.0\n",
              " 30.0\n",
              "[torch.FloatStorage of size 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiLNRm8zfyGb",
        "outputId": "3c927c50-1f8d-40f4-8dec-3f17ef990eb0"
      },
      "source": [
        "t.stride(), t.shape"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 0), torch.Size([3, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aro2f-vf1Jl",
        "outputId": "3db8acfa-5064-428e-87ce-93a6b7822510"
      },
      "source": [
        "c = tensor([10.,20,30])\n",
        "c.shape,c.unsqueeze(0).shape,c.unsqueeze(1).shape"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-m9QNrKf-nt",
        "outputId": "d0e40174-d650-4f97-be33-76043c01646e"
      },
      "source": [
        "c.shape, c.unsqueeze(0).shape, c.unsqueeze(1).shape"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J-Izs5sgGVY",
        "outputId": "10927066-18cf-4c9a-d290-c5857618bcfb"
      },
      "source": [
        "c.shape, c[None,:].shape, c[:,None].shape"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsmUBypNgMXf",
        "outputId": "969a8ba4-d40d-4cd7-a9b1-1bf4a7183ac0"
      },
      "source": [
        "c[None].shape,c[..., None].shape"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3]), torch.Size([3, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsNmxOs4g3oU"
      },
      "source": [
        "def matmul(a,b):\n",
        "  ar,ac = a.shape\n",
        "  br,bc = b.shape\n",
        "  assert ac == br\n",
        "  c = torch.zeros(ar, bc)\n",
        "  for i in range(ar):\n",
        "    c[i] = (a[i].unsqueeze(-1)*b).sum(dim=0)\n",
        "  return c"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_5B83khhPYz",
        "outputId": "fc17de92-97ea-4a8a-f678-7cf4ebbcc5af"
      },
      "source": [
        "%timeit -n 20 t4 = matmul(m1,m2)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 loops, best of 3: 473 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JudgAS-xhdmz"
      },
      "source": [
        "### Broadcasting Rules\n",
        "\n",
        "Arrays do not need to have the same number of dimensions. For example, if you have a 256×256×3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with three values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n",
        "\n",
        "```\n",
        "Image  (3d tensor): 256 x 256 x 3\n",
        "Scale  (1d tensor):  (1)   (1)  3\n",
        "Result (3d tensor): 256 x 256 x 3\n",
        "```\n",
        "    \n",
        "However, a 2D tensor of size 256×256 isn't compatible with our image:\n",
        "\n",
        "```\n",
        "Image  (3d tensor): 256 x 256 x   3\n",
        "Scale  (1d tensor):  (1)  256 x 256\n",
        "Error\n",
        "```\n",
        "\n",
        "In our earlier examples we had with a 3×3 matrix and a vector of size 3, broadcasting was done on the rows:\n",
        "\n",
        "```\n",
        "Matrix (2d tensor):   3 x 3\n",
        "Vector (1d tensor): (1)   3\n",
        "Result (2d tensor):   3 x 3\n",
        "```\n",
        "\n",
        "As an exercise, try to determine what dimensions to add (and where) when you need to normalize a batch of images of size `64 x 3 x 256 x 256` with vectors of three elements (one for the mean and one for the standard deviation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVyBRADLjaP8"
      },
      "source": [
        "### Einstein summmation\n",
        "\n",
        "This is a compact representation for combining products and sums in general way\n",
        "\n",
        "`ik, kj -> ij`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXeZnyE4hW2w"
      },
      "source": [
        "def matmul(a,b):\n",
        "  return torch.einsum('ik, kj->ij', a, b)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEnM_4m2ksDQ",
        "outputId": "ca99cc3e-dac0-48db-aedb-1c5a371a2c37"
      },
      "source": [
        "try:\n",
        "  torch.einsum('bik, bkj->bij',a,b)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dimension mismatch for operand 0: equation 3 tensor 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IIbfDX1lsMs",
        "outputId": "57bee56c-27c4-4dca-9b4c-eb9dcde5d8fd"
      },
      "source": [
        "%timeit -n 20 t5 = matmul(m1, m2)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 loops, best of 3: 87 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAOqfY6ImWL1"
      },
      "source": [
        "## Forward and Backward Passes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RaM_fdislR3"
      },
      "source": [
        "# defining and initializing a layer\n",
        "\n",
        "def lin(x, w, b):\n",
        "  return x @w + b\n",
        "\n",
        "# the activationfunction we will use will be relu\n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCddWwQ6syd5"
      },
      "source": [
        "# lets take random tesnors as input and targets\n",
        "\n",
        "x = torch.randn(200,100)\n",
        "y = torch.randn(200)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qjx2nnks7t6"
      },
      "source": [
        "# for out two layer model we will need two weight matricss \n",
        "# and two bias vectors\n",
        "# we initiliaze the weights randomly and the bias at zero\n",
        "\n",
        "w1 = torch.randn(100,50)\n",
        "b1 = torch.zeros(50)\n",
        "w2 = torch.randn(50,1)\n",
        "b2 = torch.zeros(1)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WO0DVuRuqTX",
        "outputId": "3b5f2569-b25d-457b-af68-496a12ef1a0d"
      },
      "source": [
        "l1 = lin(x, w1, b1)\n",
        "l1.shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200, 50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD9P3RY_uvF1",
        "outputId": "fb619105-a2b1-45a9-b1d2-bba46483fd37"
      },
      "source": [
        "l1.mean(), l1.std()\n",
        "\n",
        "# there is a problem here though themean is clos to zero\n",
        "# however the standard deviation is from 1 to 10\n",
        "# it means they will scale the number by 10 everyt time\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.0778), tensor(10.1446))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q_mQLybuyFI",
        "outputId": "4be67a9e-476b-47f9-967e-57610f796a81"
      },
      "source": [
        "x = torch.randn(200,100)\n",
        "for i in range(50):\n",
        "  x = x @torch.randn(100,100)\n",
        "x[0:5,0:5]\n",
        "\n",
        "# if we do multiply 50 times x then the number is too big so it gives nan"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msf8Hr-gvGR7",
        "outputId": "cdac1f6b-538d-484d-9ed0-d30c6e40b74f"
      },
      "source": [
        "x = torch.randn(200,100)\n",
        "for i in range(50):\n",
        "  x = x @ (torch.randn(100,100) * 0.01)\n",
        "x[0:5,0:5]\n",
        "\n",
        "# if we use too small weights then we would be left with 0 everywhere"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIGLcA3X7ijU"
      },
      "source": [
        "1. We can compute the exact value to use mathematically, as illustrated by Xavier Glorot and Yoshua Bengio in [\"Understanding the Difficulty of Training Deep Feedforward Neural Networks\"](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). The right scale for a given layer is $1/\\sqrt{n_{in}}$, where $n_{in}$ represents the number of inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klirMEOu57tl",
        "outputId": "807af377-101f-4bd5-d7c6-9d96997c1bf7"
      },
      "source": [
        "# therefore we need to scale our weights matrics so \n",
        "# that the standard deviation remains one\n",
        "# since we have 100 inputswe scale it by 0.1\n",
        "\n",
        "x = torch.randn(200,100)\n",
        "for i in range(50):\n",
        "  x = x @ (torch.randn(100,100) * 0.1)\n",
        "x[0:5, 0:5]\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8013, -0.1897, -0.4023, -0.4735,  0.1845],\n",
              "        [ 0.2788, -0.1554,  0.8431,  0.1736, -0.5525],\n",
              "        [ 0.0986,  0.4494,  0.1055,  0.3769, -0.0502],\n",
              "        [ 0.9886, -0.3836, -0.2021,  0.1302, -0.4169],\n",
              "        [-1.0789,  0.4563,  0.0734, -0.1716,  0.6076]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yfvJAMJ7zCJ",
        "outputId": "b10c6817-2c18-4d2c-ba68-4f18c1efa3fd"
      },
      "source": [
        "x.std()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9380)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsDDEZXh75ws"
      },
      "source": [
        "# going back to our neural network\n",
        "\n",
        "x = torch.randn(200,100)\n",
        "y = torch.randn(200)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht6Yn0ur8Cdw"
      },
      "source": [
        "# for our weights we will use the right scale \n",
        "# thisis Xavier initialization\n",
        "\n",
        "from math import sqrt\n",
        "w1 = torch.randn(100,50)/sqrt(100)\n",
        "b1 = torch.zeros(50)\n",
        "w2 = torch.randn(50,1)/sqrt(50)\n",
        "b2 = torch.zeros(1)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jus_EgIO8YxX",
        "outputId": "53bbe9ba-3a36-457d-872f-03b166514349"
      },
      "source": [
        "l1 = lin(x, w1, b1)\n",
        "l1.mean(), l1.std()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.0078), tensor(1.0302))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xC6YQE28gqp"
      },
      "source": [
        "# now that we have mean and e=deviation under control\n",
        "# we have defined a relu\n",
        "#relu removes negatives and clamps at 0\n",
        "\n",
        "def relu(x):\n",
        "  return x.clamp_min(0.)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-EK9w5t8ly8",
        "outputId": "c44a7ddd-f6c8-4101-ee15-a176ff26288b"
      },
      "source": [
        "l2 = relu(l1)\n",
        "l2.mean(), l2.std()\n",
        "\n",
        "# this is too less"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.4058), tensor(0.6044))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTQ-1N4K8rjH",
        "outputId": "51f6a625-19e2-4b51-8858-a2fda863629e"
      },
      "source": [
        "x = torch.randn(200,100)\n",
        "for i in range(50):\n",
        "  x = relu(x @ (torch.randn(100,100)*0.1))\n",
        "x[0:5,0:5]"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.5434e-09, 0.0000e+00, 1.0853e-08, 0.0000e+00, 7.4718e-09],\n",
              "        [4.3009e-09, 0.0000e+00, 1.5764e-09, 0.0000e+00, 7.0184e-09],\n",
              "        [3.9585e-09, 0.0000e+00, 7.0260e-09, 0.0000e+00, 1.9430e-09],\n",
              "        [6.6478e-09, 0.0000e+00, 2.5971e-09, 0.0000e+00, 1.0579e-08],\n",
              "        [1.2210e-08, 0.0000e+00, 5.3824e-10, 0.0000e+00, 1.7850e-08]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25bUef1t85Br",
        "outputId": "8cc91e3a-05b9-4387-9591-a7ab8f360770"
      },
      "source": [
        "x = torch.randn(200,100)\n",
        "for i in range(50): x = relu(x@ (torch.randn(100,100) *sqrt(2/100)))\n",
        "x[0:5, 0:5]"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7071, 0.0000, 2.1879, 0.3887, 0.0000],\n",
              "        [0.5339, 0.0000, 1.4717, 0.4260, 0.0000],\n",
              "        [0.4231, 0.0000, 1.2220, 0.3425, 0.0000],\n",
              "        [0.6724, 0.0000, 2.0828, 0.3392, 0.0000],\n",
              "        [0.8956, 0.0000, 2.5786, 0.6576, 0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wLx1IqvARHT"
      },
      "source": [
        "x =  torch.randn(200,100)\n",
        "y = torch.randn(200)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gXjmFdTAXli"
      },
      "source": [
        "w1= torch.randn(100,50) * sqrt(2/100)\n",
        "b1 = torch.randn(50) * sqrt(2/50)\n",
        "w2 = torch.randn(50,1) * sqrt(2/50)\n",
        "b2 = torch.zeros(1)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYtfqWHHArBF",
        "outputId": "c985fcee-c47c-47b3-a8da-51784c82499f"
      },
      "source": [
        "l1 = lin(x, w1, b1)\n",
        "l2 = relu(l1)\n",
        "l2.mean(), l2.std()\n",
        "\n",
        "#much better"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5606), tensor(0.8227))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMwX_a-YA0N3"
      },
      "source": [
        "# letss define the model\n",
        "# this is the forwrd pass\n",
        "def model(x):\n",
        "  l1 = lin(x, w1, b1)\n",
        "  l2 = relu(l1)\n",
        "  l3 = lin(l2,w2,b2)\n",
        "  return l3 "
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbwHWmqCDCy-",
        "outputId": "0bf6bf0a-ecd0-4bb3-f68b-a0578d9d2bab"
      },
      "source": [
        "out = model(x)\n",
        "out.shape"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAJlqGovDKvu"
      },
      "source": [
        "# we can get rid of this trailing 1 dimension by using the\n",
        "# squeeze function\n",
        "\n",
        "def mse(output, targ):\n",
        "  return (output.squeeze(-1) - targ).pow(2).mean()\n",
        "\n",
        "# this is our loss\n",
        "loss = mse(out,y)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dc6QDvWDrGK"
      },
      "source": [
        "## gradients and backward pass\n",
        "\n",
        "We've seen that PyTorch computes all the gradients we need with a magic call to `loss.backward`, but let's explore what's happening behind the scenes.\n",
        "\n",
        "Now comes the part where we need to compute the gradients of the loss with respect to all the weights of our model, so all the floats in `w1`, `b1`, `w2`, and `b2`. For this, we will need a bit of math—specifically the *chain rule*. This is the rule of calculus that guides how we can compute the derivative of a composed function:\n",
        "\n",
        "$$(g \\circ f)'(x) = g'(f(x)) f'(x)$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neZ3ocRzESDl"
      },
      "source": [
        "Jeremy: I find this notation very hard to wrap my head around, so instead I like to think of it as: if `y = g(u)` and `u=f(x)`; then `dy/dx = dy/du * du/dx`. The two notations mean the same thing, so use whatever works for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SStwRrvsEdcf"
      },
      "source": [
        "Our loss is a big composition of different functions: mean squared error (which is in turn the composition of a mean and a power of two), the second linear layer, a ReLU and the first linear layer. For instance, if we want the gradients of the loss with respect to `b2` and our loss is defined by:\n",
        "\n",
        "```\n",
        "loss = mse(out,y) = mse(lin(l2, w2, b2), y)\n",
        "```\n",
        "\n",
        "The chain rule tells us that we have:\n",
        "$$\\frac{\\text{d} loss}{\\text{d} b_{2}} = \\frac{\\text{d} loss}{\\text{d} out} \\times \\frac{\\text{d} out}{\\text{d} b_{2}} = \\frac{\\text{d}}{\\text{d} out} mse(out, y) \\times \\frac{\\text{d}}{\\text{d} b_{2}} lin(l_{2}, w_{2}, b_{2})$$\n",
        "\n",
        "To compute the gradients of the loss with respect to $b_{2}$, we first need the gradients of the loss with respect to our output $out$. It's the same if we want the gradients of the loss with respect to $w_{2}$. Then, to get the gradients of the loss with respect to $b_{1}$ or $w_{1}$, we will need the gradients of the loss with respect to $l_{1}$, which in turn requires the gradients of the loss with respect to $l_{2}$, which will need the gradients of the loss with respect to $out$.\n",
        "\n",
        "So to compute all the gradients we need for the update, we need to begin from the output of the model and work our way *backward*, one layer after the other—which is why this step is known as *backpropagation*. We can automate it by having each function we implemented (`relu`, `mse`, `lin`) provide its backward step: that is, how to derive the gradients of the loss with respect to the input(s) from the gradients of the loss with respect to the output.\n",
        "\n",
        "Here we populate those gradients in an attribute of each tensor, a bit like PyTorch does with `.grad`. \n",
        "\n",
        "The first are the gradients of the loss with respect to the output of our model (which is the input of the loss function). We undo the `squeeze` we did in `mse`, then we use the formula that gives us the derivative of $x^{2}$: $2x$. The derivative of the mean is just $1/n$ where $n$ is the number of elements in our input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufCX41KODXrC"
      },
      "source": [
        "def mse_grad(inp, targ):\n",
        "  inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1)/ inp.shape[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma_5w7EhKdFa"
      },
      "source": [
        "def relu_grad(inp, out):\n",
        "  inp.g = (inp>0).float() *out.g"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFJfqp-TKkkk"
      },
      "source": [
        "def lin_grad(inp, out, w, b):\n",
        "  inp.g = out.g @ w.t()\n",
        "  w.g = inp.t() @ out.g \n",
        "  b.g = out.g.sum(0)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKhhVuegLCO_"
      },
      "source": [
        "## Sympy\n",
        "\n",
        "for doing symboliccalculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iZxGvkKK-x3",
        "outputId": "0fb1d1d6-6a91-4885-f78f-b18f71d15138"
      },
      "source": [
        "from sympy import symbols, diff\n",
        "sx, sy = symbols('sx sy')\n",
        "diff(sx ** 2, sx)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2*sx"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cYsMykiLWd4"
      },
      "source": [
        "def forward_and_backward(inp, targ):\n",
        "  l1 = inp @ w1 + b1\n",
        "  l2 = relu(l1)\n",
        "  out = l2 @ w2 + b2\n",
        "  loss = mse(out, targ)\n",
        "\n",
        "  mse_grad(out, targ)\n",
        "  lin_grad(l2, out, w2, b2)\n",
        "  relu_grad(l1, l2)\n",
        "  lin_grad(inp, l1, w1, b1)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inwz-9OOM-wu"
      },
      "source": [
        "## refactoring the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWQVDhtEMms9"
      },
      "source": [
        "class Relu():\n",
        "    def __call__(self, inp):\n",
        "        self.inp = inp\n",
        "        self.out = inp.clamp_min(0.)\n",
        "        return self.out\n",
        "    \n",
        "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8s6iIoUNHl6"
      },
      "source": [
        "`__call__` is a magic name in Python that will make our class callable. This is what will be executed when we type `y = Relu()(x)`. We can do the same for our linear layer and the MSE loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InIZlEvQM7lZ"
      },
      "source": [
        "class Lin():\n",
        "  def __init__(self, w, b):\n",
        "    self.w, self.b = w, b\n",
        "  \n",
        "  def __call__(self, inp):\n",
        "    self.inp = inp\n",
        "    self.out = inp@self.w + self.b\n",
        "    return self.out\n",
        "  \n",
        "  def backward(self):\n",
        "    self.inp.g = self.out.g @ self.w.t()\n",
        "    self.w.g = self.inp.t() @ self.out.g\n",
        "    self.b.g = self.out.g.sum(0)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIxGgGt7Nq77"
      },
      "source": [
        "class Mse():\n",
        "  def __call__(self, inp, targ):\n",
        "    self.inp = inp\n",
        "    self.targ = targ\n",
        "    self.out = (inp.squeeze() - targ).pow(2).mean()\n",
        "    return self.out\n",
        "  \n",
        "  def backward(self):\n",
        "    x = (self.inp.squeeze()-self.targ).unsqueeze(-1)\n",
        "    self.inp.g  = 2.*x/self.targ.shape[0]\n",
        "    "
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xt9j_xwOGUD"
      },
      "source": [
        "class Model():\n",
        "  def __init__(self, w1, b1, w2, b2):\n",
        "    self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2) ]\n",
        "    self.loss = Mse()\n",
        "\n",
        "  def __call__(self, x, targ):\n",
        "    for l in self.layers: x = l(x)\n",
        "    return self.loss(x, targ)\n",
        "  \n",
        "  def backward(self):\n",
        "    self.loss.backward()\n",
        "    for l in reversed(self.layers):\n",
        "      l.backward()\n",
        "      "
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az_BtAPpOo6_"
      },
      "source": [
        "model = Model(w1, b1, w2, b2)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RELhmeUOs2w"
      },
      "source": [
        "loss = model(x, y) # forward pass"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30FRPaoeOwUs"
      },
      "source": [
        "model.backward() # backward pass"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npNqj-SMSmXJ"
      },
      "source": [
        "## Going to pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuxcndsmSay6"
      },
      "source": [
        "class LayerFunction():\n",
        "  def __call__(self, *args):\n",
        "    self.args = args\n",
        "    self.out = self.forward(*args)\n",
        "    return self.out\n",
        "  \n",
        "  def forward(self):\n",
        "    raise Exception('not implemented')\n",
        "  def bwd(self):\n",
        "    raise Exception('not implemented')\n",
        "  def backward(self):\n",
        "    self.bwd(self.out, *self.args)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpbFMdNuTCwQ"
      },
      "source": [
        "# lets just implement the layer functionfor creating varios layers\n",
        "\n",
        "class Relu(LayerFunction):\n",
        "  def forward(self, inp):\n",
        "    return inp.clamp_min(0.)\n",
        "  def bwd(self, out, inp):\n",
        "    inp.g = (inp>0).float() * out.g"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1ED9IYUTmJI"
      },
      "source": [
        "class Lin(LayerFunction):\n",
        "  def __init__(self, w, b):\n",
        "    self.w, self.b = w, b\n",
        "  \n",
        "  def forward(self, inp):\n",
        "    return inp@self.w  + self.b\n",
        "  \n",
        "  def bwd(self, out, inp):\n",
        "    inp.g = out.g @self.w.t()\n",
        "    self.w.g = self.inp.t() @ slef.out.g\n",
        "    self.b.g = out.g.sum(0)\n",
        "  \n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wYTm9mqXFQv"
      },
      "source": [
        "class Mse(LayerFunction):\n",
        "  def forward( self, inp, targ):\n",
        "    return (inp.squeeze() - targ).pow(2).mean()\n",
        "  \n",
        "  def bwd(self, out, inp, targ):\n",
        "    inp.g = 2 * (inp.squeeze() - targ).unsqueeze(-1)/targ.shape[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpqwH9FqXhj7"
      },
      "source": [
        "The rest of our model can be the same as before. This is getting closer and closer to what PyTorch does. Each basic function we need to differentiate is written as a `torch.autograd.Function` object that has a `forward` and a `backward` method. PyTorch will then keep trace of any computation we do to be able to properly run the backward pass, unless we set the `requires_grad` attribute of our tensors to `False`.\n",
        "\n",
        "Writing one of these is (almost) as easy as writing our original classes. The difference is that we choose what to save and what to put in a context variable (so that we make sure we don't save anything we don't need), and we return the gradients in the `backward` pass. It's very rare to have to write your own `Function` but if you ever need something exotic or want to mess with the gradients of a regular function, here is how to write one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxHFLu-MXfDv"
      },
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class MyRelu(Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, i):\n",
        "    result = i.clamp_min(0.)\n",
        "    ctx.save_for_backward(i)\n",
        "    return result\n",
        "  \n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    i, = ctx.saved_tensors\n",
        "    return grad_output * (i>0).float()"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5BLEXavYDcc"
      },
      "source": [
        "The structure used to build a more complex model that takes advantage of those `Function`s is a `torch.nn.Module`. This is the base structure for all models, and all the neural nets you have seen up until now were from that class. It mostly helps to register all the trainable parameters, which as we've seen can be used in the training loop.\n",
        "\n",
        "To implement an `nn.Module` you just need to:\n",
        "\n",
        "- Make sure the superclass `__init__` is called first when you initialize it.\n",
        "- Define any parameters of the model as attributes with `nn.Parameter`.\n",
        "- Define a `forward` function that returns the output of your model.\n",
        "\n",
        "As an example, here is the linear layer from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVoVlyZwYBpZ"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LinearLayer(nn.Module):\n",
        "  def __init__(self, n_in, n_out):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.randn(n_out,n_in) * sqrt(2/n_in))\n",
        "    self.bias = nn.Parameter(torch.zeros(n_out))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return x @ self.weight.t() + self.bias"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2cTFOvSYub5",
        "outputId": "021a71f7-c9c4-4fc6-8f9a-786529ed8d76"
      },
      "source": [
        "lin = LinearLayer(10,2)\n",
        "p1,p2 = lin.parameters()\n",
        "p1.shape, p2.shape"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 10]), torch.Size([2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvKZzoQzZMVw"
      },
      "source": [
        "It is thanks to this feature of `nn.Module` that we can just say `opt.step()` and have an optimizer loop through the parameters and update each one.\n",
        "\n",
        "Note that in PyTorch, the weights are stored as an `n_out x n_in` matrix, which is why we have the transpose in the forward pass.\n",
        "\n",
        "By using the linear layer from PyTorch (which uses the Kaiming initialization as well), the model we have been building up during this chapter can be written like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK-1s3XUZIy1"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, n_in,nh, n_out):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)\n",
        "    )\n",
        "    self.loss = mse\n",
        "  \n",
        "  def forward(self, x, targ):\n",
        "    return self.loss(self.layers(x).squeeze(), targ)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4om0Xek_Zxr3"
      },
      "source": [
        "fastai provides its own variant of `Module` that is identical to `nn.Module`, but doesn't require you to call `super().__init__()` (it does that for you automatically):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeECjnSDaR-9"
      },
      "source": [
        "from fastai.basics import Module"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6jIoZNCZuBA"
      },
      "source": [
        "class Model(Module):\n",
        "  def __init__(self, n_in, nh, n_out):\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh, n_out)\n",
        "    )\n",
        "    self.loss = mse\n",
        "  \n",
        "  def forward(self, x, targ):\n",
        "    return self.loss(self.layers(x).squeeze(), targ)\n",
        "\n",
        "# we will use it next chapter"
      ],
      "execution_count": 180,
      "outputs": []
    }
  ]
}