{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is an excerpt taken from deep learning with pytorch chapter 4. \nconverting into text."},{"metadata":{},"cell_type":"markdown","source":"### Approach"},{"metadata":{},"cell_type":"markdown","source":"There are two particularly intuitive levels at which networks operate on text: at the\ncharacter level, by processing one character at a time, and at the word level, where\nindividual words are the finest-grained entities to be seen by the network. The technique with which we encode text information into tensor form is the same whether we\noperate at the character level or the word level. And it’s not magic, either. We stumbled upon it earlier: one-hot encoding."},{"metadata":{},"cell_type":"markdown","source":"### Pride and pejudice"},{"metadata":{},"cell_type":"markdown","source":" Let’s start with a character-level example. First, let’s get some text to process. An\namazing resource here is Project Gutenberg (www.gutenberg.org), a volunteer effort\nto digitize and archive cultural work and make it available for free in open formats,\nincluding plain text files. If we’re aiming at larger-scale corpora, the Wikipedia corpus\nstands out: it’s the complete collection of Wikipedia articles, containing 1.9 billion\nwords and more than 4.4 million articles. Several other corpora can be found at the\nEnglish Corpora website (www.english-corpora.org).\n Let’s load Jane Austen’s Pride and Prejudice from the Project Gutenberg website:\nwww.gutenberg.org/files/1342/1342-0.txt. We’ll just save the file and read it in\n(code/p1ch4/5_text_jane_austen.ipynb)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# getting the text file\n!wget \"www.gutenberg.org/files/1342/1342-0.txt\" ","execution_count":2,"outputs":[{"output_type":"stream","text":"--2021-01-31 06:13:55--  http://www.gutenberg.org/files/1342/1342-0.txt\nResolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\nConnecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 799738 (781K) [text/plain]\nSaving to: ‘1342-0.txt’\n\n1342-0.txt          100%[===================>] 780.99K   609KB/s    in 1.3s    \n\n2021-01-31 06:13:57 (609 KB/s) - ‘1342-0.txt’ saved [799738/799738]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('./1342-0.txt', encoding='utf8') as f:\n    text = f.read()","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One hot encoding of characters\n\nThere’s one more detail we need to take care of before we proceed: encoding. This is\na pretty vast subject, and we will just touch on it. Every written character is represented\nby a code: a sequence of bits of appropriate length so that each character can be\nuniquely identified. The simplest such encoding is ASCII (American Standard Code\nfor Information Interchange), which dates back to the 1960s. ASCII encodes 128 characters using 128 integers. For instance, the letter a corresponds to binary 1100001 or\ndecimal 97, the letter b to binary 1100010 or decimal 98, and so on. The encoding fits\n8 bits, which was a big bonus in 1965."},{"metadata":{},"cell_type":"markdown","source":"At this point, we need to parse through the characters in the text and provide a\none-hot encoding for each of them. Each character will be represented by a vector of\nlength equal to the number of different characters in the encoding. This vector will\ncontain all zeros except a one at the index corresponding to the location of the character in the encoding.\n We first split our text into a list of lines and pick an arbitrary line to focus on:"},{"metadata":{"trusted":true},"cell_type":"code","source":"text[:10]","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'\\ufeff\\nThe Proj'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines = text.split('\\n')\nline = lines[200]\nline","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"'      Michaelmas, and some of his servants are to be in the house by'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"creating a one hot encoding for the whole line\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"letter_t = torch.zeros(len(line), 128)\nletter_t.shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"torch.Size([68, 128])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Note that letter_t holds a one-hot-encoded character per row. Now we just have to\nset a one on each row in the correct position so that each row represents the correct\ncharacter. The index where the one has to be set corresponds to the index of the character in the encoding:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, letter in enumerate(line.lower().strip()):\n    letter_index = ord(letter) if ord(letter) < 128 else 0\n    # since we are only covering till ascii 128\n    letter_t[i][letter_index] = 1","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"letter_t[10],line[10] # one hot representation of letter a","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0.]),\n 'a')"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### One hot encoding of the whole words"},{"metadata":{},"cell_type":"markdown","source":"We have one-hot encoded our sentence into a representation that a neural network\ncould digest. Word-level encoding can be done the same way by establishing a vocabulary and one-hot encoding sentences—sequences of words—along the rows of our\ntensor. Since a vocabulary has many words, this will produce very wide encoded vectors, which may not be practical. We will see in the next section that there is a more\nefficient way to represent text at the word level, using embeddings. For now, let’s stick\nwith one-hot encodings and see what happens.\n We’ll define clean_words, which takes text and returns it in lowercase and\nstripped of punctuation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_words(input_str):\n    punctuation = './;:\",!?_$*-()'\n    wordlist = input_str.lower().replace('\\n', ' ').split()\n    wordlist = [word.strip(punctuation) for word in wordlist]\n    return wordlist\n    ","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_in_line = clean_words(line)\nline, words_in_line","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"('      Michaelmas, and some of his servants are to be in the house by',\n ['michaelmas',\n  'and',\n  'some',\n  'of',\n  'his',\n  'servants',\n  'are',\n  'to',\n  'be',\n  'in',\n  'the',\n  'house',\n  'by'])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"building a mapping of words to indexes in our encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordlist = sorted(set(clean_words(text)))\nwordlist[1000:1010]","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"['brightest',\n 'brighton',\n 'brighton!”',\n 'brighton?”',\n 'brilliancy',\n 'bring',\n 'bringing',\n 'brings',\n 'bring—good',\n 'brink']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2index_dict  = {word : i for (i, word) in enumerate(wordlist)}","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word2index_dict), word2index_dict['impossible']","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"(8450, 3777)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Note that word2index_dict is now a dictionary with words as keys and an integer as a\nvalue. We will use it to efficiently find the index of a word as we one-hot encode it.\nLet’s now focus on our sentence: we break it up into words and one-hot encode it— that is, we populate a tensor with one one-hot-encoded vector per word. We create an\nempty vector and assign the one-hot-encoded values of the word in the sentence:"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_t = torch.zeros(len(words_in_line), len(word2index_dict))\nfor i, word in enumerate(words_in_line):\n    word_index = word2index_dict[word]\n    word_t[i][word_index] =1 \n    print('{:2} {:4} {}'.format(i, word_index, word))","execution_count":33,"outputs":[{"output_type":"stream","text":" 0 4696 michaelmas\n 1  436 and\n 2 6808 some\n 3 5090 of\n 4 3588 his\n 5 6571 servants\n 6  547 are\n 7 7375 to\n 8  760 be\n 9 3804 in\n10 7271 the\n11 3630 house\n12 1051 by\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"At this point, tensor represents one sentence of length 11 in an encoding space of size\n8450, the number of words in our dictionary"},{"metadata":{},"cell_type":"markdown","source":"Options for splitting text (and using the embeddings we’ll look at in the next section).\n The choice between character-level and word-level encoding leaves us to make a\ntrade-off. In many languages, there are significantly fewer characters than words: representing characters has us representing just a few classes, while representing words\nrequires us to represent a very large number of classes and, in any practical application, deal with words that are not in the dictionary. On the other hand, words convey\nmuch more meaning than individual characters, so a representation of words is considerably more informative by itself."},{"metadata":{},"cell_type":"markdown","source":"### why text embeddings"},{"metadata":{},"cell_type":"markdown","source":"One-hot encoding is a very useful technique for representing categorical data in tensors. However, as we have anticipated, one-hot encoding starts to break down when\nthe number of items to encode is effectively unbound, as with words in a corpus. In\njust one book, we had over 8,000 items!"},{"metadata":{},"cell_type":"markdown","source":"### What text embeddings"},{"metadata":{},"cell_type":"markdown","source":"How can we compress our encoding down to a more manageable size and put a cap on the size growth? Well, instead of vectors of many zeros and a single one, we can use vectors of floating-point numbers. A vector of, say, 100 floating-point numbers can\nindeed represent a large number of words. The trick is to find an effective way to map\nindividual words into this 100-dimensional space in a way that facilitates downstream\nlearning. This is called an embedding.\n\nIn principle, we could simply iterate over our vocabulary and generate a set of 100\nrandom floating-point numbers for each word. This would work, in that we could\ncram a very large vocabulary into just 100 numbers, but it would forgo any concept of\ndistance between words based on meaning or context. A model using this word\nembedding would have to deal with very little structure in its input vectors. An ideal\nsolution would be to generate the embedding in such a way that words used in similar\ncontexts mapped to nearby regions of the embedding."},{"metadata":{},"cell_type":"markdown","source":"### Example"},{"metadata":{},"cell_type":"markdown","source":"Well, if we were to design a solution to this problem by hand, we might decide to\nbuild our embedding space by choosing to map basic nouns and adjectives along the\naxes. We can generate a 2D space where axes map to nouns—fruit (0.0-0.33), flower\n(0.33-0.66), and dog (0.66-1.0)—and adjectives—red (0.0-0.2), orange (0.2-0.4), yellow\n(0.4-0.6), white (0.6-0.8), and brown (0.8-1.0). Our goal is to take actual fruit, flowers,\nand dogs and lay them out in the embedding.\n As we start embedding words, we can map apple to a number in the fruit and red\nquadrant. Likewise, we can easily map tangerine, lemon, lychee, and kiwi (to round out\nour list of colorful fruits). Then we can start on flowers, and assign rose, poppy, daffodil,\nlily, and … Hmm. Not many brown flowers out there. Well, sunflower can get flower, yellow, and brown, and then daisy can get flower, white, and yellow. Perhaps we should\nupdate kiwi to map close to fruit, brown, and green.\n8\n For dogs and color, we can embed\nredbone near red; uh, fox perhaps for orange; golden retriever for yellow, poodle for white, and\n… most kinds of dogs are brown"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}