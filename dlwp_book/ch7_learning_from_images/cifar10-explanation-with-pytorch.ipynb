{"cells":[{"metadata":{},"cell_type":"markdown","source":"Taken from chapter7 of deep learning with pytorch."},{"metadata":{},"cell_type":"markdown","source":"We will approach a simple image recognition problem step by step, building from\na simple neural network like the one we defined in the last chapter. This time, instead\nof a tiny dataset of numbers, we’ll use a more extensive dataset of tiny images. Let’s\ndownload the dataset first and get to work preparing it for use."},{"metadata":{},"cell_type":"markdown","source":"### A dataset of tiny images"},{"metadata":{},"cell_type":"markdown","source":"There is nothing like an intuitive understanding of a subject, and there is nothing to\nachieve that like working on simple data. One of the most basic datasets for image\nrecognition is the handwritten digit-recognition dataset known as MNIST. Here\nwe will use another dataset that is similarly simple and a bit more fun. It’s called\nCIFAR-10, and, like its sibling CIFAR-100, it has been a computer vision classic for\na decade."},{"metadata":{},"cell_type":"markdown","source":"CIFAR-10 consists of 60,000 tiny 32 × 32 color (RGB) images, labeled with an integer corresponding to 1 of 10 classes: airplane (0), automobile (1), bird (2), cat (3),\ndeer (4), dog (5), frog (6), horse (7), ship (8), and truck (9).1\n Nowadays, CIFAR-10 is\nconsidered too simple for developing or validating new research, but it serves our\nlearning purposes just fine. We will use the torchvision module to automatically\ndownload the dataset and load it as a collection of PyTorch tensors"},{"metadata":{},"cell_type":"markdown","source":"### Downloading CIFAR-10\n\nAs we anticipated, let’s import torchvision and use the datasets module to download the CIFAR-10 data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import datasets","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"data_path = './'\n\ncifar10 =  datasets.CIFAR10(data_path, train=True, download=True)\ncifar10_val = datasets.CIFAR10(data_path, train=False, download=True)","execution_count":26,"outputs":[{"output_type":"stream","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Just like CIFAR10, the datasets submodule gives us precanned access to the most\npopular computer vision datasets, such as MNIST, Fashion-MNIST, CIFAR-100,\nSVHN, Coco, and Omniglot. In each case, the dataset is returned as a subclass of\ntorch.utils.data.Dataset. We can see that the method-resolution order of our\ncifar10 instance includes it as a base class:"},{"metadata":{"trusted":true},"cell_type":"code","source":"type(cifar10).__mro__ # method resolution order","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"(torchvision.datasets.cifar.CIFAR10,\n torchvision.datasets.vision.VisionDataset,\n torch.utils.data.dataset.Dataset,\n typing.Generic,\n object)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### The dataset class"},{"metadata":{},"cell_type":"markdown","source":"It’s a good time to discover what being a subclass of torch.utils.data.Dataset\nmeans in practice.It\nis an object that isrequired to implement twomethods: `__len__` and `__getitem__`"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cifar10)","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"50000"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Similarly, since the dataset is equipped with the __getitem__ method, we can use the\nstandard subscript for indexing tuples and lists to access individual items. Here, we get\na PIL (Python Imaging Library, the PIL package) image with our desired output—an\ninteger with the value 1, corresponding to “automobile"},{"metadata":{"trusted":true},"cell_type":"code","source":"img, label = cifar10[99]\nimg, label","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"(<PIL.Image.Image image mode=RGB size=32x32 at 0x7F201A24CA90>, 1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img)\nplt.show()","execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"### Data transforms"},{"metadata":{},"cell_type":"markdown","source":"That’s all very nice, but we’ll likely need a way to convert the PIL image to a PyTorch\ntensor before we can do anything with it. That’s where torchvision.transforms\ncomes in. This module defines a set of composable, function-like objects that can be\npassed as an argument to a torchvision dataset such as datasets.CIFAR10(…), and\nthat perform transformations on the data after it is loaded but before it is returned by\n__getitem__. We can see the list of available objects as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import transforms\n\ndir(transforms)","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"['CenterCrop',\n 'ColorJitter',\n 'Compose',\n 'ConvertImageDtype',\n 'FiveCrop',\n 'GaussianBlur',\n 'Grayscale',\n 'Lambda',\n 'LinearTransformation',\n 'Normalize',\n 'PILToTensor',\n 'Pad',\n 'RandomAffine',\n 'RandomApply',\n 'RandomChoice',\n 'RandomCrop',\n 'RandomErasing',\n 'RandomGrayscale',\n 'RandomHorizontalFlip',\n 'RandomOrder',\n 'RandomPerspective',\n 'RandomResizedCrop',\n 'RandomRotation',\n 'RandomSizedCrop',\n 'RandomVerticalFlip',\n 'Resize',\n 'Scale',\n 'TenCrop',\n 'ToPILImage',\n 'ToTensor',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n 'functional',\n 'functional_pil',\n 'functional_tensor',\n 'transforms']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Among those transforms, we can spot ToTensor, which turns NumPy arrays and PIL\nimages to tensors. It also takes care to lay out the dimensions of the output tensor as\nC × H × W (channel, height, width;).\n Let’s try out the ToTensor transform. Once instantiated, it can be called like a\nfunction with the PIL image as the argument, returning a tensor as output:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import transforms","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_tensor = transforms.ToTensor()\nimg_t = to_tensor(img)\nimg_t.shape","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"torch.Size([3, 32, 32])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The image has been turned into a 3 × 32 × 32 tensor and therefore a 3-channel (RGB)\n32 × 32 image. Note that nothing has happened to label; it is still an integer.\n As we anticipated, we can pass the transform directly as an argument to dataset\n.CIFAR10:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, accessing an element of the dataset will return a tensor, rather than a\nPIL image:"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_t,_ = tensor_cifar10[99]\n\ntype(img_t)","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"torch.Tensor"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"As expected, the shape has the channel as the first dimension, while the scalar type is\nfloat32:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_t.shape, img_t.dtype","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"(torch.Size([3, 32, 32]), torch.float32)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Whereas the values in the original PIL image ranged from 0 to 255 (8 bits per channel), the ToTensor transform turns the data into a 32-bit floating-point per channel,\nscaling the values down from 0.0 to 1.0. Let’s verify that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_t.min(), img_t.max()","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"(tensor(0.), tensor(1.))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"\nand lets verify if we have the same image out"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    plt.imshow(img_t)\nexcept Exception as e:\n    print(e)\nprint(img_t.shape, img_t.permute(1,2,0).shape)\nplt.imshow(img_t.permute(1,2,0))\nplt.show()","execution_count":39,"outputs":[{"output_type":"stream","text":"Invalid shape (3, 32, 32) for image data\ntorch.Size([3, 32, 32]) torch.Size([32, 32, 3])\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"It checks. Note how we have to use permute to change the order of the axes from\nC × H × W to H × W × C to match what Matplotlib expects. "},{"metadata":{},"cell_type":"markdown","source":"### Normalizing data"},{"metadata":{},"cell_type":"markdown","source":"Transforms are really handy because we can chain them using transforms.Compose,\nand they can handle normalization and data augmentation transparently, directly in\nthe data loader. For instance, it’s good practice to normalize the dataset so that each\nchannel has zero mean and unitary standard deviation."},{"metadata":{},"cell_type":"markdown","source":"we also have an intuition for why: by choosing\nactivation functions that are linear around 0 plus or minus 1 (or 2), keeping the data\nin the same range means it’s more likely that neurons have nonzero gradients and hence, will learn sooner. Also, normalizing each channel so that it has the same\ndistribution will ensure that channel information can be mixed and updated through\ngradient descent using the same learning rate."},{"metadata":{},"cell_type":"markdown","source":"In order to make it so that each channel has zero mean and unitary standard deviation, we can compute the mean value and the standard deviation of each channel\nacross the dataset and apply the following transform: v_n[c] = (v[c] - mean[c]) /\nstdev[c]. This is what transforms.Normalize does. The values of mean and stdev\nmust be computed offline (they are not computed by the transform). Let’s compute\nthem for the CIFAR-10 training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs = torch.stack([img_t for img_t ,_ in tensor_cifar10],dim=3)\nimgs.shape","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"torch.Size([3, 32, 32, 50000])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"lets compute mean per channel"},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs.view(3,-1).mean(dim=1)","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"tensor([0.4914, 0.4822, 0.4465])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Recall that view(3, -1) keeps the three channels and\nmerges all the remaining dimensions into one, figuring\nout the appropriate size. Here our 3 × 32 × 32 image is\ntransformed into a 3 × 1,024 vector, and then the mean\nis taken over the 1,024 elements of each channel."},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs.view(3, -1).std(dim=1)","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"tensor([0.2470, 0.2435, 0.2616])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"With these numbers in our hands, we can initialize the Normalize transform\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms.Normalize((0.4915, 0.4823, .4468), (0.2470, 0.2435, 0.2616))","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"Normalize(mean=(0.4915, 0.4823, 0.4468), std=(0.247, 0.2435, 0.2616))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"and concatenate it after the ToTensor transform:"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4915, 0.4823, 0.4468),\n                         (0.2470, 0.2435, 0.2616))\n]))","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_cifar10_val = datasets.CIFAR10(data_path, train=False, download=False, transform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4915, 0.4823, 0.4468),\n                         (0.2470, 0.2435, 0.2616))\n]))","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_t , _ = transformed_cifar10[99]\n\nplt.imshow(img_t.permute(1,2,0))\n\nplt.show()","execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"This is because normalization\nhas shifted the RGB levels outside the 0.0 to 1.0 range and changed the overall magnitudes of the channels. All of the data is still there; it’s just that Matplotlib renders it as\nblack. We’ll keep this in mind for the future."},{"metadata":{},"cell_type":"markdown","source":"### Distinguishing birds from aeroplanes"},{"metadata":{},"cell_type":"markdown","source":"Lets try making a very simple non superman detector."},{"metadata":{},"cell_type":"markdown","source":"### Building the dataset"},{"metadata":{},"cell_type":"markdown","source":"The first step is to get the data in the right shape. We could create a Dataset subclass\nthat only includes birds and airplanes. However, the dataset is small, and we only need\nindexing and len to work on our dataset. It doesn’t actually have to be a subclass of\ntorch.utils.data.dataset.Dataset! Well, why not take a shortcut and just filter the\ndata in cifar10 and remap the labels so they are contiguous? Here’s how:"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_map = {0:0, 2:1}\nclass_names = ['airplane', 'bird']","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cifar2 = [(img, label_map[label])\nfor img, label in transformed_cifar10\nif label in [0, 2]]","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cifar2_val = [(img, label_map[label])\nfor img, label in transformed_cifar10_val\nif label in [0, 2]]","execution_count":50,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cifar2 object satisfies the basic requirements for a Dataset—that is, __len__ and\n__getitem__ are defined—so we’re going to use that. We should be aware, however,\nthat this is a clever shortcut and we might wish to implement a proper Dataset if we\nhit limitations with it.4\n We have a dataset! Next, we need a model to feed our data to. "},{"metadata":{},"cell_type":"markdown","source":"### A fully connected model"},{"metadata":{},"cell_type":"markdown","source":"We learned how to build a neural network in chapter 5. We know that it’s a tensor of\nfeatures in, a tensor of features out. After all, an image is just a set of numbers laid out\nin a spatial configuration. OK, we don’t know how to handle the spatial configuration\npart just yet, but in theory if we just take the image pixels and straighten them into a\nlong 1D vector, we could consider those numbers as input features, right?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_out = 2\nmodel = nn.Sequential(\n    nn.Linear(\n        3072, 512\n    ),\n    nn.Tanh(),\n    nn.Linear(512, n_out)\n)","execution_count":52,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We somewhat arbitrarily pick 512 hidden features. A neural network needs at least\none hidden layer (of activations, so two modules) with a nonlinearity in between in\norder to be able to learn arbitrary functions in the way we discussed in section 6.3—\notherwise, it would just be a linear model. The hidden features represent (learned)\nrelations between the inputs encoded through the weight matrix. As such, the model\nmight learn to “compare” vector elements 176 and 208, but it does not a priori focus\non them because it is structurally unaware that these are, indeed (row 5, pixel 16) and\n(row 6, pixel 16), and thus adjacent."},{"metadata":{},"cell_type":"markdown","source":"### Output of a classifier"},{"metadata":{},"cell_type":"markdown","source":"Casting the problem in terms of probabilities imposes a few extra constraints on\nthe outputs of our network:\n\nEach element of the output must be in the [0.0, 1.0] range (a probability of\nan outcome cannot be less than 0 or greater than 1).\n\nThe elements of the output must add up to 1.0 (we’re certain that one of the\ntwo outcomes will occur).\n\nIt sounds like a tough constraint to enforce in a differentiable way on a vector of numbers. Yet there’s a very smart trick that does exactly that, and it’s differentiable: it’s\ncalled softmax. "},{"metadata":{},"cell_type":"markdown","source":"### Representing the output as probabilities"},{"metadata":{},"cell_type":"markdown","source":"Softmax is a function that takes a vector of values and produces another vector of the\nsame dimension, where the values satisfy the constraints we just listed to represent\nprobabilities.\n\nThat is, we take the elements of the vector, compute the elementwise exponential,\nand divide each element by the sum of exponentials. In code, it’s something like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def softmax(x):\n    return torch.exp(x) / torch.exp(x).sum()","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.tensor([1.0,2.0, 3.0])","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"softmax(x)","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"tensor([0.0900, 0.2447, 0.6652])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"softmax(x).sum()","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"tensor(1.)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Softmax is a monotone function, in that lower values in the input will correspond to\nlower values in the output. However, it’s not scale invariant, in that the ratio between\nvalues is not preserved. In fact, the ratio between the first and second elements of the\ninput is 0.5, while the ratio between the same elements in the output is 0.3678. This is\nnot a real issue, since the learning process will drive the parameters of the model in a\nway that values have appropriate ratios.\n The nn module makes softmax available as a module. Since, as usual, input tensors\nmay have an additional batch 0th dimension, or have dimensions along which they\nencode probabilities and others in which they don’t, nn.Softmax requires us to specify\nthe dimension along which the softmax function is applied:"},{"metadata":{"trusted":true},"cell_type":"code","source":"softmax = nn.Softmax(dim=1)\n\nx = torch.tensor([[1.0,2.0,3.0],\n                  [1.0,2.0,3.0]])","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"softmax(x)","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"tensor([[0.0900, 0.2447, 0.6652],\n        [0.0900, 0.2447, 0.6652]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"In this case, we have two input vectors in two rows (just like when we work with\nbatches), so we initialize nn.Softmax to operate along dimension 1.\n Excellent! We can now add a softmax at the end of our model, and our network\nwill be equipped to produce probabilities:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = nn.Sequential(\n        nn.Linear(3072, 512),\n        nn.Tanh(),\n        nn.Linear(512, 2),\n        nn.Softmax(dim=1)\n    )","execution_count":59,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can actually try running the model before even training it. Let’s do it, just to see\nwhat comes out. We first build a batch of one image, our bird"},{"metadata":{"trusted":true},"cell_type":"code","source":"img, _ = cifar2[99]\n\nplt.imshow(img.permute(1,2,0))","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f201a389850>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"Oh, hello there. In order to call the model, we need to make the input have the right\ndimensions. We recall that our model expects 3,072 features in the input, and that nn\nworks with data organized into batches along the zeroth dimension. So we need to\nturn our 3 × 32 × 32 image into a 1D tensor and then add an extra dimension in the\nzeroth position"},{"metadata":{"trusted":true},"cell_type":"code","source":"img.shape","execution_count":61,"outputs":[{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"torch.Size([3, 32, 32])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"img.view(-1).shape","execution_count":62,"outputs":[{"output_type":"execute_result","execution_count":62,"data":{"text/plain":"torch.Size([3072])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    img_batch = img.view(-1).unsqueeze(0)\nexcept Exception as e:\n    print(e)","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = model(img_batch)\nout","execution_count":64,"outputs":[{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"tensor([[0.6301, 0.3699]], grad_fn=<SoftmaxBackward>)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"So, we got probabilities! Well, we know we shouldn’t get too excited: the weights and\nbiases of our linear layers have not been trained at all. Their elements are initialized\nrandomly by PyTorch between –1.0 and 1.0. Interestingly, we also see grad_fn for the\noutput, which is the tip of the backward computation graph (it will be used as soon as\nwe need to backpropagate)."},{"metadata":{},"cell_type":"markdown","source":"The network can’t even tell\nthat at this point. It’s the loss function that associates a meaning with these two numbers, after backpropagation. If the labels are provided as index 0 for “airplane” and\nindex 1 for “bird,” then that’s the order the outputs will be induced to take. Thus,\nafter training, we will be able to get the label as an index by computing the argmax of\nthe output probabilities: that is, the index at which we get the maximum probability.\nConveniently, when supplied with a dimension, torch.max returns the maximum element along that dimension as well as the index at which that value occurs. In our case,\nwe need to take the max along the probability vector (not across batches), therefore,\ndimension 1:"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, index = torch.max(out, dim=1)\n\nindex","execution_count":65,"outputs":[{"output_type":"execute_result","execution_count":65,"data":{"text/plain":"tensor([0])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"It says the image is a bird. Pure luck. But we have adapted our model output to the\nclassification task at hand by getting it to output probabilities. We also have now run\nour model against an input image and verified that our plumbing works. Time to get\ntraining. As in the previous two chapters, we need a loss to minimize during training. "},{"metadata":{},"cell_type":"markdown","source":"### A loss for classfiying"},{"metadata":{},"cell_type":"markdown","source":"Looking back at the\nargmax operation we used to extract the index of the predicted class, what we’re really\ninterested in is that the first probability is higher than the second for airplanes and vice\nversa for birds. In other words, we want to penalize misclassifications rather than painstakingly penalize everything that doesn’t look exactly like a 0.0 or 1.0."},{"metadata":{},"cell_type":"markdown","source":"What we need to maximize in this case is the probability associated with the correct\nclass, out[class_index], where out is the output of softmax and class_index is a vector containing 0 for “airplane” and 1 for “bird” for each sample. This quantity—that\nis, the probability associated with the correct class—is referred to as the likelihood (of\nour model’s parameters, given the data).8\n In other words, we want a loss function that\nis very high when the likelihood is low: so low that the alternatives have a higher probability. "},{"metadata":{},"cell_type":"markdown","source":" There’s a loss function that behaves that way, and it’s called negative log likelihood\n(NLL). It has the expression NLL = - sum(log(out_i[c_i])), where the sum is taken\nover N samples and c_i is the correct class for sample i. Let’s take a look at figure 7.10,\nwhich shows the NLL as a function of predicted probability"},{"metadata":{},"cell_type":"markdown","source":"Summing up, our loss for classification can be computed as follows. For each sample in the batch:\n\n1 Run the forward pass, and obtain the output values from the last (linear) layer.\n\n2 Compute their softmax, and obtain probabilities.\n\n3 Take the predicted probability corresponding to the correct class (the likelihood of the parameters). Note that we know what the correct class is because\nit’s a supervised problem—it’s our ground truth.\n\n4 Compute its logarithm, slap a minus sign in front of it, and add it to the loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"out = torch.tensor([\n    [0.6, 0.4],\n    [0.9,0.1],\n    [0.3, 0.7],\n    [0.2, 0.8]\n])\n\nclass_index = torch.tensor([0,0,1,1]).unsqueeze(1)\n\ntruth = torch.zeros((4,2))\ntruth.scatter_(dim=1, index=class_index, value=1.0)\ntruth","execution_count":66,"outputs":[{"output_type":"execute_result","execution_count":66,"data":{"text/plain":"tensor([[1., 0.],\n        [1., 0.],\n        [0., 1.],\n        [0., 1.]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Training the classfier"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = nn.Sequential(\n            nn.Linear(3072, 512),\n            nn.Tanh(),\n            nn.Linear(512, 2),\n            nn.LogSoftmax(dim=1)\n)\n\nlearning_rate = 1e-2\n\noptimizer = optim.SGD(model.parameters(),lr=learning_rate)","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_fn = nn.NLLLoss()\nn_epochs = 100\n\nfor epoch in range(n_epochs):\n    for img, label in cifar2:\n        out = model(img.view(-1).unsqueeze(0))\n        loss = loss_fn(out, torch.tensor([label]))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(\"Epoch: %d, Loss %f\" % (epoch, float(loss)))","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch: 0, Loss 7.222799\nEpoch: 1, Loss 1.883938\nEpoch: 2, Loss 10.100144\nEpoch: 3, Loss 4.955270\nEpoch: 4, Loss 3.700928\nEpoch: 5, Loss 7.842928\nEpoch: 6, Loss 10.949423\nEpoch: 7, Loss 2.339513\nEpoch: 8, Loss 6.712431\nEpoch: 9, Loss 13.361099\nEpoch: 10, Loss 4.209538\nEpoch: 11, Loss 4.661271\nEpoch: 12, Loss 9.625241\nEpoch: 13, Loss 6.211294\nEpoch: 14, Loss 8.326766\nEpoch: 15, Loss 5.311778\nEpoch: 16, Loss 0.598872\nEpoch: 17, Loss 0.007767\nEpoch: 18, Loss 1.933518\nEpoch: 19, Loss 2.297608\nEpoch: 20, Loss 3.862087\nEpoch: 21, Loss 3.553736\nEpoch: 22, Loss 0.240335\nEpoch: 23, Loss 5.221899\nEpoch: 24, Loss 6.085196\nEpoch: 25, Loss 14.825875\nEpoch: 26, Loss 6.812384\nEpoch: 27, Loss 10.024420\nEpoch: 28, Loss 11.113656\nEpoch: 29, Loss 0.188393\nEpoch: 30, Loss 7.740237\nEpoch: 31, Loss 7.331666\nEpoch: 32, Loss 12.096233\nEpoch: 33, Loss 1.837277\nEpoch: 34, Loss 2.919860\nEpoch: 35, Loss 5.839210\nEpoch: 36, Loss 1.502855\nEpoch: 37, Loss 7.376925\nEpoch: 38, Loss 8.335595\nEpoch: 39, Loss 6.007591\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"This time we have incorporated thta in training loop each image may be independently catered to. By shuffling samples at each eppoch we are essentially introducing randomness in our gradient"},{"metadata":{},"cell_type":"markdown","source":"Remember SGD? It stands for stochastic gradient descent, and this is what the S is about:\nworking on small batches (aka minibatches) of shuffled data. It turns out that following\ngradients estimated over minibatches, which are poorer approximations of gradients\nestimated across the whole dataset, helps convergence and prevents the optimization\nprocess from getting stuck in local minima it encounters along the way"},{"metadata":{},"cell_type":"markdown","source":"In our training code, we chose minibatches of size 1 by picking one item at a time from\nthe dataset. The torch.utils.data module has a class that helps with shuffling and\norganizing the data in minibatches: DataLoader. The job of a data loader is to sample\nminibatches from a dataset, giving us the flexibility to choose from different sampling\nstrategies. A very common strategy is uniform sampling after shuffling the data at each\nepoch. Figure 7.14 shows the data loader shuffling the indices it gets from the Dataset."},{"metadata":{},"cell_type":"markdown","source":"Let’s see how this is done. At a minimum, the DataLoader constructor takes a Dataset\nobject as input, along with batch_size and a shuffle Boolean that indicates whether\nthe data needs to be shuffled at the beginning of each epoch:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A DataLoader can be iterated over, so we can use it directly in the inner loop of our\nnew training code:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ntrain_loader = torch.utils.data.Dataloader(cifar2, batch_size=64, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = nn.sequential(\n    nn.Linear(3072, 512),\n    nn.Tanh(),\n    nn.Linear(512, 2),\n    nn.LogSoftmax(dim=1)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 1e-2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.SGD(model.parameters(), lr=learning_rate)\nloss_fn = nn.NLLLoss()\nn_epochs = 100\n\nfor epoch in range(n_epochs):\n    for imgs, labels in train_loader:\n        batch_Size = imgs.shape[0]\n        outputs = model(imgs.view(batch_size,-1))\n        loss = loss_fn(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    print(\"Epoch: \", epoch, \" loss: \", float(loss))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the loss decreases somehow, but we have no idea whether it’s low enough.\nSince our goal here is to correctly assign classes to images, and preferably do that on\nan independent dataset, we can compute the accuracy of our model on the validation\nset in terms of the number of correct classifications over the total"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        batch_size = imgs.shape[0]\n        outputs = model(imgs.view(batch_size, -1))\n        _, predicted = torch.max(outputs, dim=1)\n        total += labels.shape[0]\n        correct += int((predicted == labels).sum())\n\nprint(\"Accuracy: \" correct/total)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not a great performance, but quite a lot better than random. In our defense, our\nmodel was quite a shallow classifier; it’s a miracle that it worked at all. It did because\nour dataset is really simple—a lot of the samples in the two classes likely have systematic differences (such as the color of the background) that help the model tell birds\nfrom airplanes, based on a few pixels.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = nn.Sequential(\n    nn.Linear(3072, 1024),\n    nn.Tanh(),\n    nn.Linear(1024, 512),\n    nn.Tanh(),\n    nn.Linear(512, 128),\n    nn.Tanh(),\n    nn.Linear(128, 2),\n    nn.LogSoftmax(dim=1)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are trying to taper the number of features more gently toward the output, in\nthe hope that intermediate layers will do a better job of squeezing information in\nincreasingly shorter intermediate outputs.\n "},{"metadata":{},"cell_type":"markdown","source":"The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using\nnn.CrossEntropyLoss. This terminology is a particularity of PyTorch, as the\nnn.NLLoss computes, in fact, the cross entropy but with log probability predictions as\ninputs where nn.CrossEntropyLoss takes scores (sometimes called logits). Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass\non the target, and the predicted distribution given by the log probability inputs."},{"metadata":{},"cell_type":"markdown","source":" To add to the confusion, in information theory, up to normalization by sample size,\nthis cross entropy can be interpreted as a negative log likelihood of the predicted distribution under the target distribution as an outcome. So both losses are the negative\nlog likelihood of the model parameters given the data when our model predicts the\n(softmax-applied) probabilities. In this book, we won’t rely on these details, but don’t\nlet the PyTorch naming confuse you when you see the terms used in the literature."},{"metadata":{},"cell_type":"markdown","source":"lets drop the softmax"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = nn.Sequential(\n    nn.Linear(3072, 1024),\n    nn.Tanh(),\n    nn.Linear(1024, 512),\n    nn.Tanh(),\n    nn.Linear(512, 128),\n    nn.tanh(),\n    nn.linear(128, 2)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the numbers will be exactly the same as with nn.LogSoftmax and nn.NLLLoss.\nIt’s just more convenient to do it all in one pass, with the only gotcha being that the output of our model will not be interpretable as probabilities (or log probabilities). We’ll\nneed to explicitly pass the output through a softmax to obtain those"},{"metadata":{},"cell_type":"markdown","source":"Lets look at the parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"numel_list = [p.numel() for p in model.parameters()]\nsum(numel_list), numel_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":". In our full network,\nwe had 1,024 output features, which led the first linear module to have 3 million\nparameters. This shouldn’t be unexpected: we know that a linear layer computes y =\nweight * x + bias, and if x has length 3,072 (disregarding the batch dimension for\nsimplicity) and y must have length 1,024, then the weight tensor needs to be of size\n1,024 × 3,072 and the bias size must be 1,024. And 1,024 * 3,072 + 1,024 = 3,146,752,\nas we found earlier. We can verify these quantities directly:"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear = nn.Linear(3072, 1024)\n\nlinear.weight.shape, linear.bias.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is this telling us? That our neural network won’t scale very well with the number\nof pixels. What if we had a 1,024 × 1,024 RGB image? That’s 3.1 million input values.\nEven abruptly going to 1,024 hidden features (which is not going to work for our classifier), we would have over 3 billion parameters. Using 32-bit floats, we’re already at 12\nGB of RAM, and we haven’t even hit the second layer, much less computed"},{"metadata":{},"cell_type":"markdown","source":"### limits of going fully connected"},{"metadata":{},"cell_type":"markdown","source":"1. a fully connected network is not translation invariant\n2. we need to use data augmentation techniques"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}