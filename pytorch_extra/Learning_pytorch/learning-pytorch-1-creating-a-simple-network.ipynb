{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Learning Pytorch\n\nThis series would be taken from various tutorial available in youtube.\n\n## 1 : Creating a simple network\n\nThis one is taken from excellent channel Alladin Perrson : https://www.youtube.com/watch?v=Jy4wM2X21u0&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=3\n\n![](https://i.morioh.com/200620/5b0ea047.jpg)","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import torch.nn as  nn\nimport torch.nn.functional as F","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torchvision.datasets as datasets\nimport torchvision.transforms as transforms","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Creating a fully connected network","metadata":{}},{"cell_type":"code","source":"class SimpleNN(nn.Module):\n    def __init__(self, input_size, classes):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 50)\n        self.fc2 = nn.Linear(50, classes)\n    \n    def forward(self,x):\n        out = F.relu(self.fc1(x))\n        out = self.fc2(out)\n        return out\n        ","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# testing\nmodel = SimpleNN(784, 10)\ninput_tensor = torch.randn(64, 784)\nout_tensor = model(input_tensor)\nout_tensor.shape","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([64, 10])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Setting the device","metadata":{}},{"cell_type":"code","source":"device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Setting up hyperparameters","metadata":{}},{"cell_type":"code","source":"input_size = 784\nclasses = 10\nlearning_rate = 0.001\nbatch_size=64\nnum_epochs=100","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset = datasets.MNIST(root='dataset/', download=True, train=True, transform=transforms.ToTensor())","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e278fc649c74499f9bb3d38ebec4ee92"}},"metadata":{}},{"name":"stdout","text":"Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eb6ffb8c4de4ed9880a2d3271eb38aa"}},"metadata":{}},{"name":"stdout","text":"Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76b0001bb589494393e4000b96488d9b"}},"metadata":{}},{"name":"stdout","text":"Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee6941678b514b4aab7ce422281b66b2"}},"metadata":{}},{"name":"stdout","text":"Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\nProcessing...\nDone!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"test_dataset = datasets.MNIST(root='dataset/', download=True, train=False, transform=transforms.ToTensor())\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Initialising the model","metadata":{}},{"cell_type":"code","source":"simple_model = SimpleNN(input_size,classes)","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Loss and optimiser","metadata":{}},{"cell_type":"code","source":"loss_criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Training the model","metadata":{}},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    current_loss  = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # reshape the data\n        data = data.reshape(data.shape[0], -1)\n        \n        # calculate score and loss forward pass\n        scores = model(data)\n        loss = loss_criterion(scores, target)\n        current_loss = loss\n        \n        # update weights backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # optimzer step\n        optimizer.step()\n    print(f\"current epoch : {epoch} loss: {current_loss}\")","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"current epoch : 0 loss: 0.0020287882070988417\ncurrent epoch : 1 loss: 0.002754819579422474\ncurrent epoch : 2 loss: 1.1342566722305492e-05\ncurrent epoch : 3 loss: 1.4342148233481566e-06\ncurrent epoch : 4 loss: 8.194585461751558e-06\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-4cc35fc99353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# optimzer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"current epoch : {epoch} loss: {current_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Calculating the accuracy","metadata":{}},{"cell_type":"code","source":"def check_accuracy(model, loader):\n    model.eval()\n    num_samples = 0\n    num_correct = 0\n    \n    with torch.no_grad():\n        for x,y in loader:\n            x = x.reshape(x.shape[0],-1)\n            score = model(x)\n            print(score)\n            _,predictions = score.max(1)\n            print(predictions, y)\n            \n            num_samples += predictions.size(0)\n            num_correct += (y==predictions).sum()\n            break\n            \n    print(f\" total samples = {num_samples} , total correct = {num_correct}, percentage = {float(num_correct)/float(num_samples)*100:.2f}%\")","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"check_accuracy(simple_model, train_loader)\ncheck_accuracy(simple_model, test_loader)","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tensor([[-0.0107,  0.0820,  0.1110, -0.0093,  0.0492, -0.0671,  0.0697,  0.1223,\n          0.0715, -0.0019],\n        [-0.2280, -0.0329,  0.0641, -0.0099,  0.0671, -0.0323,  0.1028,  0.2071,\n          0.0762,  0.0555],\n        [-0.0991,  0.0365,  0.1163,  0.1054,  0.0823, -0.0130,  0.0313,  0.1395,\n         -0.0484,  0.1080],\n        [ 0.0010, -0.0185,  0.0790,  0.1465,  0.1055,  0.0745,  0.0632,  0.1552,\n          0.0132, -0.0210],\n        [-0.0679, -0.0260,  0.1532,  0.0871,  0.0959,  0.0871, -0.0788,  0.1562,\n          0.0789,  0.0834],\n        [-0.0973, -0.0692,  0.1219, -0.0290,  0.1045,  0.0323,  0.1025,  0.2408,\n         -0.0555, -0.0102],\n        [ 0.0043, -0.0205,  0.1628,  0.0955,  0.0726,  0.0090, -0.0392,  0.0952,\n          0.0428,  0.0372],\n        [-0.2190, -0.0568,  0.0704,  0.1306,  0.1154, -0.0205,  0.0646,  0.3300,\n         -0.0077, -0.0470],\n        [-0.1645, -0.0127,  0.1136,  0.0706,  0.0310,  0.0371,  0.0530,  0.2666,\n          0.0753, -0.0369],\n        [-0.0473,  0.0603,  0.2245,  0.0593,  0.1984,  0.0619, -0.0228,  0.1166,\n         -0.0091, -0.0280],\n        [-0.1233,  0.0281,  0.1306, -0.0318,  0.0946, -0.1064,  0.0403,  0.2602,\n          0.1340,  0.0330],\n        [ 0.0198, -0.0117,  0.0200,  0.0829,  0.0598,  0.1300,  0.0307,  0.1671,\n         -0.0560,  0.0521],\n        [-0.0880,  0.0525,  0.0873,  0.0401,  0.0576, -0.0116,  0.0986,  0.2025,\n         -0.0376, -0.0016],\n        [ 0.0069, -0.0973,  0.1097,  0.0897,  0.1089,  0.0735,  0.0328,  0.1876,\n          0.0132, -0.0379],\n        [-0.0671,  0.0460,  0.0643, -0.0774,  0.0180,  0.0465,  0.1129,  0.2444,\n         -0.0770,  0.0805],\n        [-0.0225,  0.0893,  0.1223, -0.0330,  0.0361, -0.0296,  0.0325,  0.0603,\n          0.0503, -0.0111],\n        [-0.2268,  0.0426,  0.1273, -0.0798,  0.0935, -0.0447,  0.0973,  0.2381,\n          0.1298, -0.0683],\n        [-0.0600,  0.0603,  0.0968,  0.0978,  0.0658,  0.0399,  0.0817,  0.1070,\n         -0.0649, -0.0518],\n        [-0.1642,  0.0344,  0.1422,  0.1396,  0.0748, -0.0748,  0.0987,  0.1616,\n          0.1048,  0.0624],\n        [-0.1001,  0.0535,  0.1349,  0.0275,  0.1235,  0.0638,  0.0294,  0.1956,\n         -0.0171,  0.0135],\n        [-0.1882, -0.0465,  0.1387,  0.0131,  0.0547,  0.0159, -0.0680,  0.2089,\n          0.1870,  0.0543],\n        [ 0.0321,  0.0038,  0.1618,  0.0588,  0.1147,  0.0481, -0.0241,  0.1201,\n         -0.0162, -0.1391],\n        [-0.0905,  0.0148,  0.1089,  0.0184,  0.1019,  0.0324,  0.0352,  0.2330,\n          0.1721,  0.0107],\n        [-0.0161,  0.1042,  0.1871, -0.0530,  0.0995, -0.0935,  0.0045,  0.0909,\n          0.0538,  0.0304],\n        [-0.1467, -0.0086,  0.1241,  0.0345,  0.0818,  0.0772,  0.0626,  0.1989,\n          0.0874,  0.0067],\n        [-0.0248,  0.0229,  0.0493,  0.0687,  0.0031,  0.0691,  0.0777,  0.1435,\n          0.0768,  0.0361],\n        [-0.1605, -0.1032,  0.0883,  0.0431,  0.0512,  0.1117,  0.0549,  0.2895,\n          0.1874, -0.0488],\n        [-0.0329,  0.0104,  0.0858,  0.0343,  0.0782,  0.0220,  0.0761,  0.1852,\n          0.1161, -0.0115],\n        [-0.1104, -0.0124,  0.0547,  0.0577,  0.0586,  0.0412,  0.0571,  0.1510,\n          0.1167,  0.0343],\n        [-0.0407,  0.0072,  0.0282,  0.0200, -0.0235,  0.0129,  0.1077,  0.0944,\n          0.0175,  0.0158],\n        [-0.1143, -0.0306,  0.0438,  0.0310,  0.0373,  0.1443,  0.1087,  0.2785,\n          0.0359,  0.0521],\n        [-0.1073, -0.0026,  0.0315, -0.0409,  0.1220,  0.0054,  0.0500,  0.2641,\n          0.1259, -0.0383],\n        [-0.1014,  0.0839,  0.1249, -0.1219,  0.1367,  0.0941,  0.0077,  0.2628,\n         -0.1022, -0.0743],\n        [-0.0648,  0.0213,  0.1470, -0.0633,  0.0359,  0.0201,  0.0868,  0.0417,\n          0.0760,  0.0460],\n        [ 0.0359, -0.0304,  0.0598, -0.0186,  0.0807,  0.0898,  0.0244,  0.1541,\n         -0.0034, -0.0348],\n        [-0.0447,  0.0381,  0.1153,  0.0435,  0.0908, -0.0974,  0.0858,  0.1252,\n          0.0285,  0.0547],\n        [ 0.0223,  0.0910,  0.1282, -0.0253,  0.0415, -0.0342,  0.0538,  0.0785,\n          0.0924,  0.0083],\n        [ 0.0250, -0.0586,  0.1035,  0.1640,  0.1087,  0.0514,  0.0309,  0.2034,\n          0.1345,  0.0837],\n        [-0.0810, -0.0277,  0.1110,  0.0604,  0.0677, -0.0030,  0.0309,  0.2645,\n          0.1034,  0.0015],\n        [-0.0538, -0.0377,  0.1370,  0.0299,  0.1441,  0.0557, -0.0537,  0.2281,\n          0.0390,  0.0120],\n        [-0.0397,  0.0366,  0.1038,  0.1096,  0.0937,  0.0613, -0.0215,  0.1976,\n         -0.0024,  0.0444],\n        [ 0.0290, -0.0110,  0.0395,  0.1692,  0.0461,  0.1206,  0.0428,  0.2216,\n          0.0268,  0.0216],\n        [-0.0803,  0.0196,  0.2562, -0.0240,  0.1661, -0.0834,  0.0123,  0.1352,\n          0.0741,  0.0813],\n        [-0.1476,  0.0441,  0.0951,  0.1224,  0.0226,  0.0545,  0.0343,  0.1549,\n         -0.0544,  0.0963],\n        [-0.1405,  0.0602,  0.1142,  0.0371,  0.0945, -0.0968,  0.0786,  0.0844,\n         -0.1640,  0.0250],\n        [ 0.0157,  0.0462,  0.0737,  0.0941,  0.0495, -0.0510,  0.0782,  0.1185,\n          0.0224, -0.0741],\n        [ 0.0028, -0.0021,  0.0789,  0.0013,  0.0708,  0.0061,  0.0841,  0.1682,\n          0.0385, -0.0578],\n        [-0.0826,  0.0817,  0.0955,  0.0913,  0.1024, -0.0450,  0.0909,  0.1705,\n          0.0808,  0.0231],\n        [-0.0916, -0.0062,  0.0498,  0.1327, -0.0327,  0.0015,  0.1133,  0.1613,\n          0.1372, -0.0053],\n        [-0.0761,  0.0271,  0.1020, -0.0022,  0.0318, -0.0123, -0.0324,  0.1640,\n          0.2012, -0.0085],\n        [ 0.0108, -0.0167,  0.0670,  0.1693,  0.0345,  0.0609,  0.0092,  0.1686,\n          0.1168,  0.0588],\n        [-0.1598, -0.0296,  0.0817,  0.1264,  0.0946, -0.0116,  0.0983,  0.1613,\n         -0.0464,  0.0733],\n        [-0.1506,  0.0373,  0.0293, -0.0990,  0.0933, -0.0137,  0.1009,  0.2403,\n          0.0311,  0.0203],\n        [-0.0960,  0.0310,  0.0928,  0.0519,  0.0275,  0.0475,  0.0479,  0.0719,\n          0.0903, -0.0529],\n        [-0.0764,  0.0900,  0.1055,  0.0461,  0.0373, -0.0034,  0.0849,  0.0803,\n          0.0457,  0.0521],\n        [-0.0747,  0.0069,  0.1432,  0.0127,  0.0302,  0.0851,  0.0044,  0.0841,\n          0.0243, -0.0063],\n        [-0.0356, -0.0203,  0.1094,  0.0081,  0.1399,  0.0520, -0.0083,  0.1976,\n         -0.0311,  0.0189],\n        [-0.2010,  0.0401,  0.0203, -0.0114,  0.0393, -0.0448,  0.1464,  0.2122,\n          0.0260, -0.0594],\n        [-0.1010, -0.0239,  0.1210,  0.0249,  0.0583, -0.0242,  0.1096,  0.1215,\n          0.0628, -0.0573],\n        [-0.1270, -0.1183,  0.0423,  0.0674,  0.1295,  0.0509, -0.0169,  0.2853,\n          0.1429,  0.1195],\n        [-0.1607,  0.0457,  0.0301,  0.0582,  0.0351, -0.0898,  0.1561,  0.1149,\n          0.0188,  0.0405],\n        [ 0.0044,  0.0864,  0.0834,  0.0657,  0.0445, -0.0461,  0.0594,  0.1198,\n          0.0207, -0.0903],\n        [-0.0047, -0.0115,  0.0706,  0.1713,  0.0168,  0.0667,  0.0318,  0.1220,\n          0.0445,  0.0883],\n        [-0.1402, -0.0708,  0.1362,  0.0761,  0.1402,  0.0372, -0.1203,  0.2456,\n          0.1068,  0.1084]])\ntensor([7, 7, 7, 7, 7, 7, 2, 7, 7, 2, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 2, 7, 2,\n        7, 7, 7, 7, 7, 6, 7, 7, 7, 2, 7, 7, 2, 7, 7, 7, 7, 7, 2, 7, 2, 7, 7, 7,\n        7, 8, 3, 7, 7, 2, 2, 2, 7, 7, 7, 7, 6, 7, 3, 7]) tensor([1, 8, 7, 5, 6, 8, 7, 6, 8, 8, 8, 7, 8, 5, 0, 1, 0, 4, 2, 9, 6, 5, 0, 1,\n        3, 2, 3, 7, 8, 1, 9, 0, 8, 1, 3, 8, 1, 7, 7, 9, 9, 9, 1, 4, 5, 1, 1, 7,\n        4, 6, 5, 4, 0, 6, 7, 2, 8, 0, 5, 5, 0, 1, 7, 6])\n total samples = 64 , total correct = 7, percentage = 10.94%\ntensor([[-8.5872e-02, -4.1266e-03,  1.7070e-01,  1.0122e-01,  2.0336e-02,\n          6.5693e-02,  6.6635e-02,  2.0380e-01,  9.3262e-02,  1.1309e-01],\n        [-2.7044e-02,  6.5488e-02,  4.5013e-02,  1.3260e-01,  5.6485e-02,\n          7.6372e-02,  1.1513e-01,  1.3657e-01, -8.2232e-03,  7.8530e-03],\n        [-6.5442e-02, -1.1649e-01,  1.1588e-01,  9.0251e-03,  1.3268e-01,\n          5.2128e-02,  8.8719e-02,  2.3815e-01,  7.1637e-02, -6.3466e-02],\n        [-1.2362e-01, -1.8659e-02,  6.4155e-02,  7.9303e-02,  5.2844e-02,\n         -1.8566e-04,  8.8227e-02,  1.3292e-01,  5.7027e-02,  4.3099e-03],\n        [-1.3199e-02,  6.0418e-02,  7.0142e-02,  3.2210e-02,  3.2130e-02,\n          1.3835e-02,  7.9550e-02,  1.1702e-01,  3.0366e-02, -2.7486e-02],\n        [-2.0977e-01, -7.9109e-03,  9.5352e-02,  3.5500e-02,  8.4759e-02,\n          6.4552e-02,  1.3500e-01,  2.6866e-01, -6.5461e-02, -1.7209e-02],\n        [-3.9742e-02,  1.1320e-02,  1.2293e-01,  3.8701e-02,  9.6846e-02,\n         -4.8411e-02,  1.7792e-02,  1.7152e-01,  7.0022e-02,  5.4761e-02],\n        [-5.2507e-02,  8.3628e-02,  1.7403e-01, -1.1350e-01,  1.0536e-01,\n          1.0565e-03,  3.0240e-02,  1.9206e-01,  7.3080e-02, -7.7081e-03],\n        [-2.0570e-01,  6.8874e-03,  1.8483e-01,  1.0396e-01,  1.8585e-02,\n          3.4339e-02,  4.4118e-02,  5.7551e-02, -6.3644e-02,  8.7900e-03],\n        [-4.9148e-02,  7.5029e-02,  8.9817e-02,  1.4815e-02,  2.9993e-02,\n         -2.9533e-02,  4.1344e-02,  5.6883e-02,  8.6641e-02, -2.1582e-02],\n        [-2.2136e-01, -4.5877e-02,  8.5754e-02,  4.0168e-02,  6.5909e-02,\n         -1.8839e-02,  8.2021e-02,  2.8883e-01,  1.2246e-01, -3.1803e-02],\n        [ 2.4139e-02, -4.4992e-02,  4.4897e-02,  1.2455e-01,  1.2991e-01,\n          1.0052e-01, -7.5284e-02,  1.8856e-01, -5.2257e-02, -2.9584e-02],\n        [-1.0149e-01,  8.4005e-02,  2.8322e-02,  1.1133e-01,  1.0731e-02,\n          3.8364e-02,  1.1298e-01,  1.5661e-01, -3.4861e-02,  1.5849e-02],\n        [-6.0348e-02,  7.9265e-02,  2.1566e-01,  1.4791e-02,  1.3930e-01,\n         -5.6175e-02,  1.3154e-01,  1.2340e-01,  8.4761e-02, -3.5978e-02],\n        [-2.4695e-02, -8.7223e-02,  5.5890e-02,  1.6705e-01,  1.2067e-01,\n          1.1351e-01, -5.0642e-03,  2.3708e-01, -2.3673e-02, -5.2356e-02],\n        [-8.6836e-02,  1.4852e-02,  7.7224e-02,  4.0167e-02,  8.5368e-02,\n          2.4231e-02,  7.9024e-02,  1.7525e-01,  9.5211e-02,  2.0788e-02],\n        [ 1.7373e-02, -1.7289e-02,  7.3931e-02,  1.2610e-01,  5.5585e-02,\n          1.1231e-01,  2.1082e-02,  1.7727e-01, -2.7322e-02,  1.1101e-01],\n        [-9.7422e-02,  6.8187e-02,  8.9573e-02,  1.6698e-02,  7.4331e-02,\n         -5.3365e-02,  8.2169e-02,  1.5739e-01,  3.4662e-02,  4.1006e-02],\n        [-2.8731e-02, -1.2507e-02,  6.0834e-02,  7.2312e-02,  4.7711e-02,\n          4.7140e-02,  8.2645e-02,  1.0996e-01,  5.0803e-02, -6.5203e-02],\n        [-3.2636e-02, -4.3296e-02,  1.9323e-01,  9.2784e-02,  1.2781e-01,\n          2.3043e-02,  2.5030e-02,  1.8365e-01,  2.4827e-02,  7.2477e-02],\n        [-6.9069e-02,  2.1439e-02,  7.1677e-02,  6.3113e-02,  5.2635e-02,\n         -6.6226e-02,  1.4496e-01,  1.7153e-01,  8.0403e-02,  5.2019e-02],\n        [-6.4414e-02, -1.4010e-02,  1.1568e-01,  9.8052e-02,  2.2465e-02,\n          1.0363e-01, -1.6252e-02,  1.6138e-01, -3.4392e-02,  3.6633e-02],\n        [-1.3841e-01,  2.4214e-02,  5.7254e-02,  9.8034e-02,  9.2539e-02,\n         -7.0534e-02,  1.0171e-01,  1.5704e-01,  3.0562e-02, -6.6333e-02],\n        [-2.3679e-02,  9.7477e-02,  1.5075e-01, -9.3350e-02,  8.3513e-02,\n         -7.0652e-02,  5.7037e-03,  1.1508e-01,  4.9306e-02,  3.8632e-02],\n        [-3.8860e-02, -1.1632e-02,  7.2377e-02,  3.4128e-02,  1.0001e-01,\n         -8.0835e-03,  4.1097e-02,  1.8901e-01,  5.0360e-02,  2.5684e-02],\n        [-1.2185e-01, -4.0582e-02,  2.8340e-02,  1.5764e-01,  1.5877e-02,\n          6.7491e-02,  1.0450e-01,  2.1163e-01,  1.5152e-01,  8.5562e-02],\n        [-8.8179e-02, -2.3588e-02,  7.7834e-02,  9.7714e-02,  9.6704e-02,\n          7.1200e-02,  5.3843e-02,  2.0027e-01, -4.1303e-02,  5.4488e-02],\n        [-2.5505e-02,  8.0771e-02,  1.3197e-01, -7.3352e-02,  7.9739e-02,\n         -6.7039e-02,  3.8519e-02,  1.1562e-01,  3.2948e-02,  1.9484e-02],\n        [-1.4723e-01,  1.0749e-01,  3.4200e-02,  1.4314e-01,  3.4219e-02,\n          7.6615e-02,  7.5276e-02,  1.8123e-01, -6.2429e-03, -6.2672e-03],\n        [-8.9933e-02, -8.6419e-03,  1.5171e-01, -1.7858e-03,  1.2920e-01,\n         -2.9257e-03,  4.7045e-02,  1.6091e-01,  2.4782e-01, -4.9632e-02],\n        [-1.2692e-01, -1.1936e-01,  3.4483e-02,  1.0072e-01,  2.7767e-02,\n          3.2540e-02,  7.8872e-02,  2.8647e-01,  1.7271e-01,  4.2086e-02],\n        [-1.9914e-02, -5.9579e-02,  5.8958e-02,  1.1521e-02,  1.0344e-01,\n         -3.0715e-02,  1.5630e-01,  2.4190e-01,  2.8865e-02, -3.5043e-02],\n        [-7.5698e-02,  3.0291e-02,  1.0170e-01,  1.4493e-01, -1.6345e-02,\n          1.9904e-02,  9.8461e-02,  1.2905e-01, -2.8484e-02, -1.7644e-02],\n        [-2.0105e-01, -2.3307e-02,  1.1647e-01,  8.6065e-02, -3.9697e-02,\n          2.3052e-02,  1.3745e-01,  2.2303e-01,  6.2590e-02,  9.0442e-02],\n        [-9.1905e-02,  6.2449e-02,  1.5907e-01,  4.5371e-02,  1.0166e-01,\n         -3.0675e-02,  3.6007e-02,  1.2347e-01,  1.2652e-01, -7.2048e-02],\n        [-7.6814e-02, -5.2034e-02,  1.4774e-01,  1.5930e-02,  1.5129e-01,\n          8.7847e-02, -2.5948e-02,  2.7020e-01,  2.8331e-02,  1.6494e-02],\n        [-7.1892e-02, -1.3946e-02,  5.3054e-02,  5.4298e-02,  7.4342e-02,\n          2.5420e-02,  6.8044e-02,  1.9125e-01,  1.1435e-02,  2.4350e-02],\n        [-5.3042e-02,  8.9991e-02,  1.5219e-01, -3.6116e-03,  3.1903e-02,\n         -6.6816e-02,  4.0748e-02,  9.3408e-02,  5.6781e-02,  3.1247e-02],\n        [-8.9833e-02,  2.6293e-02,  1.5794e-01,  4.6577e-02,  5.5460e-02,\n          3.8987e-02,  1.7403e-02,  1.6614e-01,  9.8933e-02,  1.2369e-02],\n        [-1.9105e-01,  1.0611e-01,  2.9198e-02, -8.3910e-02,  1.0025e-01,\n         -1.9081e-01,  6.0553e-02,  2.0182e-01,  2.0759e-01,  5.1961e-03],\n        [-4.8054e-02,  1.3543e-02,  1.1532e-01,  1.6578e-01,  5.4305e-02,\n          8.8404e-03,  2.1158e-02,  2.1972e-01,  1.2337e-02, -6.2046e-02],\n        [-1.0317e-02,  4.6816e-02,  1.0364e-01,  3.0918e-02,  5.9534e-02,\n          2.3216e-02,  7.4393e-02,  1.1648e-01,  4.3946e-02, -4.3772e-02],\n        [-8.4180e-02, -9.2931e-02,  1.0909e-01,  6.1320e-02,  1.1851e-01,\n          5.4243e-02,  5.0053e-02,  2.1280e-01,  7.2248e-02,  1.0872e-01],\n        [ 1.6324e-02,  7.4139e-02,  9.7073e-02,  4.3080e-02,  2.0359e-02,\n         -5.5474e-02,  3.7358e-02,  1.0675e-01,  4.7020e-02, -7.7754e-02],\n        [-5.1406e-03,  4.4629e-02,  1.6659e-01, -2.5870e-02,  1.2862e-01,\n          6.8679e-02, -2.4092e-02,  2.1402e-01,  9.9384e-02, -3.8606e-02],\n        [-1.2116e-01, -6.4923e-02,  8.1466e-02,  6.1651e-02,  1.0408e-01,\n          4.3050e-02,  6.9563e-02,  2.7584e-01,  7.6222e-02, -4.0992e-02],\n        [-1.3470e-01, -3.0131e-02,  1.1463e-01,  4.0402e-02,  1.3106e-01,\n          2.8388e-03,  1.7307e-02,  1.6289e-01, -3.3372e-02,  8.5960e-02],\n        [-1.8926e-01,  5.3168e-02,  1.2233e-01,  3.3815e-02,  1.2935e-01,\n         -1.2065e-01,  5.9185e-02,  2.3484e-01,  1.0567e-01, -5.5388e-02],\n        [-4.9236e-02,  1.5901e-02,  1.2483e-01,  7.6741e-02,  7.4836e-02,\n          1.8175e-02,  1.7967e-02,  1.2827e-01,  1.8285e-02,  8.1351e-02],\n        [-1.7536e-03,  5.9218e-02,  9.5061e-02,  3.9622e-02,  8.0534e-02,\n          2.3544e-02,  1.5981e-02,  1.5387e-01,  8.2582e-03,  5.6727e-02],\n        [-1.0724e-01, -1.0121e-01,  1.1407e-01,  7.3979e-02,  1.9042e-01,\n          6.3904e-02, -4.2343e-02,  3.1111e-01,  2.6577e-03,  1.0044e-01],\n        [-1.5380e-01, -3.9014e-02,  1.6723e-01,  1.9562e-02,  7.2162e-02,\n          4.7020e-03, -2.6787e-02,  2.4259e-01,  1.3872e-01,  8.0103e-02],\n        [-4.4537e-02, -6.2877e-02,  4.2886e-02,  2.4884e-02,  6.9580e-02,\n          9.9919e-02,  3.1238e-02,  2.3276e-01,  7.6951e-02,  1.3858e-02],\n        [ 2.0661e-02,  3.1961e-03, -1.8293e-02,  1.0503e-01,  1.5196e-02,\n          1.3274e-01,  9.8412e-02,  1.5229e-01,  1.5410e-02,  6.5112e-02],\n        [-3.4830e-02, -3.6878e-02,  6.7244e-03,  1.2603e-01,  9.1972e-02,\n          1.1129e-02,  2.5234e-02,  1.6536e-01,  1.0501e-02, -1.6065e-02],\n        [-1.2984e-02,  8.4783e-02,  1.0966e-01,  1.5543e-02,  4.9230e-02,\n         -7.9816e-02,  7.3797e-02,  1.1608e-01,  9.5581e-02,  1.6102e-02],\n        [-2.5668e-02, -5.3735e-02,  7.4549e-02, -1.8423e-02,  1.1479e-01,\n          1.0394e-01, -6.9197e-02,  2.4854e-01,  7.2367e-02, -2.4147e-02],\n        [-2.0620e-01, -4.6905e-02,  6.2853e-02, -1.4309e-02,  1.3710e-01,\n         -5.5271e-02,  1.1391e-01,  2.5842e-01,  6.7911e-02, -2.8575e-02],\n        [-1.1348e-01, -1.3005e-02,  2.3166e-01,  6.3980e-03,  1.4355e-01,\n          7.3211e-02, -9.4058e-02,  1.3058e-01,  7.8594e-03,  5.5539e-02],\n        [ 6.5097e-02,  2.9523e-02,  6.4318e-02,  9.8197e-02,  6.5726e-02,\n          2.0291e-01, -3.3422e-02,  8.0931e-02, -5.8931e-02, -3.1500e-02],\n        [-2.9131e-02, -2.6940e-02,  4.8132e-02,  3.4262e-02,  2.7178e-02,\n          5.6532e-02,  6.1421e-02,  2.5015e-01,  8.9080e-02, -4.6657e-02],\n        [-8.6455e-02,  3.7720e-02,  1.5260e-01,  3.2688e-02,  7.3466e-02,\n         -2.3630e-02,  7.6585e-02,  2.1494e-01,  7.2047e-02, -2.3711e-02],\n        [-1.3511e-01, -2.9897e-02,  5.7815e-02,  1.6413e-01,  2.5671e-02,\n          2.4766e-02,  1.0512e-01,  1.9182e-01,  9.4403e-02,  1.8793e-02],\n        [-1.1613e-01, -9.9574e-02,  1.3072e-01,  3.9438e-02,  9.1148e-02,\n          2.1311e-01,  4.3965e-02,  2.2985e-01,  5.7872e-02, -8.7023e-03]])\ntensor([7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 7, 7, 7, 2, 7, 7, 7, 7, 7, 2, 7, 7, 7, 2,\n        7, 7, 7, 2, 7, 8, 7, 7, 3, 7, 2, 7, 7, 2, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7,\n        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 5, 7, 7, 7, 7]) tensor([6, 4, 9, 9, 1, 0, 3, 6, 6, 1, 2, 5, 0, 8, 5, 7, 7, 7, 5, 9, 7, 9, 0, 1,\n        5, 4, 9, 1, 4, 0, 2, 5, 4, 4, 2, 9, 5, 1, 8, 3, 4, 1, 9, 1, 8, 3, 4, 2,\n        3, 7, 9, 0, 3, 7, 5, 1, 8, 3, 4, 9, 3, 3, 6, 3])\n total samples = 64 , total correct = 7, percentage = 10.94%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The model seems to be very biased towards 7. I wonder why.","metadata":{}}]}