{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### We look at ResNet model more closely here.\n\nThe resnet paper is available here if you are interested :\nhttps://arxiv.org/pdf/1512.03385.pdf","metadata":{}},{"cell_type":"markdown","source":"ref:https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035\n\n# Residual Network\n\nwhy?\n\nEven though given enough capacity , a feedforward network with a single layer should be enough to represent any function but practically the network is prone to overfit the data. So we tried making our models deeper.\n\nBut with deeper network comes the problem of vanishing gradients, as the gradient is back propogated to earlier layers repeted multiplication may make the gradient infinitely small. So performances decreases.\n\nThe core idea of ResNet is introducing these shortcut connection that skips one or more layers.\n\n![skip_connections](https://miro.medium.com/max/510/1*ByrVJspW-TefwlH7OLxNkg.png)\n\nThe authors were of the opinion that stacking layers should not degrade the network performance, because they were stacking identity mapping (layers which don't do anything). \n\nAuthors in their paper claim that \n\n\n>We present comprehensive experiments on ImageNet\n[36] to show the degradation problem and evaluate our\nmethod. We show that: 1) Our extremely deep residual nets\nare easy to optimize, but the counterpart “plain” nets (that\nsimply stack layers) exhibit higher training error when the\ndepth increases; 2) Our deep residual nets can easily enjoy\naccuracy gains from greatly increased depth, producing results substantially better than previous networks.\n\n","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/1144/1*2ns4ota94je5gSVjrpFq3A.png)\n\nThe weird connections that we see here is that the next layer not only takes input from the previous layer but also from the layer implemented before that. One we are going to implement will have three layers in between. ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"![](https://www.researchgate.net/publication/334288428/figure/tbl1/AS:778196211466240@1562547843868/Architectures-for-ResNet34-ResNet50-and-ResNet101-in-this-paper-Building-blocks-are.png)","metadata":{}},{"cell_type":"markdown","source":"In all resnet her we can see that in beginning it dows a convolution, at conv1 with 7 x 7 (the kernel size), 64 (number of channels) and then stride of convolution is 2.\n\nIt does not mention padding but given the output size we can say that the padding should be 3. Then it has 4 different resnet layers\n\nLook at resnet 50, in the first resnet layer 3 layers repeated  times so a total of 9 layers. The second resnet layer has 12, third has 18 and and fourth has 9. so ( 9 + 12 + 18 + 9 + 2 ( layers conv1 and maxpool) makes it 50.\n\nWe can see that each resnet layer has decreased the input by half. Also the input channel in last layer is always 4 times the starting input layer. this we have catered for in ourcode by `self.expansion` variable.\nfor example look at resnet 50 conv2 block, here starting kernel size is 64 which eventually becomes 256\n","metadata":{}},{"cell_type":"markdown","source":"since we are going to use these block architecture multiple times in resnet architecture let us define the block first. ","metadata":{}},{"cell_type":"code","source":"class block(nn.Module):\n    def __init__(\n        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n    ):\n        super(block, self).__init__()\n        self.expansion = 4\n        self.conv1 = nn.Conv2d(\n            in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n        self.conv2 = nn.Conv2d(\n            intermediate_channels,\n            intermediate_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n        self.conv3 = nn.Conv2d(\n            intermediate_channels,\n            intermediate_channels * self.expansion,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False\n        )\n        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.identity_downsample = identity_downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x.clone()\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n\n        if self.identity_downsample is not None:\n            identity = self.identity_downsample(identity)\n\n        x += identity\n        x = self.relu(x)\n        return x","metadata":{"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self, block, layers, img_channels, num_classes):\n        super(ResNet, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(img_channels, 64, kernel_size=7, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        #resnet layers\n        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=64, stride=1)\n        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=128, stride=2)\n        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=256, stride=2)\n        self.layer4 = self._make_layer(block, layers[3], intermediate_channels=512, stride=2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.fc = nn.Linear(512*4, num_classes)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = self.fc(x)\n        \n        return x\n        \n        \n    def _make_layer(self,block, num_residual_blocks , intermediate_channels, stride):\n        identity_downsample = None\n        layers = []\n        \n        if stride != 1 or self.in_channels != intermediate_channels * 4:\n            # if its not the first conv or if the out_channels * 4 valueis not matching\n            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*4, kernel_size=1, stride=stride),\n                                                nn.BatchNorm2d(intermediate_channels*4))\n            \n        tempblock = block(self.in_channels, intermediate_channels, identity_downsample, stride)\n        layers.append(tempblock)\n        self.in_channels = intermediate_channels * 4\n            \n        for i in range(num_residual_blocks-1):\n            layers.append(block(self.in_channels, intermediate_channels))\n            \n        return nn.Sequential(*layers)\n        ","metadata":{"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def ResNet50(img_channels=3, num_classes=1000):\n    return ResNet(block, [3,4,6,3], img_channels, num_classes)","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def ResNet101(img_channels=3, num_classes=1000):\n    return ResNet(block, [3,4,23,3], img_channels, num_classes)","metadata":{"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def ResNet152(img_channels=3, num_classes=1000):\n    return ResNet(block, [3,8,36,3], img_channels, num_classes)","metadata":{"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def test():\n    net = ResNet50()\n    x= torch.randn(2,3,224,224)\n    y =net(x)\n    print(y.shape)\n\ntest()","metadata":{"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"torch.Size([2, 1000])\n","output_type":"stream"}]}]}